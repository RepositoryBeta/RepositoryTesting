{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343396806,"user_tz":300,"elapsed":22148,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"d573d287-45b0-49bc-a4e5-a66eb1d5edcf"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'roberta-base'...\n","remote: Enumerating objects: 81, done.\u001b[K\n","remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 81\u001b[K\n","Unpacking objects: 100% (81/81), 1.63 MiB | 1.65 MiB/s, done.\n","Filtering content: 100% (5/5), 2.61 GiB | 141.60 MiB/s, done.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343417296,"user_tz":300,"elapsed":19138,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"82ba758e-4220-4f23-fc4e-a7566d3bfb50"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688343425728,"user_tz":300,"elapsed":7272,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688343428891,"user_tz":300,"elapsed":434,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688343433875,"user_tz":300,"elapsed":3840,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688343435797,"user_tz":300,"elapsed":387,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688343437318,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343438773,"user_tz":300,"elapsed":4,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"fb0550a7-1a0b-4249-b05f-f6ff9d42c80f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688343440331,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    # combine_feat_method='text_only',\n","    # combine_feat_method='concat',\n","    # combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    # combine_feat_method='attention_on_cat_and_numerical_feats',\n","    # combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688343443671,"user_tz":300,"elapsed":375,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343446476,"user_tz":300,"elapsed":3,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"fc2c7f6b-64d7-416d-9c87-39521cdd42a0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343449703,"user_tz":300,"elapsed":1518,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"0091bbab-ba21-42d6-92ce-f9d84657824d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688343450860,"user_tz":300,"elapsed":1,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688343453800,"user_tz":300,"elapsed":1392,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"aeafd393-8a63-4958-86ba-34b35f3c0892"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_classifier.bn.3.bias', 'tabular_classifier.layers.3.weight', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_classifier.layers.2.bias', 'classifier.dense.weight', 'tabular_combiner.num_bn.running_var', 'tabular_classifier.bn.2.running_var', 'tabular_classifier.bn.0.running_mean', 'tabular_combiner.num_bn.weight', 'tabular_combiner.num_layer.bias', 'tabular_combiner.layer_norm.bias', 'tabular_combiner.num_layer.weight', 'tabular_combiner.weight_num', 'tabular_classifier.bn.2.weight', 'classifier.out_proj.bias', 'tabular_classifier.bn.3.num_batches_tracked', 'tabular_classifier.layers.3.bias', 'tabular_classifier.layers.0.bias', 'tabular_classifier.layers.4.bias', 'tabular_combiner.layer_norm.weight', 'tabular_classifier.bn.0.bias', 'tabular_classifier.layers.1.bias', 'tabular_classifier.bn.0.weight', 'tabular_classifier.bn.1.bias', 'tabular_classifier.bn.1.running_var', 'tabular_classifier.bn.1.weight', 'tabular_classifier.layers.4.weight', 'tabular_classifier.bn.3.running_var', 'tabular_classifier.bn.2.bias', 'tabular_classifier.bn.1.num_batches_tracked', 'classifier.out_proj.weight', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_classifier.layers.2.weight', 'classifier.dense.bias', 'tabular_combiner.num_bn.running_mean', 'tabular_classifier.layers.1.weight', 'tabular_classifier.layers.0.weight', 'tabular_classifier.bn.3.running_mean', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_combiner.num_bn.bias', 'tabular_classifier.bn.3.weight', 'tabular_classifier.bn.2.running_mean', 'tabular_classifier.bn.0.running_var', 'tabular_classifier.bn.1.running_mean']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688343455785,"user_tz":300,"elapsed":378,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688343461791,"user_tz":300,"elapsed":4275,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688343463561,"user_tz":300,"elapsed":384,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"6b4446b3-8792-450b-cac7-1d4b1587b5de"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'weighted_feature_sum_on_transformer_cat_and_numerical_feats'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688352863964,"user_tz":300,"elapsed":9398082,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"9d51f24e-8759-44a6-996b-f5950dbba330"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:36:34, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.100700</td>\n","      <td>0.099826</td>\n","      <td>0.290038</td>\n","      <td>0.099826</td>\n","      <td>0.315953</td>\n","      <td>-4.801474</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.086500</td>\n","      <td>0.070394</td>\n","      <td>0.240947</td>\n","      <td>0.070394</td>\n","      <td>0.265319</td>\n","      <td>-3.090986</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.074100</td>\n","      <td>0.066285</td>\n","      <td>0.234667</td>\n","      <td>0.066285</td>\n","      <td>0.257458</td>\n","      <td>-2.852174</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.063100</td>\n","      <td>0.067502</td>\n","      <td>0.235072</td>\n","      <td>0.067502</td>\n","      <td>0.259811</td>\n","      <td>-2.922916</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.052500</td>\n","      <td>0.048271</td>\n","      <td>0.195934</td>\n","      <td>0.048271</td>\n","      <td>0.219707</td>\n","      <td>-1.805309</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.050500</td>\n","      <td>0.043965</td>\n","      <td>0.183551</td>\n","      <td>0.043965</td>\n","      <td>0.209679</td>\n","      <td>-1.555079</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.040900</td>\n","      <td>0.037434</td>\n","      <td>0.166432</td>\n","      <td>0.037434</td>\n","      <td>0.193480</td>\n","      <td>-1.175518</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.034500</td>\n","      <td>0.026970</td>\n","      <td>0.137989</td>\n","      <td>0.026970</td>\n","      <td>0.164227</td>\n","      <td>-0.567398</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.028600</td>\n","      <td>0.019875</td>\n","      <td>0.114607</td>\n","      <td>0.019875</td>\n","      <td>0.140979</td>\n","      <td>-0.155052</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.024100</td>\n","      <td>0.023038</td>\n","      <td>0.126246</td>\n","      <td>0.023038</td>\n","      <td>0.151782</td>\n","      <td>-0.338848</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.020500</td>\n","      <td>0.017207</td>\n","      <td>0.108207</td>\n","      <td>0.017207</td>\n","      <td>0.131175</td>\n","      <td>0.000007</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.017900</td>\n","      <td>0.018242</td>\n","      <td>0.112216</td>\n","      <td>0.018242</td>\n","      <td>0.135064</td>\n","      <td>-0.060154</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.016100</td>\n","      <td>0.017158</td>\n","      <td>0.107260</td>\n","      <td>0.017158</td>\n","      <td>0.130987</td>\n","      <td>0.002878</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.014700</td>\n","      <td>0.026674</td>\n","      <td>0.140684</td>\n","      <td>0.026674</td>\n","      <td>0.163321</td>\n","      <td>-0.550161</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.013300</td>\n","      <td>0.012746</td>\n","      <td>0.088533</td>\n","      <td>0.012746</td>\n","      <td>0.112900</td>\n","      <td>0.259235</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.011800</td>\n","      <td>0.012513</td>\n","      <td>0.089576</td>\n","      <td>0.012513</td>\n","      <td>0.111860</td>\n","      <td>0.272822</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.011000</td>\n","      <td>0.010109</td>\n","      <td>0.078756</td>\n","      <td>0.010109</td>\n","      <td>0.100545</td>\n","      <td>0.412494</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.010500</td>\n","      <td>0.009997</td>\n","      <td>0.080225</td>\n","      <td>0.009997</td>\n","      <td>0.099987</td>\n","      <td>0.419000</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.009800</td>\n","      <td>0.011179</td>\n","      <td>0.085286</td>\n","      <td>0.011179</td>\n","      <td>0.105732</td>\n","      <td>0.350317</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.010500</td>\n","      <td>0.014151</td>\n","      <td>0.099553</td>\n","      <td>0.014151</td>\n","      <td>0.118958</td>\n","      <td>0.177604</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.009200</td>\n","      <td>0.009969</td>\n","      <td>0.079251</td>\n","      <td>0.009969</td>\n","      <td>0.099843</td>\n","      <td>0.420670</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.009400</td>\n","      <td>0.008276</td>\n","      <td>0.072194</td>\n","      <td>0.008276</td>\n","      <td>0.090972</td>\n","      <td>0.519037</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.008800</td>\n","      <td>0.016406</td>\n","      <td>0.104221</td>\n","      <td>0.016406</td>\n","      <td>0.128087</td>\n","      <td>0.046542</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.008000</td>\n","      <td>0.009868</td>\n","      <td>0.076700</td>\n","      <td>0.009868</td>\n","      <td>0.099340</td>\n","      <td>0.426492</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.007600</td>\n","      <td>0.009335</td>\n","      <td>0.077136</td>\n","      <td>0.009335</td>\n","      <td>0.096620</td>\n","      <td>0.457466</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.007000</td>\n","      <td>0.007852</td>\n","      <td>0.070048</td>\n","      <td>0.007852</td>\n","      <td>0.088610</td>\n","      <td>0.543691</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.007100</td>\n","      <td>0.012632</td>\n","      <td>0.086392</td>\n","      <td>0.012632</td>\n","      <td>0.112393</td>\n","      <td>0.265870</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.006600</td>\n","      <td>0.008325</td>\n","      <td>0.073390</td>\n","      <td>0.008325</td>\n","      <td>0.091239</td>\n","      <td>0.516213</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.006200</td>\n","      <td>0.010914</td>\n","      <td>0.082392</td>\n","      <td>0.010914</td>\n","      <td>0.104469</td>\n","      <td>0.365742</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.006200</td>\n","      <td>0.013155</td>\n","      <td>0.090236</td>\n","      <td>0.013155</td>\n","      <td>0.114695</td>\n","      <td>0.235497</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.010300</td>\n","      <td>0.022171</td>\n","      <td>0.112239</td>\n","      <td>0.022171</td>\n","      <td>0.148899</td>\n","      <td>-0.288480</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.008000</td>\n","      <td>0.009306</td>\n","      <td>0.076236</td>\n","      <td>0.009306</td>\n","      <td>0.096467</td>\n","      <td>0.459189</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.006600</td>\n","      <td>0.008084</td>\n","      <td>0.070197</td>\n","      <td>0.008084</td>\n","      <td>0.089909</td>\n","      <td>0.530216</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.006700</td>\n","      <td>0.009515</td>\n","      <td>0.077021</td>\n","      <td>0.009515</td>\n","      <td>0.097547</td>\n","      <td>0.447009</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.006000</td>\n","      <td>0.008321</td>\n","      <td>0.071894</td>\n","      <td>0.008321</td>\n","      <td>0.091219</td>\n","      <td>0.516423</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.006000</td>\n","      <td>0.008278</td>\n","      <td>0.071057</td>\n","      <td>0.008278</td>\n","      <td>0.090985</td>\n","      <td>0.518907</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.005700</td>\n","      <td>0.008088</td>\n","      <td>0.070760</td>\n","      <td>0.008088</td>\n","      <td>0.089932</td>\n","      <td>0.529978</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.005600</td>\n","      <td>0.009045</td>\n","      <td>0.072489</td>\n","      <td>0.009045</td>\n","      <td>0.095106</td>\n","      <td>0.474336</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.005400</td>\n","      <td>0.012721</td>\n","      <td>0.090760</td>\n","      <td>0.012721</td>\n","      <td>0.112789</td>\n","      <td>0.260689</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.005300</td>\n","      <td>0.008486</td>\n","      <td>0.071583</td>\n","      <td>0.008486</td>\n","      <td>0.092120</td>\n","      <td>0.506829</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.005100</td>\n","      <td>0.010617</td>\n","      <td>0.080353</td>\n","      <td>0.010617</td>\n","      <td>0.103037</td>\n","      <td>0.383014</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.005000</td>\n","      <td>0.008589</td>\n","      <td>0.072746</td>\n","      <td>0.008589</td>\n","      <td>0.092675</td>\n","      <td>0.500870</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.005000</td>\n","      <td>0.008579</td>\n","      <td>0.073740</td>\n","      <td>0.008579</td>\n","      <td>0.092625</td>\n","      <td>0.501405</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.005100</td>\n","      <td>0.009003</td>\n","      <td>0.073786</td>\n","      <td>0.009003</td>\n","      <td>0.094882</td>\n","      <td>0.476804</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.004800</td>\n","      <td>0.008635</td>\n","      <td>0.073504</td>\n","      <td>0.008635</td>\n","      <td>0.092924</td>\n","      <td>0.498185</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.004800</td>\n","      <td>0.010280</td>\n","      <td>0.079819</td>\n","      <td>0.010280</td>\n","      <td>0.101389</td>\n","      <td>0.402589</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.004700</td>\n","      <td>0.009553</td>\n","      <td>0.074891</td>\n","      <td>0.009553</td>\n","      <td>0.097741</td>\n","      <td>0.444804</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.005100</td>\n","      <td>0.010881</td>\n","      <td>0.082802</td>\n","      <td>0.010881</td>\n","      <td>0.104310</td>\n","      <td>0.367669</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.004600</td>\n","      <td>0.008782</td>\n","      <td>0.073988</td>\n","      <td>0.008782</td>\n","      <td>0.093712</td>\n","      <td>0.489637</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.004500</td>\n","      <td>0.009975</td>\n","      <td>0.076316</td>\n","      <td>0.009975</td>\n","      <td>0.099876</td>\n","      <td>0.420282</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.004200</td>\n","      <td>0.009377</td>\n","      <td>0.075197</td>\n","      <td>0.009377</td>\n","      <td>0.096834</td>\n","      <td>0.455057</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.004300</td>\n","      <td>0.008658</td>\n","      <td>0.074458</td>\n","      <td>0.008658</td>\n","      <td>0.093048</td>\n","      <td>0.496835</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.004200</td>\n","      <td>0.007826</td>\n","      <td>0.069922</td>\n","      <td>0.007826</td>\n","      <td>0.088464</td>\n","      <td>0.545199</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.004300</td>\n","      <td>0.011166</td>\n","      <td>0.083847</td>\n","      <td>0.011166</td>\n","      <td>0.105670</td>\n","      <td>0.351072</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.004400</td>\n","      <td>0.007773</td>\n","      <td>0.068938</td>\n","      <td>0.007773</td>\n","      <td>0.088162</td>\n","      <td>0.548292</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.004200</td>\n","      <td>0.007914</td>\n","      <td>0.070259</td>\n","      <td>0.007914</td>\n","      <td>0.088962</td>\n","      <td>0.540055</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.004100</td>\n","      <td>0.009724</td>\n","      <td>0.077296</td>\n","      <td>0.009724</td>\n","      <td>0.098611</td>\n","      <td>0.434882</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.004000</td>\n","      <td>0.009145</td>\n","      <td>0.074943</td>\n","      <td>0.009145</td>\n","      <td>0.095629</td>\n","      <td>0.468541</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.004000</td>\n","      <td>0.008418</td>\n","      <td>0.071993</td>\n","      <td>0.008418</td>\n","      <td>0.091751</td>\n","      <td>0.510771</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.004100</td>\n","      <td>0.007883</td>\n","      <td>0.070156</td>\n","      <td>0.007883</td>\n","      <td>0.088784</td>\n","      <td>0.541896</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.003800</td>\n","      <td>0.008111</td>\n","      <td>0.069950</td>\n","      <td>0.008111</td>\n","      <td>0.090063</td>\n","      <td>0.528610</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.003800</td>\n","      <td>0.007700</td>\n","      <td>0.068565</td>\n","      <td>0.007700</td>\n","      <td>0.087752</td>\n","      <td>0.552490</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.003700</td>\n","      <td>0.008184</td>\n","      <td>0.071402</td>\n","      <td>0.008184</td>\n","      <td>0.090467</td>\n","      <td>0.524370</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.003700</td>\n","      <td>0.007958</td>\n","      <td>0.069589</td>\n","      <td>0.007958</td>\n","      <td>0.089207</td>\n","      <td>0.537524</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.003800</td>\n","      <td>0.008652</td>\n","      <td>0.073465</td>\n","      <td>0.008652</td>\n","      <td>0.093017</td>\n","      <td>0.497174</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.003800</td>\n","      <td>0.008630</td>\n","      <td>0.073032</td>\n","      <td>0.008630</td>\n","      <td>0.092897</td>\n","      <td>0.498472</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.003800</td>\n","      <td>0.009427</td>\n","      <td>0.077669</td>\n","      <td>0.009427</td>\n","      <td>0.097092</td>\n","      <td>0.452152</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.003700</td>\n","      <td>0.007837</td>\n","      <td>0.069465</td>\n","      <td>0.007837</td>\n","      <td>0.088525</td>\n","      <td>0.544569</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.003500</td>\n","      <td>0.008029</td>\n","      <td>0.069660</td>\n","      <td>0.008029</td>\n","      <td>0.089607</td>\n","      <td>0.533366</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.003500</td>\n","      <td>0.007929</td>\n","      <td>0.069766</td>\n","      <td>0.007929</td>\n","      <td>0.089045</td>\n","      <td>0.539201</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.003500</td>\n","      <td>0.007962</td>\n","      <td>0.069836</td>\n","      <td>0.007962</td>\n","      <td>0.089229</td>\n","      <td>0.537294</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.003400</td>\n","      <td>0.008040</td>\n","      <td>0.071116</td>\n","      <td>0.008040</td>\n","      <td>0.089667</td>\n","      <td>0.532746</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.003400</td>\n","      <td>0.007739</td>\n","      <td>0.068390</td>\n","      <td>0.007739</td>\n","      <td>0.087971</td>\n","      <td>0.550248</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.003500</td>\n","      <td>0.007722</td>\n","      <td>0.069304</td>\n","      <td>0.007722</td>\n","      <td>0.087875</td>\n","      <td>0.551230</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.003500</td>\n","      <td>0.007689</td>\n","      <td>0.068734</td>\n","      <td>0.007689</td>\n","      <td>0.087685</td>\n","      <td>0.553167</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.003500</td>\n","      <td>0.007926</td>\n","      <td>0.069928</td>\n","      <td>0.007926</td>\n","      <td>0.089025</td>\n","      <td>0.539404</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.003400</td>\n","      <td>0.008399</td>\n","      <td>0.071104</td>\n","      <td>0.008399</td>\n","      <td>0.091645</td>\n","      <td>0.511897</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.003400</td>\n","      <td>0.008457</td>\n","      <td>0.072018</td>\n","      <td>0.008457</td>\n","      <td>0.091964</td>\n","      <td>0.508492</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.003200</td>\n","      <td>0.007754</td>\n","      <td>0.069883</td>\n","      <td>0.007754</td>\n","      <td>0.088055</td>\n","      <td>0.549388</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.003100</td>\n","      <td>0.008057</td>\n","      <td>0.071568</td>\n","      <td>0.008057</td>\n","      <td>0.089763</td>\n","      <td>0.531743</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.003400</td>\n","      <td>0.007790</td>\n","      <td>0.069443</td>\n","      <td>0.007790</td>\n","      <td>0.088259</td>\n","      <td>0.547298</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.003200</td>\n","      <td>0.007851</td>\n","      <td>0.069767</td>\n","      <td>0.007851</td>\n","      <td>0.088603</td>\n","      <td>0.543759</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.003200</td>\n","      <td>0.008555</td>\n","      <td>0.071832</td>\n","      <td>0.008555</td>\n","      <td>0.092496</td>\n","      <td>0.502793</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.003200</td>\n","      <td>0.007834</td>\n","      <td>0.069605</td>\n","      <td>0.007834</td>\n","      <td>0.088509</td>\n","      <td>0.544734</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.003300</td>\n","      <td>0.007819</td>\n","      <td>0.069479</td>\n","      <td>0.007819</td>\n","      <td>0.088426</td>\n","      <td>0.545582</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.003200</td>\n","      <td>0.007719</td>\n","      <td>0.069532</td>\n","      <td>0.007719</td>\n","      <td>0.087858</td>\n","      <td>0.551404</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.003200</td>\n","      <td>0.007567</td>\n","      <td>0.068442</td>\n","      <td>0.007567</td>\n","      <td>0.086987</td>\n","      <td>0.560257</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.003200</td>\n","      <td>0.007532</td>\n","      <td>0.068144</td>\n","      <td>0.007532</td>\n","      <td>0.086789</td>\n","      <td>0.562258</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.003200</td>\n","      <td>0.007910</td>\n","      <td>0.069731</td>\n","      <td>0.007910</td>\n","      <td>0.088937</td>\n","      <td>0.540322</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.003100</td>\n","      <td>0.007550</td>\n","      <td>0.068812</td>\n","      <td>0.007550</td>\n","      <td>0.086893</td>\n","      <td>0.561207</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.003200</td>\n","      <td>0.007964</td>\n","      <td>0.069750</td>\n","      <td>0.007964</td>\n","      <td>0.089241</td>\n","      <td>0.537167</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.003200</td>\n","      <td>0.007586</td>\n","      <td>0.068421</td>\n","      <td>0.007586</td>\n","      <td>0.087096</td>\n","      <td>0.559150</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.003100</td>\n","      <td>0.007735</td>\n","      <td>0.069118</td>\n","      <td>0.007735</td>\n","      <td>0.087949</td>\n","      <td>0.550472</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.003200</td>\n","      <td>0.007912</td>\n","      <td>0.069521</td>\n","      <td>0.007912</td>\n","      <td>0.088947</td>\n","      <td>0.540210</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.002900</td>\n","      <td>0.007887</td>\n","      <td>0.069637</td>\n","      <td>0.007887</td>\n","      <td>0.088811</td>\n","      <td>0.541623</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.003000</td>\n","      <td>0.007818</td>\n","      <td>0.069237</td>\n","      <td>0.007818</td>\n","      <td>0.088421</td>\n","      <td>0.545635</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.003300</td>\n","      <td>0.007885</td>\n","      <td>0.069587</td>\n","      <td>0.007885</td>\n","      <td>0.088795</td>\n","      <td>0.541781</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.003000</td>\n","      <td>0.007847</td>\n","      <td>0.069391</td>\n","      <td>0.007847</td>\n","      <td>0.088584</td>\n","      <td>0.543962</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.002900</td>\n","      <td>0.007777</td>\n","      <td>0.069060</td>\n","      <td>0.007777</td>\n","      <td>0.088185</td>\n","      <td>0.548059</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.003100</td>\n","      <td>0.007685</td>\n","      <td>0.068948</td>\n","      <td>0.007685</td>\n","      <td>0.087666</td>\n","      <td>0.553360</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 36min 13s, sys: 57.1 s, total: 2h 37min 11s\n","Wall time: 2h 36min 38s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.01057794906876304, metrics={'train_runtime': 9398.0313, 'train_samples_per_second': 63.535, 'train_steps_per_second': 1.99, 'total_flos': 1.1469141443375683e+17, 'train_loss': 0.01057794906876304, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688352893404,"user_tz":300,"elapsed":439,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688352897918,"user_tz":300,"elapsed":1892,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688352901815,"user_tz":300,"elapsed":2014,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"54dc4978-090b-4e7d-bff0-570fbd80db47"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.007466352079063654,\n"," 'eval_MAE': 0.06558784046418076,\n"," 'eval_MSE': 0.007466352539637629,\n"," 'eval_RMSE': 0.08640805830267007,\n"," 'eval_R2': 0.5499732600629859,\n"," 'eval_runtime': 2.2455,\n"," 'eval_samples_per_second': 332.671,\n"," 'eval_steps_per_second': 41.862,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":22}]}]}