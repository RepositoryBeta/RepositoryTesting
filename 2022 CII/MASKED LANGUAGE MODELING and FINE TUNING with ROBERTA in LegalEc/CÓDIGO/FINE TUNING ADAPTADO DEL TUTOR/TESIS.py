# -*- coding: utf-8 -*-
"""Codigo_tutor.ipynb

Automatically generated by Colaboratory.

* Se recomienda utilizar una RAM de 32gb para agilizar la ejecución
* input_dim = 1024 --- se utiliza en el modelo roberta-large-bne  
* input_dim = 768 --- se utiliza en el modelo xml-roberta-base

Original file is located at
    https://colab.research.google.com/drive/1hLjWVpVjqRhq1FGC3KCq_Tun4vCOk9jy
"""
#si se utiliza colab, importar la librería
#!pip install transformers

#****************************************************************************************importar librerias 
import pandas as pd
import numpy as np
import unicodedata
import re
import os
import csv
import sklearn.metrics as metrics
from math import sqrt
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AutoTokenizer, AutoModel
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
from transformers import RobertaForSequenceClassification, RobertaConfig
from transformers import Trainer, TrainingArguments
from transformers import EvalPrediction
from sklearn import metrics, feature_selection
from transformers import AutoModelForMaskedLM
from transformers import AutoTokenizer
from transformers.modeling_outputs import SequenceClassifierOutput

#****************************************************************************************importar archivos en colab desde drive
#from google.colab import drive
#drive.mount('/content/drive')

#****************************************************************************************archivos y directorios
print("Initializing")

DATA_FILE_PATH = "C:/TESIS/CODIGO/CORPUS/CEDUG_single_train_nor.xlsx"
#MODEL_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Bert/bert-base-uncased"
#****************************************************************************************se utiliza para el modelo de xml-roberta-base
MODEL_PATH = "C:/TESIS/CODIGO/MODELOS/xlm-roberta-base"
#****************************************************************************************se utiliza para el modelo de roberta-large-bne
#MODEL_PATH = "C:/TESIS/CODIGO/MODELOS/roberta-large-bne"
#****************************************************************************************se utiliza para el modelo de xlm-roberta-large 
#MODEL_PATH = "C:/TESIS/CODIGO/MODELOS/xlm-roberta-large"
#MODEL_TYPE = 'bert'
MODEL_TYPE = 'roberta'
#USE_FEATURES = True #con características
USE_FEATURES = False #sin características
EPOCHS = 10
OUTPUT_PATH = "C:/TESIS/CODIGO/MODELOS/" + MODEL_PATH.split('/')[-1] + "-finetuned"

#****************************************************************************************cargar y preparar datos
print("Loading data")

df = pd.read_excel(DATA_FILE_PATH)
NUM_FEATURES = 23
df['features'] = df.iloc[:,5:].values.tolist()
df = df.dropna()
df['sentence_token'] = df.apply(
      lambda x: str(x['sentence']).lower() + ' [SEP] ' + str(x['token']).lower(), 
      axis=1)

#print(df)
#df

#**************************************************************************************vectorizar los textos
print("Vectorizing texts")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

def vectorize_text(s, max_length):

    # Unicode normalization
    #s = s.strip().lower()
    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

    # Remove undesired characters
    #s = re.sub(r"([.¿?()/'"",;:$€])", r' /1 ', s)
    s = re.sub(r"[^a-zA-Záéíóú.,!?;:()$€]+", r" ", s)

    # Text to tensor
    input_ids = tokenizer.encode(
      s,
      add_special_tokens=True,
      max_length=max_length,
      padding='longest', 
      truncation=True,
      return_tensors='np'
    )
    return input_ids[0]

df['text_vec'] = df.apply(lambda r: vectorize_text(r['sentence_token'], 512), axis=1)

#print(df)
#df

#****************************************************************************************crear train y datasets
print("Creating datasets")

df = df.sample(frac=1)
train_portion = 0.8
split_point = int(train_portion*len(df))
train_data, test_data =  df[:split_point], df[split_point:] 
print(len(train_data), 'train, ', len(test_data), 'test')

class MyDataset(Dataset):
    def __init__(self, dataframe):
        #print(dataframe)
        self.len = len(dataframe)
        self.data = dataframe
        
    def __getitem__(self, index):
        input_ids = torch.tensor(self.data.text_vec.iloc[index]).cpu()
        attention_mask = torch.ones([input_ids.size(0)]).cpu()
        targets = self.data.complexity.iloc[index]
        added_features = self.data.features.iloc[index] if USE_FEATURES else None
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': targets,
            'added_features': added_features,
         }
            
    def __len__(self):
        return self.len

train_set, test_set = MyDataset(train_data), MyDataset(test_data)

#print(train_set)

#print(test_set)

#****************************************************************************************definir el modelo personalizado
print("Creating model")

def collate_batch(batch):
    """ Optimize memory by setting all vectors in batch to a length equal to max
        length found
    """
    
    def pad_sequence(in_tensor, max_size):
        """ Fill tensor with zeros up to max_size
        """
        out_tensor = np.zeros(max_size)
        out_tensor[:in_tensor.size(0)] = in_tensor.numpy()
        return out_tensor

    print("BATCH SIZE:", len(batch))
    
    batch_inputs = []
    batch_attention_masks = []
    batch_targets = []
    batch_added_features = []

    max_size = max([ex['input_ids'].size(0) for ex in batch])
    for item in batch:
        batch_inputs.append(pad_sequence(item['input_ids'], max_size))
        batch_attention_masks.append(pad_sequence(item['attention_mask'], max_size))
        batch_targets.append([float(item['labels'])])
        if USE_FEATURES:
            batch_added_features.append(item['added_features'])
    input_ids = torch.tensor(batch_inputs, dtype=torch.long)
    attention_mask = torch.tensor(batch_attention_masks, dtype=torch.long)
    labels = torch.tensor(batch_targets, dtype=torch.float)
    if USE_FEATURES:
        added_features = torch.tensor(batch_added_features, dtype=torch.float)
    else:
        added_features = None
    #print("input_ids:", input_ids.size(), "/ attention_mask:", attention_mask.size(), "/ labels:", labels.size(), "/ added_features:", added_features.size())
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "added_features": added_features
    }

class SequenceClassifierWithFeatures(nn.Module):

    def __init__(self, num_labels=1, model_path=MODEL_PATH, model_type=MODEL_TYPE):
        """
        model_type: "bert" or "roberta"
        """
        super(SequenceClassifierWithFeatures, self).__init__()
        
        #Usar si sale RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1024 and 768x768)
        #input_dim = 1024  
        input_dim = 768
        if USE_FEATURES:
            input_dim += NUM_FEATURES
        print("input_dim:", input_dim, USE_FEATURES, NUM_FEATURES)
        
        output_dim = num_labels

        self.num_labels= num_labels
        self.problem_type='regression'
        self.model_path = model_path
        self.model_type = model_type
        self.base_model = AutoModel.from_pretrained(model_path, num_labels=self.num_labels)
        self.dropout = nn.Dropout(0.5)
        self.dense = nn.Linear(input_dim, input_dim)
        self.linear = nn.Linear(input_dim, output_dim)
        self.loss_func = nn.MSELoss()
        
      
    def forward(self, input_ids=None, attention_mask=None, labels=None, added_features=None):
        #print("input_ids:", input_ids.size(), "added_features:", added_features.size())
        outputs = self.base_model(input_ids, attention_mask=attention_mask)
        if self.model_type == "bert":
            outputs = outputs[1]
            if USE_FEATURES:
                outputs = torch.cat((outputs, added_features), dim = -1)
        elif self.model_type == "roberta":
            outputs = outputs[0][:,0,:]
            if USE_FEATURES:
                outputs = torch.cat((outputs, added_features), dim = -1)
            outputs = self.dropout(outputs)
            outputs = self.dense(outputs)
            outputs = torch.tanh(outputs)
        else:
            raise Exception('Invalid model_type: only "bert" or "roberta" models are supported')

        outputs = self.dropout(outputs)
        logits = self.linear(outputs)
        
        loss = None
        if labels is not None:
          loss = self.loss_func(logits.view(-1), labels.view(-1))
          print("loss:", loss)

        return SequenceClassifierOutput(loss=loss, logits=logits)

#****************************************************************************************definir entrenamiento personalizado 
print("Defining custom training")

class MyTrainer(Trainer):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)

  def get_train_dataloader(self):
    return DataLoader(
        self.train_dataset,
        collate_fn=collate_batch,
        batch_size=self.args.per_device_train_batch_size
    )

  def get_eval_dataloader(self, eval_dataset):
    return DataLoader(
        self.eval_dataset,
        collate_fn=collate_batch,
        batch_size=self.args.per_device_eval_batch_size
    )

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.squeeze(preds)
    labels = np.squeeze(p.label_ids)
    print("SIZES::::",labels.shape, preds.shape)
    mse = metrics.mean_squared_error(labels, preds)
    return {"MA": metrics.mean_absolute_error(labels, preds),
            'MSE': mse,
            'RMSE': sqrt(mse), 
            'R2': metrics.r2_score(labels, preds),                     
            'Poisson': metrics.mean_poisson_deviance(labels, preds),
            'Pearson': np.corrcoef(labels, preds)[0,1]}

training_args = TrainingArguments(
    output_dir='output',
    evaluation_strategy='epoch',
    num_train_epochs=EPOCHS,
    remove_unused_columns=False,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
)

model = SequenceClassifierWithFeatures(model_path=MODEL_PATH, model_type=MODEL_TYPE)

trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=test_set,
    compute_metrics=compute_metrics
)

#****************************************************************************************resultados del modelo
print("Training")
trainer.train()

print("Saving model")
if not os.path.exists(OUTPUT_PATH):
    os.mkdir(OUTPUT_PATH)
trainer.save_model(OUTPUT_PATH)