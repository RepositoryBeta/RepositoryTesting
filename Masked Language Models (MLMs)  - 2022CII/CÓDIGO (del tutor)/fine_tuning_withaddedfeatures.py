# -*- coding: utf-8 -*-
"""fine_tuning-withAddedFeatures.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmg8mUZ6Z2cMtBJq1RZae6VYjdsPZ8V7
"""

""" 
This notebook trains and evaluates models with linguistic features integration

Authors:
    Jenny Ortiz-Zambrano
    Arturo Montejo-Ráez
    2022
"""

#!pip install transformers

import pandas as pd
import numpy as np
import unicodedata
import re
import os
import csv
import sklearn.metrics as metrics
from math import sqrt
from transformers import RobertaTokenizer, RobertaForMaskedLM
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AutoTokenizer, AutoModel
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
from transformers import RobertaForSequenceClassification, RobertaConfig
from transformers import Trainer, TrainingArguments
from transformers import EvalPrediction
from sklearn import metrics, feature_selection
from transformers import AutoModelForMaskedLM
from transformers import AutoTokenizer
from transformers.modeling_outputs import SequenceClassifierOutput

#from google.colab import drive
#drive.mount('/content/drive')

print("Initializing")
    #SE ESCRIBE EL DIRECTORIO DEL CORPUS A CARGAR
DATA_FILE_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/corpus/Complex.xlsx"

    #SE USA MODELO ROBERTA-BASE PARA IDIOMA INGLÉS
#MODEL_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Roberta-base/roberta-base"  

    #SE USA MODELO ROBERTA-LARGE PARA IDIOMA INGLÉS    
MODEL_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Roberta-large/roberta-large"  

    #SE USA MODELO BERT-BASE-UNCASED PARA IDIOMA INGLÉS
#MODEL_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Bert/bert-base-uncased"  
  
    #SE USA MODELO XLM-ROBERTA-BASE PARA IDIOMA INGLÉS     
#MODEL_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Xlm-roberta/xlm-roberta-base" 

    #SE SELECCIONA QUE TIPO DE MODELO SE EJECUTARÁ
#MODEL_TYPE = 'bert'
MODEL_TYPE = 'roberta'

USE_FEATURES = True    #True para ejecutar con características, False para ejecutar sin características
EPOCHS = 10

    #SE ESCRIBE EL DIRECTORIO DONDE SE GUARDARÁ EL NUEVO MODELO EJECUTADO
OUTPUT_PATH = "C:/Users/Marcos/Desktop/Nueva carpeta/Tesis/Roberta-large/" + MODEL_PATH.split('/')[-1] + "-finetuned"

print("Loading data")

df = pd.read_excel(DATA_FILE_PATH)
NUM_FEATURES = 23       #SE ESCRIBE EL NÚMERO DE CARACTERÍSTICAS QUE TIENE EL CORPUS CARGADO
df['features'] = df.iloc[:,5:].values.tolist()      #SE ESCRIBE EL NÚMERO DE LA COLUMNA QUE EMPIEZAN LAS CARACTERÍSTICAS
df = df.dropna()
df['sentence_token'] = df.apply(
      lambda x: str(x['sentence']).lower() + ' [SEP] ' + str(x['token']).lower(), 
      axis=1)

#print(df)
#df

print("Vectorizing texts")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

def vectorize_text(s, max_length):

    # Unicode normalization
    #s = s.strip().lower()
    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

    # Remove undesired characters
    #s = re.sub(r"([.¿?()/'"",;:$€])", r' /1 ', s)
    s = re.sub(r"[^a-zA-Záéíóú.,!?;:()$€]+", r" ", s)

    # Text to tensor
    input_ids = tokenizer.encode(
      s,
      add_special_tokens=True,
      max_length=max_length,
      padding='longest', 
      truncation=True,
      return_tensors='np'
    )
    return input_ids[0]

df['text_vec'] = df.apply(lambda r: vectorize_text(r['sentence_token'], 512), axis=1)

#print(df)
#df

print("Creating datasets")

df = df.sample(frac=1)
train_portion = 0.8
split_point = int(train_portion*len(df))
train_data, test_data =  df[:split_point], df[split_point:] 
print(len(train_data), 'train, ', len(test_data), 'test')

class MyDataset(Dataset):
    def __init__(self, dataframe):
        #print(dataframe)
        self.len = len(dataframe)
        self.data = dataframe
        
    def __getitem__(self, index):
        input_ids = torch.tensor(self.data.text_vec.iloc[index]).cpu()
        attention_mask = torch.ones([input_ids.size(0)]).cpu()
        targets = self.data.complexity.iloc[index]
        added_features = self.data.features.iloc[index] if USE_FEATURES else None
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': targets,
            'added_features': added_features,
         }
            
    def __len__(self):
        return self.len

train_set, test_set = MyDataset(train_data), MyDataset(test_data)

#print(train_set)

#print(test_set)

print("Creating model")

def collate_batch(batch):
    """ Optimize memory by setting all vectors in batch to a length equal to max
        length found
    """
    
    def pad_sequence(in_tensor, max_size):
        """ Fill tensor with zeros up to max_size
        """
        out_tensor = np.zeros(max_size)
        out_tensor[:in_tensor.size(0)] = in_tensor.numpy()
        return out_tensor

    print("BATCH SIZE:", len(batch))
    
    batch_inputs = []
    batch_attention_masks = []
    batch_targets = []
    batch_added_features = []

    max_size = max([ex['input_ids'].size(0) for ex in batch])
    for item in batch:
        batch_inputs.append(pad_sequence(item['input_ids'], max_size))
        batch_attention_masks.append(pad_sequence(item['attention_mask'], max_size))
        batch_targets.append([float(item['labels'])])
        if USE_FEATURES:
            batch_added_features.append(item['added_features'])

    #input_ids = torch.tensor(batch_inputs, dtype=torch.long)
    #attention_mask = torch.tensor(batch_attention_masks, dtype=torch.long)
    ''' USO DE LA FUNCIÓN NP.ARRAY() PARA TRANSFORMAR LA LISTA DE CONJUNTO DE DATOS EN UN ARREGLO Y ASÍ 
        EVITAR EL COLAPSO POR MEMORIA
    '''
    input_ids = torch.tensor(np.array(batch_inputs), dtype=torch.long)
    attention_mask = torch.tensor(np.array(batch_attention_masks), dtype=torch.long)

    labels = torch.tensor(batch_targets, dtype=torch.float)
    if USE_FEATURES:
        added_features = torch.tensor(batch_added_features, dtype=torch.float)
    else:
        added_features = None
    #print("input_ids:", input_ids.size(), "/ attention_mask:", attention_mask.size(), "/ labels:", labels.size(), "/ added_features:", added_features.size())
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
        "added_features": added_features
    }

class SequenceClassifierWithFeatures(nn.Module):

    def __init__(self, num_labels=1, model_path=MODEL_PATH, model_type=MODEL_TYPE):
        """
        model_type: "bert" or "roberta"
        """
        super(SequenceClassifierWithFeatures, self).__init__()
        
        #input_dim = 768  #El input_dim se elije dependiendo del archivo config.json del modelo a usar
        input_dim = 1024  # 1024 USA CONFIG.JSON DEL MODELO ROBERTA-LARGE
        if USE_FEATURES:
            input_dim += NUM_FEATURES
        print("input_dim:", input_dim, USE_FEATURES, NUM_FEATURES)
        
        output_dim = num_labels

        self.num_labels= num_labels
        self.problem_type='regression'
        self.model_path = model_path
        self.model_type = model_type
        self.base_model = AutoModel.from_pretrained(model_path, num_labels=self.num_labels)
        self.dropout = nn.Dropout(0.5)
        self.dense = nn.Linear(input_dim, input_dim)
        self.linear = nn.Linear(input_dim, output_dim)
        self.loss_func = nn.MSELoss()
        
      
    def forward(self, input_ids=None, attention_mask=None, labels=None, added_features=None):
        #print("input_ids:", input_ids.size(), "added_features:", added_features.size())
        outputs = self.base_model(input_ids, attention_mask=attention_mask)
        if self.model_type == "bert":
            outputs = outputs[1]
            if USE_FEATURES:
                outputs = torch.cat((outputs, added_features), dim = -1)
        elif self.model_type == "roberta":
            outputs = outputs[0][:,0,:]
            if USE_FEATURES:
                outputs = torch.cat((outputs, added_features), dim = -1)
            outputs = self.dropout(outputs)
            outputs = self.dense(outputs)
            outputs = torch.tanh(outputs)
        else:
            raise Exception('Invalid model_type: only "bert" or "roberta" models are supported')

        outputs = self.dropout(outputs)
        logits = self.linear(outputs)
        
        loss = None
        if labels is not None:
          loss = self.loss_func(logits.view(-1), labels.view(-1))
          print("loss:", loss)

        return SequenceClassifierOutput(loss=loss, logits=logits)

print("Defining custom training")

class MyTrainer(Trainer):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)

  def get_train_dataloader(self):
    return DataLoader(
        self.train_dataset,
        collate_fn=collate_batch,
        batch_size=self.args.per_device_train_batch_size
    )

  def get_eval_dataloader(self, eval_dataset):
    return DataLoader(
        self.eval_dataset,
        collate_fn=collate_batch,
        batch_size=self.args.per_device_eval_batch_size
    )

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
    preds = np.squeeze(preds)
    labels = np.squeeze(p.label_ids)
    print("SIZES::::",labels.shape, preds.shape)
    mse = metrics.mean_squared_error(labels, preds)
    return {"MAE": metrics.mean_absolute_error(labels, preds),
            'MSE': mse,
            'RMSE': sqrt(mse), 
            'R2': metrics.r2_score(labels, preds),                     
            'Poisson': metrics.mean_poisson_deviance(labels, preds),
            'Pearson': np.corrcoef(labels, preds)[0,1]}

training_args = TrainingArguments(
    output_dir='output',
    evaluation_strategy='epoch',
    num_train_epochs=EPOCHS,
    remove_unused_columns=False,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
)

model = SequenceClassifierWithFeatures(model_path=MODEL_PATH, model_type=MODEL_TYPE)

trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=test_set,
    compute_metrics=compute_metrics
)

print("Training")
trainer.train()

print("Saving model")
if not os.path.exists(OUTPUT_PATH):
    os.mkdir(OUTPUT_PATH)
trainer.save_model(OUTPUT_PATH)


# SE USA PARA QUE SE GUARDE UN ARCHIVO LLAMADO trainer_state.json DONDE SE GUARDAN LOS VALORES DE LAS METRICAS
""""
def save_state(self):
    
    if not self.is_world_process_zero():
        return

    path = os.path.join(self.args.output_dir, "trainer_state.json")
    self.state.save_to_json(path)


trainer.save_state()
"""