{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258378055,"user_tz":300,"elapsed":21508,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"8ecbbe9e-b423-4faa-e589-f8d989e765db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'roberta-base'...\n","remote: Enumerating objects: 81, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n","Unpacking objects: 100% (81/81), 1.63 MiB | 5.36 MiB/s, done.\n","Filtering content: 100% (5/5), 2.61 GiB | 130.48 MiB/s, done.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258400269,"user_tz":300,"elapsed":19899,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"9adfcabd-bb4f-40d7-ebbd-91c940285f1e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688258409298,"user_tz":300,"elapsed":7148,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688258412236,"user_tz":300,"elapsed":302,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688258417653,"user_tz":300,"elapsed":3905,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688258419128,"user_tz":300,"elapsed":318,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688258420892,"user_tz":300,"elapsed":265,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258422608,"user_tz":300,"elapsed":3,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"c243113f-d52a-4afa-9e03-c2b9c5c52587"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688258426659,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    # combine_feat_method='text_only',\n","    combine_feat_method='concat',\n","    # combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    #combine_feat_method='attention_on_cat_and_numerical_feats',\n","    #combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688258440705,"user_tz":300,"elapsed":275,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258444541,"user_tz":300,"elapsed":274,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"cdb48aba-822a-4888-f98a-792dbf265b3c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258449128,"user_tz":300,"elapsed":2227,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"b00fb1e4-a598-46dc-ad8f-9e46127e1e39"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688258451451,"user_tz":300,"elapsed":273,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688258455335,"user_tz":300,"elapsed":1401,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"7366a1ac-c75c-4da3-a44a-bde3a4a9ff6e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_classifier.bn.2.bias', 'tabular_classifier.layers.1.bias', 'tabular_classifier.layers.3.bias', 'tabular_classifier.bn.1.num_batches_tracked', 'tabular_classifier.bn.3.running_mean', 'tabular_combiner.num_bn.weight', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_classifier.bn.2.running_mean', 'tabular_classifier.layers.2.bias', 'tabular_classifier.layers.4.weight', 'tabular_classifier.bn.0.weight', 'classifier.out_proj.weight', 'tabular_classifier.bn.3.weight', 'tabular_classifier.layers.4.bias', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_classifier.layers.0.weight', 'tabular_combiner.num_bn.bias', 'tabular_combiner.num_bn.running_var', 'tabular_classifier.bn.1.bias', 'tabular_classifier.bn.2.weight', 'tabular_classifier.bn.3.num_batches_tracked', 'tabular_classifier.bn.0.bias', 'classifier.dense.weight', 'tabular_classifier.bn.1.running_var', 'tabular_classifier.layers.2.weight', 'tabular_classifier.bn.3.bias', 'tabular_classifier.bn.0.running_mean', 'classifier.out_proj.bias', 'tabular_classifier.bn.1.running_mean', 'tabular_classifier.layers.0.bias', 'tabular_classifier.layers.3.weight', 'tabular_classifier.bn.3.running_var', 'tabular_classifier.bn.1.weight', 'tabular_combiner.num_bn.running_mean', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_classifier.bn.0.running_var', 'classifier.dense.bias', 'tabular_classifier.bn.2.running_var', 'tabular_classifier.layers.1.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688258459055,"user_tz":300,"elapsed":296,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688258466283,"user_tz":300,"elapsed":5402,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688258468370,"user_tz":300,"elapsed":307,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"b25ab509-b4d7-4635-c864-f31a1a654cae"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'concat'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688268053989,"user_tz":300,"elapsed":9582966,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"289bb9d8-2f47-4d5b-e724-eae27ee8bf8f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:39:38, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.099000</td>\n","      <td>0.087094</td>\n","      <td>0.272141</td>\n","      <td>0.087094</td>\n","      <td>0.295118</td>\n","      <td>-4.407197</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.084600</td>\n","      <td>0.080275</td>\n","      <td>0.260422</td>\n","      <td>0.080275</td>\n","      <td>0.283328</td>\n","      <td>-3.983817</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.072700</td>\n","      <td>0.064359</td>\n","      <td>0.229972</td>\n","      <td>0.064359</td>\n","      <td>0.253691</td>\n","      <td>-2.995678</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.063600</td>\n","      <td>0.063193</td>\n","      <td>0.226828</td>\n","      <td>0.063193</td>\n","      <td>0.251383</td>\n","      <td>-2.923308</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.061200</td>\n","      <td>0.061093</td>\n","      <td>0.217263</td>\n","      <td>0.061093</td>\n","      <td>0.247170</td>\n","      <td>-2.792909</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.051500</td>\n","      <td>0.053473</td>\n","      <td>0.200416</td>\n","      <td>0.053473</td>\n","      <td>0.231243</td>\n","      <td>-2.319861</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.045500</td>\n","      <td>0.047744</td>\n","      <td>0.186871</td>\n","      <td>0.047744</td>\n","      <td>0.218505</td>\n","      <td>-1.964171</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.041100</td>\n","      <td>0.029837</td>\n","      <td>0.138545</td>\n","      <td>0.029837</td>\n","      <td>0.172734</td>\n","      <td>-0.852421</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.037100</td>\n","      <td>0.030874</td>\n","      <td>0.142904</td>\n","      <td>0.030874</td>\n","      <td>0.175709</td>\n","      <td>-0.916760</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.033700</td>\n","      <td>0.028452</td>\n","      <td>0.135370</td>\n","      <td>0.028452</td>\n","      <td>0.168677</td>\n","      <td>-0.766419</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.030700</td>\n","      <td>0.029353</td>\n","      <td>0.138603</td>\n","      <td>0.029353</td>\n","      <td>0.171326</td>\n","      <td>-0.822342</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.028400</td>\n","      <td>0.022473</td>\n","      <td>0.118532</td>\n","      <td>0.022473</td>\n","      <td>0.149908</td>\n","      <td>-0.395191</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.026900</td>\n","      <td>0.023802</td>\n","      <td>0.122336</td>\n","      <td>0.023802</td>\n","      <td>0.154279</td>\n","      <td>-0.477726</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.025600</td>\n","      <td>0.023581</td>\n","      <td>0.121977</td>\n","      <td>0.023581</td>\n","      <td>0.153561</td>\n","      <td>-0.464015</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.023800</td>\n","      <td>0.019593</td>\n","      <td>0.110067</td>\n","      <td>0.019593</td>\n","      <td>0.139974</td>\n","      <td>-0.216402</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.023200</td>\n","      <td>0.020467</td>\n","      <td>0.112948</td>\n","      <td>0.020467</td>\n","      <td>0.143064</td>\n","      <td>-0.270703</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.022000</td>\n","      <td>0.019362</td>\n","      <td>0.109508</td>\n","      <td>0.019362</td>\n","      <td>0.139149</td>\n","      <td>-0.202103</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.021400</td>\n","      <td>0.018669</td>\n","      <td>0.107678</td>\n","      <td>0.018669</td>\n","      <td>0.136634</td>\n","      <td>-0.159040</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.021200</td>\n","      <td>0.019666</td>\n","      <td>0.110238</td>\n","      <td>0.019666</td>\n","      <td>0.140236</td>\n","      <td>-0.220966</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.020800</td>\n","      <td>0.018136</td>\n","      <td>0.104804</td>\n","      <td>0.018136</td>\n","      <td>0.134669</td>\n","      <td>-0.125948</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.019900</td>\n","      <td>0.017944</td>\n","      <td>0.104272</td>\n","      <td>0.017944</td>\n","      <td>0.133957</td>\n","      <td>-0.114070</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.019500</td>\n","      <td>0.016237</td>\n","      <td>0.098780</td>\n","      <td>0.016237</td>\n","      <td>0.127425</td>\n","      <td>-0.008077</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.018600</td>\n","      <td>0.017072</td>\n","      <td>0.101111</td>\n","      <td>0.017072</td>\n","      <td>0.130658</td>\n","      <td>-0.059873</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.018400</td>\n","      <td>0.015758</td>\n","      <td>0.097130</td>\n","      <td>0.015758</td>\n","      <td>0.125532</td>\n","      <td>0.021656</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.018500</td>\n","      <td>0.015476</td>\n","      <td>0.095482</td>\n","      <td>0.015476</td>\n","      <td>0.124402</td>\n","      <td>0.039197</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.018100</td>\n","      <td>0.014795</td>\n","      <td>0.094133</td>\n","      <td>0.014795</td>\n","      <td>0.121634</td>\n","      <td>0.081468</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.017600</td>\n","      <td>0.014923</td>\n","      <td>0.094550</td>\n","      <td>0.014923</td>\n","      <td>0.122160</td>\n","      <td>0.073514</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.017800</td>\n","      <td>0.015602</td>\n","      <td>0.095796</td>\n","      <td>0.015602</td>\n","      <td>0.124907</td>\n","      <td>0.031369</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.017500</td>\n","      <td>0.014791</td>\n","      <td>0.093263</td>\n","      <td>0.014791</td>\n","      <td>0.121618</td>\n","      <td>0.081711</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.017100</td>\n","      <td>0.015031</td>\n","      <td>0.093302</td>\n","      <td>0.015031</td>\n","      <td>0.122602</td>\n","      <td>0.066792</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.017100</td>\n","      <td>0.014427</td>\n","      <td>0.091836</td>\n","      <td>0.014427</td>\n","      <td>0.120111</td>\n","      <td>0.104329</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.016600</td>\n","      <td>0.013952</td>\n","      <td>0.090472</td>\n","      <td>0.013952</td>\n","      <td>0.118119</td>\n","      <td>0.133789</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.016900</td>\n","      <td>0.013667</td>\n","      <td>0.089562</td>\n","      <td>0.013667</td>\n","      <td>0.116906</td>\n","      <td>0.151491</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.016600</td>\n","      <td>0.013533</td>\n","      <td>0.088694</td>\n","      <td>0.013533</td>\n","      <td>0.116332</td>\n","      <td>0.159801</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.016100</td>\n","      <td>0.013450</td>\n","      <td>0.087981</td>\n","      <td>0.013450</td>\n","      <td>0.115976</td>\n","      <td>0.164946</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.016300</td>\n","      <td>0.013672</td>\n","      <td>0.088395</td>\n","      <td>0.013672</td>\n","      <td>0.116929</td>\n","      <td>0.151155</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.016200</td>\n","      <td>0.013539</td>\n","      <td>0.089043</td>\n","      <td>0.013539</td>\n","      <td>0.116357</td>\n","      <td>0.159450</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.015800</td>\n","      <td>0.013243</td>\n","      <td>0.087049</td>\n","      <td>0.013243</td>\n","      <td>0.115078</td>\n","      <td>0.177827</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.015700</td>\n","      <td>0.013447</td>\n","      <td>0.088232</td>\n","      <td>0.013447</td>\n","      <td>0.115962</td>\n","      <td>0.165147</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.015800</td>\n","      <td>0.013330</td>\n","      <td>0.087342</td>\n","      <td>0.013330</td>\n","      <td>0.115457</td>\n","      <td>0.172394</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.015900</td>\n","      <td>0.013085</td>\n","      <td>0.086596</td>\n","      <td>0.013085</td>\n","      <td>0.114391</td>\n","      <td>0.187607</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.015800</td>\n","      <td>0.013074</td>\n","      <td>0.086490</td>\n","      <td>0.013074</td>\n","      <td>0.114343</td>\n","      <td>0.188293</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.015700</td>\n","      <td>0.012893</td>\n","      <td>0.086264</td>\n","      <td>0.012893</td>\n","      <td>0.113549</td>\n","      <td>0.199529</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.015700</td>\n","      <td>0.013013</td>\n","      <td>0.086269</td>\n","      <td>0.013013</td>\n","      <td>0.114077</td>\n","      <td>0.192069</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.015500</td>\n","      <td>0.012949</td>\n","      <td>0.085944</td>\n","      <td>0.012949</td>\n","      <td>0.113794</td>\n","      <td>0.196063</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.015300</td>\n","      <td>0.012914</td>\n","      <td>0.085995</td>\n","      <td>0.012914</td>\n","      <td>0.113638</td>\n","      <td>0.198271</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.015300</td>\n","      <td>0.012927</td>\n","      <td>0.086075</td>\n","      <td>0.012927</td>\n","      <td>0.113696</td>\n","      <td>0.197454</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.015200</td>\n","      <td>0.012836</td>\n","      <td>0.085747</td>\n","      <td>0.012836</td>\n","      <td>0.113294</td>\n","      <td>0.203115</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.015400</td>\n","      <td>0.012740</td>\n","      <td>0.085329</td>\n","      <td>0.012740</td>\n","      <td>0.112871</td>\n","      <td>0.209061</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.015400</td>\n","      <td>0.012997</td>\n","      <td>0.085553</td>\n","      <td>0.012997</td>\n","      <td>0.114003</td>\n","      <td>0.193111</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.015200</td>\n","      <td>0.012943</td>\n","      <td>0.085345</td>\n","      <td>0.012943</td>\n","      <td>0.113768</td>\n","      <td>0.196431</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.015000</td>\n","      <td>0.012872</td>\n","      <td>0.085493</td>\n","      <td>0.012872</td>\n","      <td>0.113456</td>\n","      <td>0.200829</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.014900</td>\n","      <td>0.012721</td>\n","      <td>0.085597</td>\n","      <td>0.012721</td>\n","      <td>0.112786</td>\n","      <td>0.210245</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.015100</td>\n","      <td>0.012658</td>\n","      <td>0.085361</td>\n","      <td>0.012658</td>\n","      <td>0.112508</td>\n","      <td>0.214132</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.015000</td>\n","      <td>0.012838</td>\n","      <td>0.085239</td>\n","      <td>0.012838</td>\n","      <td>0.113305</td>\n","      <td>0.202965</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.015300</td>\n","      <td>0.012768</td>\n","      <td>0.085459</td>\n","      <td>0.012768</td>\n","      <td>0.112996</td>\n","      <td>0.207299</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.015000</td>\n","      <td>0.012870</td>\n","      <td>0.085055</td>\n","      <td>0.012870</td>\n","      <td>0.113445</td>\n","      <td>0.200991</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.015300</td>\n","      <td>0.012871</td>\n","      <td>0.084984</td>\n","      <td>0.012871</td>\n","      <td>0.113450</td>\n","      <td>0.200922</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.015100</td>\n","      <td>0.012769</td>\n","      <td>0.085015</td>\n","      <td>0.012769</td>\n","      <td>0.113000</td>\n","      <td>0.207248</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.015000</td>\n","      <td>0.012870</td>\n","      <td>0.085405</td>\n","      <td>0.012870</td>\n","      <td>0.113447</td>\n","      <td>0.200957</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.014800</td>\n","      <td>0.012794</td>\n","      <td>0.085863</td>\n","      <td>0.012794</td>\n","      <td>0.113109</td>\n","      <td>0.205713</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.014900</td>\n","      <td>0.012766</td>\n","      <td>0.085128</td>\n","      <td>0.012766</td>\n","      <td>0.112985</td>\n","      <td>0.207454</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.014800</td>\n","      <td>0.012753</td>\n","      <td>0.085324</td>\n","      <td>0.012753</td>\n","      <td>0.112928</td>\n","      <td>0.208251</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.015200</td>\n","      <td>0.012749</td>\n","      <td>0.085032</td>\n","      <td>0.012749</td>\n","      <td>0.112910</td>\n","      <td>0.208506</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.014900</td>\n","      <td>0.012719</td>\n","      <td>0.085024</td>\n","      <td>0.012719</td>\n","      <td>0.112781</td>\n","      <td>0.210322</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.014600</td>\n","      <td>0.012786</td>\n","      <td>0.084735</td>\n","      <td>0.012786</td>\n","      <td>0.113077</td>\n","      <td>0.206160</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.015200</td>\n","      <td>0.012726</td>\n","      <td>0.084983</td>\n","      <td>0.012726</td>\n","      <td>0.112807</td>\n","      <td>0.209946</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.014700</td>\n","      <td>0.012874</td>\n","      <td>0.085364</td>\n","      <td>0.012874</td>\n","      <td>0.113462</td>\n","      <td>0.200745</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.014900</td>\n","      <td>0.012657</td>\n","      <td>0.085103</td>\n","      <td>0.012657</td>\n","      <td>0.112505</td>\n","      <td>0.214175</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.014900</td>\n","      <td>0.012896</td>\n","      <td>0.084892</td>\n","      <td>0.012896</td>\n","      <td>0.113559</td>\n","      <td>0.199386</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.014800</td>\n","      <td>0.012837</td>\n","      <td>0.084822</td>\n","      <td>0.012837</td>\n","      <td>0.113299</td>\n","      <td>0.203041</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.014700</td>\n","      <td>0.012705</td>\n","      <td>0.085069</td>\n","      <td>0.012705</td>\n","      <td>0.112715</td>\n","      <td>0.211236</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.014900</td>\n","      <td>0.012714</td>\n","      <td>0.085032</td>\n","      <td>0.012714</td>\n","      <td>0.112756</td>\n","      <td>0.210662</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.014800</td>\n","      <td>0.012681</td>\n","      <td>0.085163</td>\n","      <td>0.012681</td>\n","      <td>0.112611</td>\n","      <td>0.212698</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.014800</td>\n","      <td>0.012686</td>\n","      <td>0.085181</td>\n","      <td>0.012686</td>\n","      <td>0.112630</td>\n","      <td>0.212425</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.014700</td>\n","      <td>0.012685</td>\n","      <td>0.084985</td>\n","      <td>0.012685</td>\n","      <td>0.112628</td>\n","      <td>0.212458</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.014800</td>\n","      <td>0.012662</td>\n","      <td>0.084861</td>\n","      <td>0.012662</td>\n","      <td>0.112524</td>\n","      <td>0.213917</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.014700</td>\n","      <td>0.012740</td>\n","      <td>0.085003</td>\n","      <td>0.012740</td>\n","      <td>0.112872</td>\n","      <td>0.209037</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.014800</td>\n","      <td>0.012660</td>\n","      <td>0.085071</td>\n","      <td>0.012660</td>\n","      <td>0.112515</td>\n","      <td>0.214035</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.014400</td>\n","      <td>0.012745</td>\n","      <td>0.084934</td>\n","      <td>0.012745</td>\n","      <td>0.112894</td>\n","      <td>0.208731</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.014500</td>\n","      <td>0.012740</td>\n","      <td>0.084791</td>\n","      <td>0.012740</td>\n","      <td>0.112873</td>\n","      <td>0.209026</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.014500</td>\n","      <td>0.012714</td>\n","      <td>0.084831</td>\n","      <td>0.012714</td>\n","      <td>0.112756</td>\n","      <td>0.210670</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.014400</td>\n","      <td>0.012668</td>\n","      <td>0.085072</td>\n","      <td>0.012668</td>\n","      <td>0.112553</td>\n","      <td>0.213498</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.014200</td>\n","      <td>0.012826</td>\n","      <td>0.085106</td>\n","      <td>0.012826</td>\n","      <td>0.113254</td>\n","      <td>0.203680</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.014700</td>\n","      <td>0.012587</td>\n","      <td>0.085100</td>\n","      <td>0.012587</td>\n","      <td>0.112193</td>\n","      <td>0.218523</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.014600</td>\n","      <td>0.012749</td>\n","      <td>0.085355</td>\n","      <td>0.012749</td>\n","      <td>0.112912</td>\n","      <td>0.208477</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.014600</td>\n","      <td>0.012681</td>\n","      <td>0.084917</td>\n","      <td>0.012681</td>\n","      <td>0.112612</td>\n","      <td>0.212687</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.014300</td>\n","      <td>0.012749</td>\n","      <td>0.085011</td>\n","      <td>0.012749</td>\n","      <td>0.112912</td>\n","      <td>0.208486</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.014800</td>\n","      <td>0.012666</td>\n","      <td>0.084950</td>\n","      <td>0.012666</td>\n","      <td>0.112544</td>\n","      <td>0.213634</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.014600</td>\n","      <td>0.012649</td>\n","      <td>0.084891</td>\n","      <td>0.012649</td>\n","      <td>0.112466</td>\n","      <td>0.214717</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.014400</td>\n","      <td>0.012692</td>\n","      <td>0.084952</td>\n","      <td>0.012692</td>\n","      <td>0.112660</td>\n","      <td>0.212010</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.014500</td>\n","      <td>0.012687</td>\n","      <td>0.085206</td>\n","      <td>0.012687</td>\n","      <td>0.112636</td>\n","      <td>0.212352</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.014700</td>\n","      <td>0.012721</td>\n","      <td>0.084856</td>\n","      <td>0.012721</td>\n","      <td>0.112786</td>\n","      <td>0.210245</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.014600</td>\n","      <td>0.012703</td>\n","      <td>0.084854</td>\n","      <td>0.012703</td>\n","      <td>0.112707</td>\n","      <td>0.211357</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.014500</td>\n","      <td>0.012716</td>\n","      <td>0.085129</td>\n","      <td>0.012716</td>\n","      <td>0.112767</td>\n","      <td>0.210512</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.014500</td>\n","      <td>0.012799</td>\n","      <td>0.084971</td>\n","      <td>0.012799</td>\n","      <td>0.113131</td>\n","      <td>0.205403</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.014600</td>\n","      <td>0.012725</td>\n","      <td>0.085025</td>\n","      <td>0.012725</td>\n","      <td>0.112803</td>\n","      <td>0.210006</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.014400</td>\n","      <td>0.012834</td>\n","      <td>0.085096</td>\n","      <td>0.012834</td>\n","      <td>0.113287</td>\n","      <td>0.203209</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.014500</td>\n","      <td>0.012742</td>\n","      <td>0.084860</td>\n","      <td>0.012742</td>\n","      <td>0.112881</td>\n","      <td>0.208909</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.014400</td>\n","      <td>0.012609</td>\n","      <td>0.084867</td>\n","      <td>0.012609</td>\n","      <td>0.112292</td>\n","      <td>0.217154</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 39min 14s, sys: 57.8 s, total: 2h 40min 12s\n","Wall time: 2h 39min 42s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.020729452454470058, metrics={'train_runtime': 9582.382, 'train_samples_per_second': 62.312, 'train_steps_per_second': 1.951, 'total_flos': 1.1467551501975677e+17, 'train_loss': 0.020729452454470058, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688268059537,"user_tz":300,"elapsed":559,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688268062288,"user_tz":300,"elapsed":1575,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688268066507,"user_tz":300,"elapsed":2236,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"dc6e9694-140a-4739-af24-8f0e3469d8b4"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.01258587371557951,\n"," 'eval_MAE': 0.0855758983746341,\n"," 'eval_MSE': 0.012585874037178736,\n"," 'eval_RMSE': 0.11218678191827564,\n"," 'eval_R2': 0.24286592313958244,\n"," 'eval_runtime': 2.2897,\n"," 'eval_samples_per_second': 326.243,\n"," 'eval_steps_per_second': 41.053,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":22}]}]}