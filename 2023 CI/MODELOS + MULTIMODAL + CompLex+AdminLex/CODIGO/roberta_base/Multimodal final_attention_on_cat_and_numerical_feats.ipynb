{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319063962,"user_tz":300,"elapsed":21803,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"74114bf2-028c-49b7-a1ed-88b94a25be96"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'roberta-base'...\n","remote: Enumerating objects: 81, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n","Unpacking objects: 100% (81/81), 1.63 MiB | 6.12 MiB/s, done.\n","Filtering content: 100% (5/5), 2.61 GiB | 129.53 MiB/s, done.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319089160,"user_tz":300,"elapsed":17583,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"25269d58-9369-4833-adc6-ceec91e85226"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688319112402,"user_tz":300,"elapsed":7566,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688319117349,"user_tz":300,"elapsed":162,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688319121495,"user_tz":300,"elapsed":3433,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688319122370,"user_tz":300,"elapsed":170,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688319123307,"user_tz":300,"elapsed":168,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319124854,"user_tz":300,"elapsed":368,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"962cc4fa-437e-40d9-977a-666351ff3db8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688319126803,"user_tz":300,"elapsed":172,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    # combine_feat_method='text_only',\n","    # combine_feat_method='concat',\n","    # combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    combine_feat_method='attention_on_cat_and_numerical_feats',\n","    #combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688319129924,"user_tz":300,"elapsed":170,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319133897,"user_tz":300,"elapsed":184,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"c84021c6-3d2e-4292-a948-8d2775ba53ba"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319137201,"user_tz":300,"elapsed":2101,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"072c75bb-7b0c-471b-e086-d38d5cf3f1c0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688319141216,"user_tz":300,"elapsed":168,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688319144034,"user_tz":300,"elapsed":1358,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"854b6da0-1ca1-4f90-eaf9-414a44b1cf16"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_classifier.bn.1.running_mean', 'classifier.out_proj.weight', 'tabular_classifier.layers.1.weight', 'tabular_combiner.num_bn.weight', 'tabular_classifier.bn.2.running_mean', 'tabular_combiner.num_bn.bias', 'tabular_combiner.bias_num', 'tabular_combiner.num_bn.running_mean', 'tabular_combiner.bias', 'tabular_combiner.weight_a', 'tabular_classifier.bn.2.weight', 'tabular_classifier.bn.3.running_var', 'tabular_classifier.bn.1.bias', 'tabular_classifier.bn.1.running_var', 'tabular_classifier.layers.4.bias', 'classifier.dense.weight', 'tabular_classifier.layers.3.weight', 'tabular_classifier.bn.3.num_batches_tracked', 'tabular_classifier.bn.3.running_mean', 'tabular_combiner.weight_transformer', 'classifier.dense.bias', 'tabular_classifier.layers.1.bias', 'tabular_classifier.bn.3.weight', 'tabular_classifier.layers.2.weight', 'tabular_combiner.weight_num', 'tabular_classifier.bn.2.running_var', 'tabular_classifier.layers.3.bias', 'tabular_classifier.bn.0.bias', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_classifier.layers.0.bias', 'tabular_classifier.bn.0.running_var', 'tabular_classifier.bn.3.bias', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_classifier.layers.2.bias', 'tabular_classifier.bn.1.weight', 'tabular_classifier.bn.2.bias', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_classifier.layers.4.weight', 'tabular_classifier.bn.0.weight', 'tabular_classifier.layers.0.weight', 'tabular_classifier.bn.1.num_batches_tracked', 'tabular_combiner.bias_transformer', 'tabular_classifier.bn.0.running_mean', 'classifier.out_proj.bias', 'tabular_combiner.num_bn.running_var']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688319145315,"user_tz":300,"elapsed":3,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688319152545,"user_tz":300,"elapsed":5084,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688319153455,"user_tz":300,"elapsed":6,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"21cdf86a-f7b6-436b-aaa7-c4a1efc0eeb8"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'attention_on_cat_and_numerical_feats'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688328484437,"user_tz":300,"elapsed":9328872,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"d40fb7c8-1c24-41b4-d641-42fc2dc75d5f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:35:24, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.094200</td>\n","      <td>0.088537</td>\n","      <td>0.269366</td>\n","      <td>0.088537</td>\n","      <td>0.297551</td>\n","      <td>-4.006981</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.080900</td>\n","      <td>0.075820</td>\n","      <td>0.246546</td>\n","      <td>0.075820</td>\n","      <td>0.275355</td>\n","      <td>-3.287842</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.070000</td>\n","      <td>0.063517</td>\n","      <td>0.222328</td>\n","      <td>0.063517</td>\n","      <td>0.252027</td>\n","      <td>-2.592085</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.060600</td>\n","      <td>0.074710</td>\n","      <td>0.245045</td>\n","      <td>0.074710</td>\n","      <td>0.273331</td>\n","      <td>-3.225044</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.051900</td>\n","      <td>0.083382</td>\n","      <td>0.257076</td>\n","      <td>0.083382</td>\n","      <td>0.288760</td>\n","      <td>-3.715490</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.043300</td>\n","      <td>0.038349</td>\n","      <td>0.168737</td>\n","      <td>0.038349</td>\n","      <td>0.195829</td>\n","      <td>-1.168730</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.034800</td>\n","      <td>0.024551</td>\n","      <td>0.133139</td>\n","      <td>0.024551</td>\n","      <td>0.156688</td>\n","      <td>-0.388434</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.029800</td>\n","      <td>0.028830</td>\n","      <td>0.146916</td>\n","      <td>0.028830</td>\n","      <td>0.169795</td>\n","      <td>-0.630442</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.024700</td>\n","      <td>0.023234</td>\n","      <td>0.128556</td>\n","      <td>0.023234</td>\n","      <td>0.152427</td>\n","      <td>-0.313942</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.020400</td>\n","      <td>0.015762</td>\n","      <td>0.103506</td>\n","      <td>0.015762</td>\n","      <td>0.125548</td>\n","      <td>0.108596</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.017700</td>\n","      <td>0.012181</td>\n","      <td>0.089177</td>\n","      <td>0.012181</td>\n","      <td>0.110369</td>\n","      <td>0.311111</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.015600</td>\n","      <td>0.025737</td>\n","      <td>0.139344</td>\n","      <td>0.025737</td>\n","      <td>0.160428</td>\n","      <td>-0.455511</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.013800</td>\n","      <td>0.010679</td>\n","      <td>0.082477</td>\n","      <td>0.010679</td>\n","      <td>0.103341</td>\n","      <td>0.396051</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.012000</td>\n","      <td>0.009506</td>\n","      <td>0.078220</td>\n","      <td>0.009506</td>\n","      <td>0.097498</td>\n","      <td>0.462417</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.011000</td>\n","      <td>0.010000</td>\n","      <td>0.081405</td>\n","      <td>0.010000</td>\n","      <td>0.099998</td>\n","      <td>0.434498</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.010200</td>\n","      <td>0.009805</td>\n","      <td>0.079997</td>\n","      <td>0.009805</td>\n","      <td>0.099021</td>\n","      <td>0.445494</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.013100</td>\n","      <td>0.012719</td>\n","      <td>0.087667</td>\n","      <td>0.012719</td>\n","      <td>0.112776</td>\n","      <td>0.280734</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.011100</td>\n","      <td>0.012506</td>\n","      <td>0.091482</td>\n","      <td>0.012506</td>\n","      <td>0.111830</td>\n","      <td>0.292757</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.009300</td>\n","      <td>0.012851</td>\n","      <td>0.090822</td>\n","      <td>0.012851</td>\n","      <td>0.113364</td>\n","      <td>0.273221</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.008300</td>\n","      <td>0.008360</td>\n","      <td>0.071929</td>\n","      <td>0.008360</td>\n","      <td>0.091434</td>\n","      <td>0.527212</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.007800</td>\n","      <td>0.008446</td>\n","      <td>0.070988</td>\n","      <td>0.008446</td>\n","      <td>0.091903</td>\n","      <td>0.522349</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.007700</td>\n","      <td>0.007579</td>\n","      <td>0.067482</td>\n","      <td>0.007579</td>\n","      <td>0.087055</td>\n","      <td>0.571413</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.007300</td>\n","      <td>0.010447</td>\n","      <td>0.077759</td>\n","      <td>0.010447</td>\n","      <td>0.102211</td>\n","      <td>0.409190</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.007300</td>\n","      <td>0.009023</td>\n","      <td>0.073848</td>\n","      <td>0.009023</td>\n","      <td>0.094989</td>\n","      <td>0.489728</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.008600</td>\n","      <td>0.008009</td>\n","      <td>0.070692</td>\n","      <td>0.008009</td>\n","      <td>0.089492</td>\n","      <td>0.547080</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.007600</td>\n","      <td>0.010724</td>\n","      <td>0.081289</td>\n","      <td>0.010724</td>\n","      <td>0.103557</td>\n","      <td>0.393525</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.006800</td>\n","      <td>0.007458</td>\n","      <td>0.067607</td>\n","      <td>0.007458</td>\n","      <td>0.086357</td>\n","      <td>0.578256</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.006800</td>\n","      <td>0.007316</td>\n","      <td>0.066348</td>\n","      <td>0.007316</td>\n","      <td>0.085532</td>\n","      <td>0.586277</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.006800</td>\n","      <td>0.008053</td>\n","      <td>0.068602</td>\n","      <td>0.008053</td>\n","      <td>0.089741</td>\n","      <td>0.544556</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.007400</td>\n","      <td>0.014789</td>\n","      <td>0.099698</td>\n","      <td>0.014789</td>\n","      <td>0.121612</td>\n","      <td>0.163616</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.007200</td>\n","      <td>0.007482</td>\n","      <td>0.068103</td>\n","      <td>0.007482</td>\n","      <td>0.086500</td>\n","      <td>0.576855</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.007300</td>\n","      <td>0.007184</td>\n","      <td>0.066775</td>\n","      <td>0.007184</td>\n","      <td>0.084761</td>\n","      <td>0.593697</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.006700</td>\n","      <td>0.007310</td>\n","      <td>0.066173</td>\n","      <td>0.007310</td>\n","      <td>0.085500</td>\n","      <td>0.586590</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.006300</td>\n","      <td>0.009396</td>\n","      <td>0.073667</td>\n","      <td>0.009396</td>\n","      <td>0.096933</td>\n","      <td>0.468631</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.006300</td>\n","      <td>0.014884</td>\n","      <td>0.092395</td>\n","      <td>0.014884</td>\n","      <td>0.122000</td>\n","      <td>0.158276</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.006200</td>\n","      <td>0.011033</td>\n","      <td>0.084983</td>\n","      <td>0.011033</td>\n","      <td>0.105036</td>\n","      <td>0.376075</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.006300</td>\n","      <td>0.008637</td>\n","      <td>0.070073</td>\n","      <td>0.008637</td>\n","      <td>0.092936</td>\n","      <td>0.511546</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.008000</td>\n","      <td>0.008410</td>\n","      <td>0.073003</td>\n","      <td>0.008410</td>\n","      <td>0.091707</td>\n","      <td>0.524387</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.007700</td>\n","      <td>0.008718</td>\n","      <td>0.074572</td>\n","      <td>0.008718</td>\n","      <td>0.093372</td>\n","      <td>0.506955</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.006400</td>\n","      <td>0.011877</td>\n","      <td>0.084807</td>\n","      <td>0.011877</td>\n","      <td>0.108982</td>\n","      <td>0.328324</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.005800</td>\n","      <td>0.008338</td>\n","      <td>0.069765</td>\n","      <td>0.008338</td>\n","      <td>0.091311</td>\n","      <td>0.528485</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.006000</td>\n","      <td>0.008097</td>\n","      <td>0.070725</td>\n","      <td>0.008097</td>\n","      <td>0.089983</td>\n","      <td>0.542100</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.005400</td>\n","      <td>0.007312</td>\n","      <td>0.066306</td>\n","      <td>0.007312</td>\n","      <td>0.085509</td>\n","      <td>0.586501</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.005600</td>\n","      <td>0.007506</td>\n","      <td>0.068797</td>\n","      <td>0.007506</td>\n","      <td>0.086638</td>\n","      <td>0.575508</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.005400</td>\n","      <td>0.009048</td>\n","      <td>0.073371</td>\n","      <td>0.009048</td>\n","      <td>0.095119</td>\n","      <td>0.488331</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.005200</td>\n","      <td>0.007325</td>\n","      <td>0.065936</td>\n","      <td>0.007325</td>\n","      <td>0.085588</td>\n","      <td>0.585736</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.005200</td>\n","      <td>0.007785</td>\n","      <td>0.068077</td>\n","      <td>0.007785</td>\n","      <td>0.088232</td>\n","      <td>0.559745</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.005100</td>\n","      <td>0.007472</td>\n","      <td>0.066792</td>\n","      <td>0.007472</td>\n","      <td>0.086439</td>\n","      <td>0.577452</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.004900</td>\n","      <td>0.007935</td>\n","      <td>0.069041</td>\n","      <td>0.007935</td>\n","      <td>0.089077</td>\n","      <td>0.551271</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.004900</td>\n","      <td>0.007104</td>\n","      <td>0.066007</td>\n","      <td>0.007104</td>\n","      <td>0.084282</td>\n","      <td>0.598278</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.004900</td>\n","      <td>0.007538</td>\n","      <td>0.068057</td>\n","      <td>0.007538</td>\n","      <td>0.086820</td>\n","      <td>0.573724</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.004600</td>\n","      <td>0.007547</td>\n","      <td>0.067463</td>\n","      <td>0.007547</td>\n","      <td>0.086874</td>\n","      <td>0.573196</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.004900</td>\n","      <td>0.007343</td>\n","      <td>0.066589</td>\n","      <td>0.007343</td>\n","      <td>0.085689</td>\n","      <td>0.584755</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.004800</td>\n","      <td>0.007306</td>\n","      <td>0.066749</td>\n","      <td>0.007306</td>\n","      <td>0.085475</td>\n","      <td>0.586832</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.004700</td>\n","      <td>0.007238</td>\n","      <td>0.066850</td>\n","      <td>0.007238</td>\n","      <td>0.085075</td>\n","      <td>0.590685</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.004600</td>\n","      <td>0.007539</td>\n","      <td>0.067842</td>\n","      <td>0.007539</td>\n","      <td>0.086825</td>\n","      <td>0.573675</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.004300</td>\n","      <td>0.007456</td>\n","      <td>0.067518</td>\n","      <td>0.007456</td>\n","      <td>0.086350</td>\n","      <td>0.578322</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.004300</td>\n","      <td>0.007503</td>\n","      <td>0.067546</td>\n","      <td>0.007503</td>\n","      <td>0.086620</td>\n","      <td>0.575683</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.004400</td>\n","      <td>0.008022</td>\n","      <td>0.068750</td>\n","      <td>0.008022</td>\n","      <td>0.089566</td>\n","      <td>0.546332</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.004100</td>\n","      <td>0.007317</td>\n","      <td>0.068169</td>\n","      <td>0.007317</td>\n","      <td>0.085537</td>\n","      <td>0.586226</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.004300</td>\n","      <td>0.008812</td>\n","      <td>0.072710</td>\n","      <td>0.008812</td>\n","      <td>0.093871</td>\n","      <td>0.501676</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.004200</td>\n","      <td>0.007215</td>\n","      <td>0.066247</td>\n","      <td>0.007215</td>\n","      <td>0.084942</td>\n","      <td>0.591967</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.004000</td>\n","      <td>0.007158</td>\n","      <td>0.066129</td>\n","      <td>0.007158</td>\n","      <td>0.084604</td>\n","      <td>0.595201</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.004100</td>\n","      <td>0.007118</td>\n","      <td>0.065678</td>\n","      <td>0.007118</td>\n","      <td>0.084370</td>\n","      <td>0.597443</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.004100</td>\n","      <td>0.008692</td>\n","      <td>0.073208</td>\n","      <td>0.008692</td>\n","      <td>0.093233</td>\n","      <td>0.508422</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.003900</td>\n","      <td>0.007088</td>\n","      <td>0.066150</td>\n","      <td>0.007088</td>\n","      <td>0.084189</td>\n","      <td>0.599168</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.004000</td>\n","      <td>0.007451</td>\n","      <td>0.067578</td>\n","      <td>0.007451</td>\n","      <td>0.086317</td>\n","      <td>0.578646</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.003700</td>\n","      <td>0.007353</td>\n","      <td>0.066473</td>\n","      <td>0.007353</td>\n","      <td>0.085748</td>\n","      <td>0.584186</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.004000</td>\n","      <td>0.007290</td>\n","      <td>0.066557</td>\n","      <td>0.007290</td>\n","      <td>0.085382</td>\n","      <td>0.587725</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.004000</td>\n","      <td>0.007026</td>\n","      <td>0.065745</td>\n","      <td>0.007026</td>\n","      <td>0.083818</td>\n","      <td>0.602689</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.003700</td>\n","      <td>0.006959</td>\n","      <td>0.065248</td>\n","      <td>0.006959</td>\n","      <td>0.083422</td>\n","      <td>0.606435</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.003600</td>\n","      <td>0.007512</td>\n","      <td>0.067477</td>\n","      <td>0.007512</td>\n","      <td>0.086673</td>\n","      <td>0.575161</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.003800</td>\n","      <td>0.007567</td>\n","      <td>0.067028</td>\n","      <td>0.007567</td>\n","      <td>0.086991</td>\n","      <td>0.572045</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.003800</td>\n","      <td>0.007022</td>\n","      <td>0.065356</td>\n","      <td>0.007022</td>\n","      <td>0.083796</td>\n","      <td>0.602901</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.003700</td>\n","      <td>0.007050</td>\n","      <td>0.065284</td>\n","      <td>0.007050</td>\n","      <td>0.083967</td>\n","      <td>0.601278</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.003600</td>\n","      <td>0.007552</td>\n","      <td>0.067175</td>\n","      <td>0.007552</td>\n","      <td>0.086905</td>\n","      <td>0.572889</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.003600</td>\n","      <td>0.007215</td>\n","      <td>0.066141</td>\n","      <td>0.007215</td>\n","      <td>0.084940</td>\n","      <td>0.591982</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.003700</td>\n","      <td>0.007491</td>\n","      <td>0.067646</td>\n","      <td>0.007491</td>\n","      <td>0.086551</td>\n","      <td>0.576360</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.003500</td>\n","      <td>0.007018</td>\n","      <td>0.065869</td>\n","      <td>0.007018</td>\n","      <td>0.083772</td>\n","      <td>0.603132</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.003500</td>\n","      <td>0.007009</td>\n","      <td>0.065447</td>\n","      <td>0.007009</td>\n","      <td>0.083718</td>\n","      <td>0.603634</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.003500</td>\n","      <td>0.006979</td>\n","      <td>0.065637</td>\n","      <td>0.006979</td>\n","      <td>0.083538</td>\n","      <td>0.605345</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.003300</td>\n","      <td>0.007129</td>\n","      <td>0.065689</td>\n","      <td>0.007129</td>\n","      <td>0.084435</td>\n","      <td>0.596817</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.003400</td>\n","      <td>0.007156</td>\n","      <td>0.065970</td>\n","      <td>0.007156</td>\n","      <td>0.084593</td>\n","      <td>0.595309</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.003400</td>\n","      <td>0.007169</td>\n","      <td>0.066715</td>\n","      <td>0.007169</td>\n","      <td>0.084669</td>\n","      <td>0.594580</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.003300</td>\n","      <td>0.007023</td>\n","      <td>0.065208</td>\n","      <td>0.007023</td>\n","      <td>0.083801</td>\n","      <td>0.602855</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.003400</td>\n","      <td>0.007115</td>\n","      <td>0.065947</td>\n","      <td>0.007115</td>\n","      <td>0.084349</td>\n","      <td>0.597637</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.003400</td>\n","      <td>0.007121</td>\n","      <td>0.066085</td>\n","      <td>0.007121</td>\n","      <td>0.084386</td>\n","      <td>0.597290</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.003500</td>\n","      <td>0.007091</td>\n","      <td>0.066189</td>\n","      <td>0.007091</td>\n","      <td>0.084209</td>\n","      <td>0.598979</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.003300</td>\n","      <td>0.007161</td>\n","      <td>0.066493</td>\n","      <td>0.007161</td>\n","      <td>0.084623</td>\n","      <td>0.595026</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.003400</td>\n","      <td>0.007145</td>\n","      <td>0.066175</td>\n","      <td>0.007145</td>\n","      <td>0.084528</td>\n","      <td>0.595930</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.003200</td>\n","      <td>0.007111</td>\n","      <td>0.066507</td>\n","      <td>0.007111</td>\n","      <td>0.084326</td>\n","      <td>0.597862</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.003100</td>\n","      <td>0.007169</td>\n","      <td>0.066337</td>\n","      <td>0.007169</td>\n","      <td>0.084671</td>\n","      <td>0.594565</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.003300</td>\n","      <td>0.006974</td>\n","      <td>0.065715</td>\n","      <td>0.006974</td>\n","      <td>0.083509</td>\n","      <td>0.605617</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.003200</td>\n","      <td>0.007165</td>\n","      <td>0.066612</td>\n","      <td>0.007165</td>\n","      <td>0.084648</td>\n","      <td>0.594783</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.003100</td>\n","      <td>0.007071</td>\n","      <td>0.065889</td>\n","      <td>0.007071</td>\n","      <td>0.084090</td>\n","      <td>0.600113</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.003200</td>\n","      <td>0.007099</td>\n","      <td>0.065988</td>\n","      <td>0.007099</td>\n","      <td>0.084255</td>\n","      <td>0.598537</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.003400</td>\n","      <td>0.007092</td>\n","      <td>0.065976</td>\n","      <td>0.007092</td>\n","      <td>0.084215</td>\n","      <td>0.598919</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.003200</td>\n","      <td>0.007088</td>\n","      <td>0.065889</td>\n","      <td>0.007088</td>\n","      <td>0.084191</td>\n","      <td>0.599146</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.003200</td>\n","      <td>0.007119</td>\n","      <td>0.065948</td>\n","      <td>0.007119</td>\n","      <td>0.084373</td>\n","      <td>0.597413</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.003000</td>\n","      <td>0.007074</td>\n","      <td>0.065950</td>\n","      <td>0.007074</td>\n","      <td>0.084108</td>\n","      <td>0.599941</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 34min 56s, sys: 56.3 s, total: 2h 35min 53s\n","Wall time: 2h 35min 28s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.010169368800632456, metrics={'train_runtime': 9328.5439, 'train_samples_per_second': 64.008, 'train_steps_per_second': 2.005, 'total_flos': 1.1548062831115843e+17, 'train_loss': 0.010169368800632456, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688328489268,"user_tz":300,"elapsed":185,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688328491483,"user_tz":300,"elapsed":1513,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688328497459,"user_tz":300,"elapsed":4420,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"9f08a3a5-ccd7-40d2-9cc1-b89654e871e9"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.008763696067035198,\n"," 'eval_MAE': 0.07236076388233618,\n"," 'eval_MSE': 0.008763696872535316,\n"," 'eval_RMSE': 0.09361461890396881,\n"," 'eval_R2': 0.5238264590790844,\n"," 'eval_runtime': 4.3119,\n"," 'eval_samples_per_second': 173.241,\n"," 'eval_steps_per_second': 21.8,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":22}]}]}