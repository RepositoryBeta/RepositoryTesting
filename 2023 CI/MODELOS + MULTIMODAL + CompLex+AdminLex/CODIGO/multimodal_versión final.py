# -*- coding: utf-8 -*-
"""Multimodal_Prueba.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ektOEpyoozahlUS6yB8d_OOhU4o9cR-p
"""

!pip install multimodal-transformers
!pip install --upgrade accelerate
!pip install transformers accelerate
# !pip install --upgrade numpy

!git clone https://huggingface.co/xlm-roberta-large

from google.colab import drive
drive.mount('/content/drive')

from dataclasses import dataclass, field
import logging
import os
from typing import Optional

import numpy as np
import pandas as pd
from transformers import (
    AutoTokenizer,
    AutoConfig,
    Trainer,
    EvalPrediction,
    set_seed
)

import sklearn.metrics as metrics
from sklearn import metrics
from math import sqrt

from transformers.training_args import TrainingArguments

from multimodal_transformers.data import load_data_from_folder
from multimodal_transformers.model import TabularConfig
from multimodal_transformers.model import RobertaWithTabular
from sklearn.preprocessing import LabelEncoder
import sklearn.metrics as metrics
from sklearn import metrics
from math import sqrt

logging.basicConfig(level=logging.INFO)
os.environ['COMET_MODE'] = 'DISABLED'

DATA_PATH = '/content/drive/MyDrive/Tesis/corpus/CLEXIS2.tsv'
#DATA_PATH = 'Prueba.tsv'

df = pd.read_csv(DATA_PATH, sep='\t')
#df

df['sentence'] = df['sentence'].str.replace('"', '').str.replace("'", '')

df['sentence_token'] = df.apply(
      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),
      axis=1)

#df['sentence_token'].values

#df

train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])
print('Num ejemplos train-val-test')
print(len(train_df), len(val_df), len(test_df))
train_df.to_csv('train.csv')
val_df.to_csv('val.csv')
test_df.to_csv('test.csv')

#train_df

@dataclass
class MultimodalDataTrainingArguments:
  """
  Arguments pertaining to how we combine tabular features
  Using `HfArgumentParser` we can turn this class
  into argparse arguments to be able to specify them on
  the command line.
  """

  data_path: str = field(metadata={
                            'help': 'the path to the csv file containing the dataset'
                        })
  column_info_path: str = field(
      default=None,
      metadata={
          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'
  })

  column_info: dict = field(
      default=None,
      metadata={
          'help': 'a dict referencing the text, categorical, numerical, and label columns'
                  'its keys are text_cols, num_cols, cat_cols, and label_col'
  })

  categorical_encode_type: str = field(default='none',
                                        metadata={
                                            'help': 'sklearn encoder to use for categorical data',
                                            'choices': ['ohe', 'binary', 'label', 'none']
                                        })
  numerical_transformer_method: str = field(default='yeo_johnson',
                                            metadata={
                                                'help': 'sklearn numerical transformer to preprocess numerical data',
                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']
                                            })
  task: str = field(default="regression",
                    metadata={
                        "help": "The downstream training task",
                        "choices": ["classification", "regression"]
                    })

  mlp_division: int = field(default=4,
                            metadata={
                                'help': 'the ratio of the number of '
                                        'hidden dims in a current layer to the next MLP layer'
                            })
  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',
                                    metadata={
                                        'help': 'method to combine categorical and numerical features, '
                                                'see README for all the method'
                                    })
  mlp_dropout: float = field(default=0.1,
                              metadata={
                                'help': 'dropout ratio used for MLP layers'
                              })
  numerical_bn: bool = field(default=True,
                              metadata={
                                  'help': 'whether to use batchnorm on numerical features'
                              })
  use_simple_classifier: str = field(default=False,
                                      metadata={
                                          'help': 'whether to use single layer or MLP as final classifier'
                                      })
  mlp_act: str = field(default='relu',
                        metadata={
                            'help': 'the activation function to use for finetuning layers',
                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']
                        })
  gating_beta: float = field(default=0.2,
                              metadata={
                                  'help': "the beta hyperparameters used for gating tabular data "
                                          "see https://www.aclweb.org/anthology/2020.acl-main.214.pdf"
                              })

model_name = 'xlm-roberta-large'

column_info_dict = {
    'text_cols': ['corpus','sentence_token'],
    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',
                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',
                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],
    'label_col': ['complexity']
}

data_args = MultimodalDataTrainingArguments(
    data_path='.',
    #combine_feat_method='text_only',
    combine_feat_method='concat',
    #combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',
    #combine_feat_method='attention_on_cat_and_numerical_feats',
    #combine_feat_method='gating_on_cat_and_num_feats_then_sum',
    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',
    column_info=column_info_dict,
    task='regression',
)

training_args = TrainingArguments(
    output_dir="./logs/model_name",
    logging_dir="./logs/runs",
    overwrite_output_dir=True,
    do_train=True,
    do_eval=True,
    per_device_train_batch_size=32,
    num_train_epochs=50,
    evaluation_strategy='epoch',
    logging_strategy='epoch',
    logging_steps=16,
    eval_steps=5
)

set_seed(training_args.seed)

tokenizer = AutoTokenizer.from_pretrained(model_name)
print('Specified tokenizer: ', model_name)

# Get Datasets
train_dataset, val_dataset, test_dataset = load_data_from_folder(
    data_args.data_path,
    data_args.column_info['text_cols'],
    tokenizer,
    label_col=data_args.column_info['label_col'],
    label_list = None,
    categorical_cols = None,
    numerical_transformer_method = 'yeo_johnson',
    numerical_cols=data_args.column_info['num_cols'],
    sep_text_token_str=' </s> ',
    categorical_encode_type = None
)

# num_labels = len(np.unique(train_dataset.labels))
# num_labels

config = AutoConfig.from_pretrained(model_name)
tabular_config = TabularConfig(num_labels=1,
                               #cat_feat_dim=train_dataset.cat_feats.shape[1],
                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],
                               **vars(data_args))
config.tabular_config = tabular_config

model = RobertaWithTabular.from_pretrained(
        model_name,
        config=config
    )

def calc_regression_metrics(p: EvalPrediction):
    predictions = p.predictions[0]
    preds = np.squeeze(predictions)
    labels = np.squeeze(p.label_ids)
    mse = metrics.mean_squared_error(labels, preds)
    rmse = sqrt(mse)
    mae = metrics.mean_absolute_error(labels, preds)
    return {
        "MAE": mae,
        "MSE": mse,
        "RMSE": rmse,
        'R2': metrics.r2_score(labels, preds)
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=calc_regression_metrics
)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trainer.train()

OUTPUT_PATH = '/content/drive/MyDrive/Tesis/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]
trainer.save_model(OUTPUT_PATH)

"""**EVALUAMOS EL MODELO**"""

#trainer.evaluate(test_dataset)

"""**Generamos Predicciones**"""

# predictions = trainer.predict(test_dataset)

# # Obtener las predicciones y las etiquetas reales del objeto de predicción
# pred_labels = predictions.predictions[0]
# true_labels = predictions.label_ids

# def calc_regression_metrics(predictions, labels):
#     preds = np.squeeze(predictions)
#     labels = np.squeeze(labels)
#     mse = metrics.mean_squared_error(labels, preds)
#     rmse = sqrt(mse)
#     mae = metrics.mean_absolute_error(labels, preds)
#     return {
#         "mse": mse,
#         "rmse": rmse,
#         "mae": mae,
#     }

# # Evaluar el rendimiento del modelo en las predicciones
# evaluation = calc_regression_metrics(pred_labels, true_labels)

# # Imprimir los resultados de la evaluación
# print("MSE:", evaluation["mse"])
# print("RMSE:", evaluation["rmse"])
# print("MAE:", evaluation["mae"])