{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688328871865,"user_tz":300,"elapsed":18673,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"1375ff94-4018-42e0-a70e-4295711316a5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688328878404,"user_tz":300,"elapsed":6542,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688328879592,"user_tz":300,"elapsed":5,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688328885935,"user_tz":300,"elapsed":4846,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688328887834,"user_tz":300,"elapsed":386,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688328889592,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688328891868,"user_tz":300,"elapsed":386,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"0a1dfba3-58e1-43c0-8674-0ca27d96d6ca"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688328895016,"user_tz":300,"elapsed":387,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    # combine_feat_method='text_only',\n","    # combine_feat_method='concat',\n","    # combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    # combine_feat_method='attention_on_cat_and_numerical_feats',\n","    combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688328902072,"user_tz":300,"elapsed":388,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688328905549,"user_tz":300,"elapsed":4,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"d63cd878-04c9-4ac4-94d5-766437d83b43"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688328910007,"user_tz":300,"elapsed":1582,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"9acd9a3d-4fa5-4f9b-8858-075aca9efbf1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688328912784,"user_tz":300,"elapsed":377,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688328916050,"user_tz":300,"elapsed":1286,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"bcc5d241-e015-4c93-b349-61a80d5141ce"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_classifier.bn.1.bias', 'tabular_classifier.bn.0.running_var', 'tabular_combiner.h_num_layer.weight', 'tabular_classifier.layers.0.bias', 'tabular_classifier.bn.0.weight', 'tabular_classifier.bn.1.num_batches_tracked', 'tabular_classifier.bn.0.bias', 'tabular_combiner.num_bn.bias', 'tabular_classifier.layers.4.weight', 'tabular_combiner.g_num_layer.bias', 'tabular_combiner.num_bn.running_var', 'tabular_classifier.layers.1.weight', 'classifier.out_proj.bias', 'tabular_combiner.h_bias', 'tabular_classifier.layers.4.bias', 'tabular_combiner.num_bn.running_mean', 'tabular_classifier.layers.1.bias', 'tabular_combiner.layer_norm.weight', 'tabular_classifier.bn.1.running_var', 'tabular_combiner.num_bn.weight', 'tabular_classifier.bn.1.running_mean', 'tabular_classifier.bn.2.running_mean', 'tabular_classifier.bn.1.weight', 'tabular_classifier.bn.0.running_mean', 'tabular_classifier.layers.3.weight', 'tabular_classifier.bn.2.weight', 'classifier.out_proj.weight', 'tabular_classifier.bn.3.num_batches_tracked', 'tabular_classifier.layers.3.bias', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_classifier.bn.3.bias', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_classifier.bn.3.running_mean', 'tabular_classifier.layers.2.weight', 'tabular_classifier.bn.2.running_var', 'tabular_combiner.g_num_layer.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'tabular_combiner.layer_norm.bias', 'tabular_classifier.bn.3.weight', 'tabular_classifier.bn.2.bias', 'tabular_classifier.layers.2.bias', 'tabular_classifier.bn.3.running_var', 'tabular_classifier.layers.0.weight', 'tabular_combiner.num_bn.num_batches_tracked']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688328919922,"user_tz":300,"elapsed":386,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688328925817,"user_tz":300,"elapsed":4534,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688328927042,"user_tz":300,"elapsed":6,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"462e61a0-4377-46b2-9ad2-8c71453f4274"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'gating_on_cat_and_num_feats_then_sum'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688338264367,"user_tz":300,"elapsed":9334663,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"183227a2-0ad8-4e4a-9f55-4db37575680e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:35:31, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.108100</td>\n","      <td>0.107120</td>\n","      <td>0.297978</td>\n","      <td>0.107120</td>\n","      <td>0.327293</td>\n","      <td>-4.530833</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.095700</td>\n","      <td>0.084913</td>\n","      <td>0.258295</td>\n","      <td>0.084913</td>\n","      <td>0.291398</td>\n","      <td>-3.384200</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.085500</td>\n","      <td>0.093232</td>\n","      <td>0.274123</td>\n","      <td>0.093232</td>\n","      <td>0.305340</td>\n","      <td>-3.813760</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.075200</td>\n","      <td>0.067523</td>\n","      <td>0.222466</td>\n","      <td>0.067523</td>\n","      <td>0.259851</td>\n","      <td>-2.486323</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.067100</td>\n","      <td>0.071895</td>\n","      <td>0.233960</td>\n","      <td>0.071895</td>\n","      <td>0.268133</td>\n","      <td>-2.712097</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.058500</td>\n","      <td>0.052241</td>\n","      <td>0.191409</td>\n","      <td>0.052241</td>\n","      <td>0.228562</td>\n","      <td>-1.697286</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.050900</td>\n","      <td>0.048543</td>\n","      <td>0.182336</td>\n","      <td>0.048543</td>\n","      <td>0.220325</td>\n","      <td>-1.506369</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.044200</td>\n","      <td>0.046509</td>\n","      <td>0.177545</td>\n","      <td>0.046509</td>\n","      <td>0.215660</td>\n","      <td>-1.401356</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.038400</td>\n","      <td>0.039704</td>\n","      <td>0.160363</td>\n","      <td>0.039704</td>\n","      <td>0.199258</td>\n","      <td>-1.049983</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.033500</td>\n","      <td>0.034827</td>\n","      <td>0.147088</td>\n","      <td>0.034827</td>\n","      <td>0.186619</td>\n","      <td>-0.798167</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.029800</td>\n","      <td>0.030641</td>\n","      <td>0.135250</td>\n","      <td>0.030641</td>\n","      <td>0.175045</td>\n","      <td>-0.582042</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.026500</td>\n","      <td>0.027529</td>\n","      <td>0.126179</td>\n","      <td>0.027529</td>\n","      <td>0.165920</td>\n","      <td>-0.421391</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.023900</td>\n","      <td>0.024217</td>\n","      <td>0.117359</td>\n","      <td>0.024217</td>\n","      <td>0.155617</td>\n","      <td>-0.250349</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.022100</td>\n","      <td>0.022909</td>\n","      <td>0.112874</td>\n","      <td>0.022909</td>\n","      <td>0.151356</td>\n","      <td>-0.182820</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.020400</td>\n","      <td>0.022432</td>\n","      <td>0.111830</td>\n","      <td>0.022432</td>\n","      <td>0.149772</td>\n","      <td>-0.158196</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.019500</td>\n","      <td>0.020003</td>\n","      <td>0.105566</td>\n","      <td>0.020003</td>\n","      <td>0.141432</td>\n","      <td>-0.032788</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.019000</td>\n","      <td>0.017888</td>\n","      <td>0.099946</td>\n","      <td>0.017888</td>\n","      <td>0.133745</td>\n","      <td>0.076424</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.018600</td>\n","      <td>0.018631</td>\n","      <td>0.101463</td>\n","      <td>0.018631</td>\n","      <td>0.136497</td>\n","      <td>0.038030</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.018400</td>\n","      <td>0.018225</td>\n","      <td>0.100599</td>\n","      <td>0.018225</td>\n","      <td>0.134998</td>\n","      <td>0.059030</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.018400</td>\n","      <td>0.018445</td>\n","      <td>0.100877</td>\n","      <td>0.018445</td>\n","      <td>0.135814</td>\n","      <td>0.047632</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.017700</td>\n","      <td>0.018015</td>\n","      <td>0.100005</td>\n","      <td>0.018015</td>\n","      <td>0.134220</td>\n","      <td>0.069851</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.017800</td>\n","      <td>0.017836</td>\n","      <td>0.099973</td>\n","      <td>0.017836</td>\n","      <td>0.133551</td>\n","      <td>0.079101</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.017100</td>\n","      <td>0.017448</td>\n","      <td>0.099887</td>\n","      <td>0.017448</td>\n","      <td>0.132092</td>\n","      <td>0.099109</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.017000</td>\n","      <td>0.017755</td>\n","      <td>0.100139</td>\n","      <td>0.017755</td>\n","      <td>0.133247</td>\n","      <td>0.083292</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.017400</td>\n","      <td>0.017334</td>\n","      <td>0.099195</td>\n","      <td>0.017334</td>\n","      <td>0.131660</td>\n","      <td>0.104988</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.017000</td>\n","      <td>0.017378</td>\n","      <td>0.098942</td>\n","      <td>0.017378</td>\n","      <td>0.131824</td>\n","      <td>0.102756</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.017200</td>\n","      <td>0.017297</td>\n","      <td>0.098688</td>\n","      <td>0.017297</td>\n","      <td>0.131516</td>\n","      <td>0.106948</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.017000</td>\n","      <td>0.017086</td>\n","      <td>0.098978</td>\n","      <td>0.017086</td>\n","      <td>0.130713</td>\n","      <td>0.117819</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.016500</td>\n","      <td>0.016878</td>\n","      <td>0.097150</td>\n","      <td>0.016878</td>\n","      <td>0.129916</td>\n","      <td>0.128543</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.016700</td>\n","      <td>0.017689</td>\n","      <td>0.098931</td>\n","      <td>0.017689</td>\n","      <td>0.133001</td>\n","      <td>0.086665</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.017000</td>\n","      <td>0.017218</td>\n","      <td>0.098817</td>\n","      <td>0.017218</td>\n","      <td>0.131216</td>\n","      <td>0.111015</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.016600</td>\n","      <td>0.017185</td>\n","      <td>0.098699</td>\n","      <td>0.017185</td>\n","      <td>0.131091</td>\n","      <td>0.112710</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.016400</td>\n","      <td>0.017001</td>\n","      <td>0.099299</td>\n","      <td>0.017001</td>\n","      <td>0.130389</td>\n","      <td>0.122197</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.016800</td>\n","      <td>0.016878</td>\n","      <td>0.098518</td>\n","      <td>0.016878</td>\n","      <td>0.129914</td>\n","      <td>0.128571</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.016300</td>\n","      <td>0.016737</td>\n","      <td>0.097866</td>\n","      <td>0.016737</td>\n","      <td>0.129372</td>\n","      <td>0.135834</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.016200</td>\n","      <td>0.016853</td>\n","      <td>0.098142</td>\n","      <td>0.016853</td>\n","      <td>0.129820</td>\n","      <td>0.129835</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.016300</td>\n","      <td>0.016449</td>\n","      <td>0.097151</td>\n","      <td>0.016449</td>\n","      <td>0.128255</td>\n","      <td>0.150682</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.016000</td>\n","      <td>0.016402</td>\n","      <td>0.096624</td>\n","      <td>0.016402</td>\n","      <td>0.128069</td>\n","      <td>0.153145</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.015900</td>\n","      <td>0.016465</td>\n","      <td>0.097047</td>\n","      <td>0.016465</td>\n","      <td>0.128316</td>\n","      <td>0.149885</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.015700</td>\n","      <td>0.016406</td>\n","      <td>0.097119</td>\n","      <td>0.016406</td>\n","      <td>0.128087</td>\n","      <td>0.152915</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.015800</td>\n","      <td>0.016239</td>\n","      <td>0.096343</td>\n","      <td>0.016239</td>\n","      <td>0.127432</td>\n","      <td>0.161547</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.015800</td>\n","      <td>0.016063</td>\n","      <td>0.096242</td>\n","      <td>0.016063</td>\n","      <td>0.126741</td>\n","      <td>0.170622</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.015800</td>\n","      <td>0.015792</td>\n","      <td>0.096389</td>\n","      <td>0.015792</td>\n","      <td>0.125666</td>\n","      <td>0.184636</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.015400</td>\n","      <td>0.015838</td>\n","      <td>0.096032</td>\n","      <td>0.015838</td>\n","      <td>0.125849</td>\n","      <td>0.182257</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.015600</td>\n","      <td>0.015942</td>\n","      <td>0.096258</td>\n","      <td>0.015942</td>\n","      <td>0.126260</td>\n","      <td>0.176904</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.015500</td>\n","      <td>0.015934</td>\n","      <td>0.095666</td>\n","      <td>0.015934</td>\n","      <td>0.126229</td>\n","      <td>0.177310</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.015600</td>\n","      <td>0.015806</td>\n","      <td>0.095647</td>\n","      <td>0.015806</td>\n","      <td>0.125721</td>\n","      <td>0.183916</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.015400</td>\n","      <td>0.016035</td>\n","      <td>0.095710</td>\n","      <td>0.016035</td>\n","      <td>0.126629</td>\n","      <td>0.172089</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.015400</td>\n","      <td>0.016082</td>\n","      <td>0.096065</td>\n","      <td>0.016082</td>\n","      <td>0.126816</td>\n","      <td>0.169635</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.015400</td>\n","      <td>0.015592</td>\n","      <td>0.095760</td>\n","      <td>0.015592</td>\n","      <td>0.124866</td>\n","      <td>0.194981</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.015500</td>\n","      <td>0.015365</td>\n","      <td>0.095590</td>\n","      <td>0.015365</td>\n","      <td>0.123955</td>\n","      <td>0.206679</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.015700</td>\n","      <td>0.015401</td>\n","      <td>0.095921</td>\n","      <td>0.015401</td>\n","      <td>0.124102</td>\n","      <td>0.204804</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.015400</td>\n","      <td>0.015655</td>\n","      <td>0.094873</td>\n","      <td>0.015655</td>\n","      <td>0.125119</td>\n","      <td>0.191718</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.015400</td>\n","      <td>0.015392</td>\n","      <td>0.094977</td>\n","      <td>0.015392</td>\n","      <td>0.124065</td>\n","      <td>0.205278</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.015500</td>\n","      <td>0.015597</td>\n","      <td>0.095368</td>\n","      <td>0.015597</td>\n","      <td>0.124888</td>\n","      <td>0.194697</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.015300</td>\n","      <td>0.015302</td>\n","      <td>0.094753</td>\n","      <td>0.015302</td>\n","      <td>0.123703</td>\n","      <td>0.209909</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.015200</td>\n","      <td>0.015868</td>\n","      <td>0.095003</td>\n","      <td>0.015868</td>\n","      <td>0.125969</td>\n","      <td>0.180701</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.015000</td>\n","      <td>0.015705</td>\n","      <td>0.095028</td>\n","      <td>0.015705</td>\n","      <td>0.125320</td>\n","      <td>0.189121</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.015300</td>\n","      <td>0.015400</td>\n","      <td>0.095046</td>\n","      <td>0.015400</td>\n","      <td>0.124099</td>\n","      <td>0.204844</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.015200</td>\n","      <td>0.015354</td>\n","      <td>0.095104</td>\n","      <td>0.015354</td>\n","      <td>0.123910</td>\n","      <td>0.207259</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.015300</td>\n","      <td>0.015821</td>\n","      <td>0.095328</td>\n","      <td>0.015821</td>\n","      <td>0.125782</td>\n","      <td>0.183119</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.015000</td>\n","      <td>0.015273</td>\n","      <td>0.095093</td>\n","      <td>0.015273</td>\n","      <td>0.123583</td>\n","      <td>0.211443</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.015100</td>\n","      <td>0.015337</td>\n","      <td>0.094667</td>\n","      <td>0.015337</td>\n","      <td>0.123844</td>\n","      <td>0.208100</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.014900</td>\n","      <td>0.015281</td>\n","      <td>0.094998</td>\n","      <td>0.015281</td>\n","      <td>0.123617</td>\n","      <td>0.211000</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.015000</td>\n","      <td>0.015263</td>\n","      <td>0.094748</td>\n","      <td>0.015263</td>\n","      <td>0.123543</td>\n","      <td>0.211943</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.015100</td>\n","      <td>0.015272</td>\n","      <td>0.094537</td>\n","      <td>0.015272</td>\n","      <td>0.123580</td>\n","      <td>0.211482</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.015100</td>\n","      <td>0.015465</td>\n","      <td>0.094676</td>\n","      <td>0.015465</td>\n","      <td>0.124357</td>\n","      <td>0.201525</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.014600</td>\n","      <td>0.015184</td>\n","      <td>0.094623</td>\n","      <td>0.015184</td>\n","      <td>0.123221</td>\n","      <td>0.216045</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.014800</td>\n","      <td>0.015540</td>\n","      <td>0.094651</td>\n","      <td>0.015540</td>\n","      <td>0.124659</td>\n","      <td>0.197648</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.014700</td>\n","      <td>0.015299</td>\n","      <td>0.094617</td>\n","      <td>0.015299</td>\n","      <td>0.123687</td>\n","      <td>0.210108</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.014800</td>\n","      <td>0.015067</td>\n","      <td>0.094504</td>\n","      <td>0.015067</td>\n","      <td>0.122746</td>\n","      <td>0.222087</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.014700</td>\n","      <td>0.015199</td>\n","      <td>0.094307</td>\n","      <td>0.015199</td>\n","      <td>0.123283</td>\n","      <td>0.215263</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.014800</td>\n","      <td>0.015202</td>\n","      <td>0.094318</td>\n","      <td>0.015202</td>\n","      <td>0.123296</td>\n","      <td>0.215099</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.014800</td>\n","      <td>0.015276</td>\n","      <td>0.094726</td>\n","      <td>0.015276</td>\n","      <td>0.123594</td>\n","      <td>0.211295</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.015000</td>\n","      <td>0.015182</td>\n","      <td>0.094166</td>\n","      <td>0.015182</td>\n","      <td>0.123217</td>\n","      <td>0.216104</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.014900</td>\n","      <td>0.015558</td>\n","      <td>0.094741</td>\n","      <td>0.015558</td>\n","      <td>0.124730</td>\n","      <td>0.196734</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.014800</td>\n","      <td>0.015304</td>\n","      <td>0.094022</td>\n","      <td>0.015304</td>\n","      <td>0.123708</td>\n","      <td>0.209841</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.014700</td>\n","      <td>0.015107</td>\n","      <td>0.094271</td>\n","      <td>0.015107</td>\n","      <td>0.122909</td>\n","      <td>0.220021</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.014700</td>\n","      <td>0.015004</td>\n","      <td>0.094222</td>\n","      <td>0.015004</td>\n","      <td>0.122489</td>\n","      <td>0.225339</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.014700</td>\n","      <td>0.015155</td>\n","      <td>0.094083</td>\n","      <td>0.015155</td>\n","      <td>0.123104</td>\n","      <td>0.217542</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.014600</td>\n","      <td>0.015344</td>\n","      <td>0.094385</td>\n","      <td>0.015344</td>\n","      <td>0.123869</td>\n","      <td>0.207783</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.014700</td>\n","      <td>0.015152</td>\n","      <td>0.094321</td>\n","      <td>0.015152</td>\n","      <td>0.123092</td>\n","      <td>0.217685</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.014400</td>\n","      <td>0.015136</td>\n","      <td>0.094390</td>\n","      <td>0.015136</td>\n","      <td>0.123030</td>\n","      <td>0.218483</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.014800</td>\n","      <td>0.015079</td>\n","      <td>0.094164</td>\n","      <td>0.015079</td>\n","      <td>0.122797</td>\n","      <td>0.221439</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.014800</td>\n","      <td>0.015302</td>\n","      <td>0.094312</td>\n","      <td>0.015302</td>\n","      <td>0.123703</td>\n","      <td>0.209903</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.014800</td>\n","      <td>0.015178</td>\n","      <td>0.094392</td>\n","      <td>0.015178</td>\n","      <td>0.123199</td>\n","      <td>0.216333</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.014400</td>\n","      <td>0.015022</td>\n","      <td>0.093994</td>\n","      <td>0.015022</td>\n","      <td>0.122563</td>\n","      <td>0.224407</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.014600</td>\n","      <td>0.015164</td>\n","      <td>0.093891</td>\n","      <td>0.015164</td>\n","      <td>0.123140</td>\n","      <td>0.217078</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.014800</td>\n","      <td>0.015548</td>\n","      <td>0.094183</td>\n","      <td>0.015548</td>\n","      <td>0.124693</td>\n","      <td>0.197214</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.014700</td>\n","      <td>0.015236</td>\n","      <td>0.094014</td>\n","      <td>0.015236</td>\n","      <td>0.123435</td>\n","      <td>0.213321</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.014600</td>\n","      <td>0.015314</td>\n","      <td>0.094388</td>\n","      <td>0.015314</td>\n","      <td>0.123751</td>\n","      <td>0.209287</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.014600</td>\n","      <td>0.015020</td>\n","      <td>0.094424</td>\n","      <td>0.015020</td>\n","      <td>0.122558</td>\n","      <td>0.224470</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.014600</td>\n","      <td>0.015049</td>\n","      <td>0.094469</td>\n","      <td>0.015049</td>\n","      <td>0.122673</td>\n","      <td>0.223012</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.014600</td>\n","      <td>0.015183</td>\n","      <td>0.093949</td>\n","      <td>0.015183</td>\n","      <td>0.123221</td>\n","      <td>0.216052</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.014500</td>\n","      <td>0.015530</td>\n","      <td>0.094133</td>\n","      <td>0.015530</td>\n","      <td>0.124621</td>\n","      <td>0.198134</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.014500</td>\n","      <td>0.014995</td>\n","      <td>0.094049</td>\n","      <td>0.014995</td>\n","      <td>0.122453</td>\n","      <td>0.225788</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.014700</td>\n","      <td>0.014959</td>\n","      <td>0.093893</td>\n","      <td>0.014959</td>\n","      <td>0.122307</td>\n","      <td>0.227642</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.014500</td>\n","      <td>0.015208</td>\n","      <td>0.094242</td>\n","      <td>0.015208</td>\n","      <td>0.123322</td>\n","      <td>0.214766</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.014600</td>\n","      <td>0.015240</td>\n","      <td>0.093814</td>\n","      <td>0.015240</td>\n","      <td>0.123449</td>\n","      <td>0.213152</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.014400</td>\n","      <td>0.015017</td>\n","      <td>0.093868</td>\n","      <td>0.015017</td>\n","      <td>0.122545</td>\n","      <td>0.224628</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 35min 7s, sys: 57.6 s, total: 2h 36min 4s\n","Wall time: 2h 35min 34s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.021074543203262085, metrics={'train_runtime': 9334.4151, 'train_samples_per_second': 63.968, 'train_steps_per_second': 2.003, 'total_flos': 1.1550320660153923e+17, 'train_loss': 0.021074543203262085, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688338271923,"user_tz":300,"elapsed":576,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688338275262,"user_tz":300,"elapsed":1930,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688338279947,"user_tz":300,"elapsed":3198,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"c660929a-e78d-4933-adab-ed22815a1c4b"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.013952775858342648,\n"," 'eval_MAE': 0.08893420376386928,\n"," 'eval_MSE': 0.013952776320168845,\n"," 'eval_RMSE': 0.11812187062592958,\n"," 'eval_R2': 0.21143751524858068,\n"," 'eval_runtime': 2.8406,\n"," 'eval_samples_per_second': 262.977,\n"," 'eval_steps_per_second': 33.092,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":22}]}]}