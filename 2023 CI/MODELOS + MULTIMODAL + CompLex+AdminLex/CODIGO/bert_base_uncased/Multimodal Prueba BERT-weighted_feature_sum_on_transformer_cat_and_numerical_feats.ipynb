{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pz7k0_0kkS5M"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!pip list"],"metadata":{"id":"qm-aVLl9Cwyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rJlQbY2ZkTFA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148307930,"user_tz":300,"elapsed":20068,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"f860db43-bb2f-4602-a49a-c27796b7e10d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'bert-base-uncased'...\n","remote: Enumerating objects: 79, done.\u001b[K\n","remote: Total 79 (delta 0), reused 0 (delta 0), pack-reused 79\u001b[K\n","Unpacking objects: 100% (79/79), 329.15 KiB | 4.01 MiB/s, done.\n","Filtering content: 100% (6/6), 2.72 GiB | 144.93 MiB/s, done.\n"]}],"source":["!git clone https://huggingface.co/bert-base-uncased"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"j6SpKf5-kTPt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148499701,"user_tz":300,"elapsed":27889,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"5faec02b-0c06-4d08-cb1d-16c5f1d3f9cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-4OdmjEmkx_2","executionInfo":{"status":"ok","timestamp":1687148452084,"user_tz":300,"elapsed":7456,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","\n","# from scipy.special import softmax\n","# from sklearn.metrics import (\n","#     f1_score,\n","#     matthews_corrcoef\n","# )\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import BertWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"]},{"cell_type":"code","source":["SEP = '[SEP]'"],"metadata":{"id":"qJB32ds_Gzol","executionInfo":{"status":"ok","timestamp":1687148455228,"user_tz":300,"elapsed":2,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pUNx4DEykTZ7","executionInfo":{"status":"ok","timestamp":1687148461703,"user_tz":300,"elapsed":181,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["# DATA_PATH = '/content/drive/MyDrive/CompLex - Multimodal/corpus/Complex.xlsx'\n","DATA_PATH = \"Complex.xlsx\""]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NMMABH_jlGIj","executionInfo":{"status":"ok","timestamp":1687148466971,"user_tz":300,"elapsed":2811,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["df = pd.read_excel(DATA_PATH)\n","#df"]},{"cell_type":"code","source":["df"],"metadata":{"id":"oUeQPX_Y2T4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tUfzsIs8opou"},"outputs":[],"source":["#label_encoder = LabelEncoder()\n","#id2label = list(set(df['complexity'].to_numpy()))\n","#df['complexity'] = label_encoder.fit_transform(df['complexity'])"]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"dVpGHUa8gneg","executionInfo":{"status":"ok","timestamp":1687148523330,"user_tz":300,"elapsed":170,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ f' {SEP} ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"oUNNu6df9euo","executionInfo":{"status":"ok","timestamp":1687148527482,"user_tz":300,"elapsed":171,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'].values"],"metadata":{"id":"z1Rnw_ch-xfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"NNsXjCsC_0te"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"tezRhiHWjUGl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148532679,"user_tz":300,"elapsed":367,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"cfee58ef-b2c5-4761-a76d-d0240dc3a4c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}],"source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bh1rU3pljXn6"},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EmXhPdUoH-VT"},"outputs":[],"source":["#id2label"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"H8q8-SuJl8id","executionInfo":{"status":"ok","timestamp":1687148534893,"user_tz":300,"elapsed":2,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=True,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"qAvJ1yEkl8ei","executionInfo":{"status":"ok","timestamp":1687148560168,"user_tz":300,"elapsed":204,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["model_name = 'bert-base-uncased'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms','number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before','len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=50,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Dt-54uLMl8Z8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148562229,"user_tz":300,"elapsed":206,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"c6e365ac-7572-4443-bffc-a85d8f4f045b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  bert-base-uncased\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"GdXlDXnOl8U7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148565603,"user_tz":300,"elapsed":2112,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"c2a3541b-716d-476a-e4ca-f36ca4b76f69"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}],"source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=SEP,\n","    categorical_encode_type = None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEm2S8lMl8PJ"},"outputs":[],"source":["# num_labels = len(np.unique(train_dataset.labels))\n","# num_labels"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"WCSUKhVpl8Bm","executionInfo":{"status":"ok","timestamp":1687148567300,"user_tz":300,"elapsed":184,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cwphF_FDe5oQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687148570057,"user_tz":300,"elapsed":1061,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"9970f261-4806-4c5b-ff52-679986d1bd2d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertWithTabular: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertWithTabular were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['tabular_combiner.num_bn.running_var', 'tabular_combiner.num_bn.weight', 'tabular_combiner.num_layer.bias', 'tabular_combiner.num_bn.bias', 'classifier.bias', 'tabular_combiner.layer_norm.bias', 'tabular_classifier.bias', 'tabular_combiner.num_layer.weight', 'tabular_classifier.weight', 'classifier.weight', 'tabular_combiner.num_bn.running_mean', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_combiner.layer_norm.weight', 'tabular_combiner.weight_num']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model = BertWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPCqgE7we53n"},"outputs":[],"source":["# def calc_classification_metrics(p: EvalPrediction):\n","#   pred_labels = np.argmax(p.predictions[0], axis=1)\n","#   pred_scores = softmax(p.predictions[0], axis=1)[:, 1]\n","#   labels = p.label_ids\n","\n","#   acc = (pred_labels == labels).mean()\n","#   f1 = f1_score(y_true=labels, y_pred=pred_labels, average='micro')\n","#   result = {\n","#       \"acc\": acc,\n","#       \"f1\": f1,\n","#       \"acc_and_f1\": (acc + f1) / 2,\n","#       \"mcc\": matthews_corrcoef(labels, pred_labels)\n","#   }\n","\n","#   return result"]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"-QnbPR8yE_c3","executionInfo":{"status":"ok","timestamp":1687148572836,"user_tz":300,"elapsed":209,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"nsfC583le9rJ","executionInfo":{"status":"ok","timestamp":1687148581162,"user_tz":300,"elapsed":4623,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ne82B8C-5QXs","executionInfo":{"status":"ok","timestamp":1687148631417,"user_tz":300,"elapsed":186,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"699dee57-388f-4142-8bfc-d14a412d5d58"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'weighted_feature_sum_on_transformer_cat_and_numerical_feats'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"6ZSY3UImqrwV","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4076290a-acb8-4976-ba32-a4cc27048a07","executionInfo":{"status":"ok","timestamp":1687152562816,"user_tz":300,"elapsed":3928362,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='9350' max='9350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9350/9350 1:05:24, Epoch 50/50]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.018400</td>\n","      <td>0.008259</td>\n","      <td>0.070330</td>\n","      <td>0.008259</td>\n","      <td>0.090877</td>\n","      <td>0.543259</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.007900</td>\n","      <td>0.007634</td>\n","      <td>0.066336</td>\n","      <td>0.007634</td>\n","      <td>0.087372</td>\n","      <td>0.577808</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.005800</td>\n","      <td>0.007862</td>\n","      <td>0.069415</td>\n","      <td>0.007862</td>\n","      <td>0.088670</td>\n","      <td>0.565172</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.004800</td>\n","      <td>0.010436</td>\n","      <td>0.077479</td>\n","      <td>0.010436</td>\n","      <td>0.102158</td>\n","      <td>0.422825</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.003800</td>\n","      <td>0.008203</td>\n","      <td>0.069717</td>\n","      <td>0.008203</td>\n","      <td>0.090571</td>\n","      <td>0.546329</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.003100</td>\n","      <td>0.008930</td>\n","      <td>0.073338</td>\n","      <td>0.008930</td>\n","      <td>0.094500</td>\n","      <td>0.506117</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.002400</td>\n","      <td>0.008847</td>\n","      <td>0.074528</td>\n","      <td>0.008847</td>\n","      <td>0.094057</td>\n","      <td>0.510732</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.002100</td>\n","      <td>0.008575</td>\n","      <td>0.072527</td>\n","      <td>0.008575</td>\n","      <td>0.092599</td>\n","      <td>0.525784</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.001700</td>\n","      <td>0.008719</td>\n","      <td>0.074419</td>\n","      <td>0.008719</td>\n","      <td>0.093373</td>\n","      <td>0.517822</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.001500</td>\n","      <td>0.008377</td>\n","      <td>0.072560</td>\n","      <td>0.008377</td>\n","      <td>0.091525</td>\n","      <td>0.536727</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.001300</td>\n","      <td>0.008029</td>\n","      <td>0.070397</td>\n","      <td>0.008029</td>\n","      <td>0.089604</td>\n","      <td>0.555965</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.001300</td>\n","      <td>0.007953</td>\n","      <td>0.069512</td>\n","      <td>0.007953</td>\n","      <td>0.089178</td>\n","      <td>0.560180</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.001100</td>\n","      <td>0.007931</td>\n","      <td>0.070336</td>\n","      <td>0.007931</td>\n","      <td>0.089054</td>\n","      <td>0.561404</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.001100</td>\n","      <td>0.008646</td>\n","      <td>0.074163</td>\n","      <td>0.008646</td>\n","      <td>0.092983</td>\n","      <td>0.521847</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.001000</td>\n","      <td>0.007745</td>\n","      <td>0.068869</td>\n","      <td>0.007745</td>\n","      <td>0.088003</td>\n","      <td>0.571690</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.000900</td>\n","      <td>0.008232</td>\n","      <td>0.071578</td>\n","      <td>0.008232</td>\n","      <td>0.090729</td>\n","      <td>0.544740</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.000900</td>\n","      <td>0.008094</td>\n","      <td>0.070406</td>\n","      <td>0.008094</td>\n","      <td>0.089965</td>\n","      <td>0.552381</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.000900</td>\n","      <td>0.007940</td>\n","      <td>0.069957</td>\n","      <td>0.007940</td>\n","      <td>0.089106</td>\n","      <td>0.560884</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.000800</td>\n","      <td>0.007767</td>\n","      <td>0.069668</td>\n","      <td>0.007767</td>\n","      <td>0.088133</td>\n","      <td>0.570423</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.000700</td>\n","      <td>0.007993</td>\n","      <td>0.070574</td>\n","      <td>0.007993</td>\n","      <td>0.089405</td>\n","      <td>0.557938</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.000700</td>\n","      <td>0.008330</td>\n","      <td>0.072245</td>\n","      <td>0.008330</td>\n","      <td>0.091270</td>\n","      <td>0.539296</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.000700</td>\n","      <td>0.007662</td>\n","      <td>0.068646</td>\n","      <td>0.007662</td>\n","      <td>0.087534</td>\n","      <td>0.576249</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.000600</td>\n","      <td>0.008050</td>\n","      <td>0.070915</td>\n","      <td>0.008050</td>\n","      <td>0.089719</td>\n","      <td>0.554820</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.000600</td>\n","      <td>0.007784</td>\n","      <td>0.068900</td>\n","      <td>0.007784</td>\n","      <td>0.088225</td>\n","      <td>0.569525</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.000600</td>\n","      <td>0.007959</td>\n","      <td>0.070169</td>\n","      <td>0.007959</td>\n","      <td>0.089211</td>\n","      <td>0.559848</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.000600</td>\n","      <td>0.007753</td>\n","      <td>0.068887</td>\n","      <td>0.007753</td>\n","      <td>0.088051</td>\n","      <td>0.571227</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.000500</td>\n","      <td>0.007949</td>\n","      <td>0.069939</td>\n","      <td>0.007949</td>\n","      <td>0.089157</td>\n","      <td>0.560386</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.000500</td>\n","      <td>0.007754</td>\n","      <td>0.069204</td>\n","      <td>0.007754</td>\n","      <td>0.088057</td>\n","      <td>0.571170</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.000500</td>\n","      <td>0.007818</td>\n","      <td>0.069464</td>\n","      <td>0.007818</td>\n","      <td>0.088421</td>\n","      <td>0.567610</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.000500</td>\n","      <td>0.007836</td>\n","      <td>0.069480</td>\n","      <td>0.007836</td>\n","      <td>0.088522</td>\n","      <td>0.566620</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.000400</td>\n","      <td>0.007836</td>\n","      <td>0.069314</td>\n","      <td>0.007836</td>\n","      <td>0.088520</td>\n","      <td>0.566647</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.000400</td>\n","      <td>0.007721</td>\n","      <td>0.068895</td>\n","      <td>0.007721</td>\n","      <td>0.087871</td>\n","      <td>0.572979</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.000400</td>\n","      <td>0.007730</td>\n","      <td>0.068828</td>\n","      <td>0.007730</td>\n","      <td>0.087919</td>\n","      <td>0.572511</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.000400</td>\n","      <td>0.008034</td>\n","      <td>0.070430</td>\n","      <td>0.008034</td>\n","      <td>0.089632</td>\n","      <td>0.555689</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.000400</td>\n","      <td>0.008125</td>\n","      <td>0.071138</td>\n","      <td>0.008125</td>\n","      <td>0.090140</td>\n","      <td>0.550636</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.000400</td>\n","      <td>0.007685</td>\n","      <td>0.068530</td>\n","      <td>0.007685</td>\n","      <td>0.087666</td>\n","      <td>0.574968</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.000300</td>\n","      <td>0.007793</td>\n","      <td>0.069480</td>\n","      <td>0.007793</td>\n","      <td>0.088278</td>\n","      <td>0.569009</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.000300</td>\n","      <td>0.007851</td>\n","      <td>0.069512</td>\n","      <td>0.007851</td>\n","      <td>0.088606</td>\n","      <td>0.565797</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.000300</td>\n","      <td>0.007990</td>\n","      <td>0.070484</td>\n","      <td>0.007990</td>\n","      <td>0.089384</td>\n","      <td>0.558139</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.000300</td>\n","      <td>0.007828</td>\n","      <td>0.069367</td>\n","      <td>0.007828</td>\n","      <td>0.088474</td>\n","      <td>0.567095</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.000300</td>\n","      <td>0.007633</td>\n","      <td>0.068383</td>\n","      <td>0.007633</td>\n","      <td>0.087367</td>\n","      <td>0.577857</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.000300</td>\n","      <td>0.007808</td>\n","      <td>0.069389</td>\n","      <td>0.007808</td>\n","      <td>0.088365</td>\n","      <td>0.568158</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.000300</td>\n","      <td>0.007597</td>\n","      <td>0.068183</td>\n","      <td>0.007597</td>\n","      <td>0.087164</td>\n","      <td>0.579823</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.000300</td>\n","      <td>0.007844</td>\n","      <td>0.069651</td>\n","      <td>0.007844</td>\n","      <td>0.088566</td>\n","      <td>0.566190</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.000300</td>\n","      <td>0.007729</td>\n","      <td>0.069064</td>\n","      <td>0.007729</td>\n","      <td>0.087913</td>\n","      <td>0.572566</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.000200</td>\n","      <td>0.007661</td>\n","      <td>0.068590</td>\n","      <td>0.007661</td>\n","      <td>0.087527</td>\n","      <td>0.576314</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.000200</td>\n","      <td>0.007718</td>\n","      <td>0.068892</td>\n","      <td>0.007718</td>\n","      <td>0.087853</td>\n","      <td>0.573154</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.000200</td>\n","      <td>0.007649</td>\n","      <td>0.068588</td>\n","      <td>0.007649</td>\n","      <td>0.087459</td>\n","      <td>0.576973</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.000200</td>\n","      <td>0.007705</td>\n","      <td>0.068902</td>\n","      <td>0.007705</td>\n","      <td>0.087778</td>\n","      <td>0.573883</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.000200</td>\n","      <td>0.007704</td>\n","      <td>0.068890</td>\n","      <td>0.007704</td>\n","      <td>0.087772</td>\n","      <td>0.573939</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 1h 5min 16s, sys: 23.5 s, total: 1h 5min 40s\n","Wall time: 1h 5min 28s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=9350, training_loss=0.001465559649196538, metrics={'train_runtime': 3928.0644, 'train_samples_per_second': 76.004, 'train_steps_per_second': 2.38, 'total_flos': 4.72657106691924e+16, 'train_loss': 0.001465559649196538, 'epoch': 50.0})"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["**EVALUAMOS EL MODELO**"],"metadata":{"id":"WN1U6OschCzd"}},{"cell_type":"code","source":["# trainer.evaluate()"],"metadata":{"id":"UXEEpahxeGif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/CompLex - Multimodal/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]\n","trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3f-5rDv1nduf","executionInfo":{"status":"ok","timestamp":1687152682714,"user_tz":300,"elapsed":1466,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["OUTPUT_PATH = 'CompLex - Multimodal/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]\n","trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"90D8HtiQ5xJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"qJF7c2LAne8W","executionInfo":{"status":"ok","timestamp":1687152689617,"user_tz":300,"elapsed":2699,"user":{"displayName":"Marcos Espinoza","userId":"16455726054832648724"}},"outputId":"ae05dd57-60a8-4467-f8f8-1e05d7ef5771"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:02]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.007703852839767933,\n"," 'eval_MAE': 0.06686960816087208,\n"," 'eval_MSE': 0.007703852899017148,\n"," 'eval_RMSE': 0.08777159505795225,\n"," 'eval_R2': 0.6045637242564463,\n"," 'eval_runtime': 2.3689,\n"," 'eval_samples_per_second': 315.339,\n"," 'eval_steps_per_second': 39.681,\n"," 'epoch': 50.0}"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["**Generamos Predicciones**"],"metadata":{"id":"U0YrvXoMhPYw"}},{"cell_type":"code","source":["# predictions = trainer.predict(test_dataset)\n","\n","# # Obtener las predicciones y las etiquetas reales del objeto de predicción\n","# pred_labels = predictions.predictions[0]\n","# true_labels = predictions.label_ids\n","\n","# def calc_regression_metrics(predictions, labels):\n","#     preds = np.squeeze(predictions)\n","#     labels = np.squeeze(labels)\n","#     mse = metrics.mean_squared_error(labels, preds)\n","#     rmse = sqrt(mse)\n","#     mae = metrics.mean_absolute_error(labels, preds)\n","#     return {\n","#         \"mse\": mse,\n","#         \"rmse\": rmse,\n","#         \"mae\": mae,\n","#     }\n","\n","# # Evaluar el rendimiento del modelo en las predicciones\n","# evaluation = calc_regression_metrics(pred_labels, true_labels)\n","\n","# # Imprimir los resultados de la evaluación\n","# print(\"MSE:\", evaluation[\"mse\"])\n","# print(\"RMSE:\", evaluation[\"rmse\"])\n","# print(\"MAE:\", evaluation[\"mae\"])"],"metadata":{"id":"SEVx1VhWebV4","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"3ec67037-9a8f-40ff-99e7-981a8e39f551"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["MSE: 0.04903765554811612\n","RMSE: 0.22144447509052043\n","MAE: 0.18595963293479548\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}