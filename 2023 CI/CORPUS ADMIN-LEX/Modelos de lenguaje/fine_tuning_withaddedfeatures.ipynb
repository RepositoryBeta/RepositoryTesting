{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cnYP65lj4JA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3747784-edc6-4820-dd61-42daedb2d8d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade accelerate\n",
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tLXQeiaL8cBo",
        "outputId": "beb5f06e-50b4-4518-ab69-da5404a67201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SE EJECUTA EN GOOGLE COLAB PARA INSTALAR LA LIBRERIA DE TRANSFORMERS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"SE EJECUTA EN GOOGLE COLAB PARA INSTALAR LA LIBRERIA DE TRANSFORMERS\"\"\"\n",
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBpXNyn4EdfS",
        "outputId": "20c8fbfc-d6ec-4771-eaed-8776c959ffe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert-base-spanish-wwm-uncased'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Total 83 (delta 0), reused 0 (delta 0), pack-reused 83\u001b[K\n",
            "Unpacking objects: 100% (83/83), 416.64 KiB | 1.96 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 1.31 GiB | 151.50 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# \"\"\"PARA CLONAR EL MODELO A USAR EN GOOGLE COLAB - para no estar descargando el modelo\"\"\"\n",
        "!git clone https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased #768\n",
        "# !git clone https://huggingface.co/xlm-roberta-base                       #768\n",
        "# !git clone https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne        #1024\n",
        "# !git clone https://huggingface.co/xlm-roberta-large                      #1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5qYrnRQ8exx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49391e6-ce7c-4bad-dc66-41b532372c06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#SE EJECUTA EN GOOGLE COLAB para conectar con google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59Zwvyc63u2J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd   #es una biblioteca para la manipulación y el análisis de datos.\n",
        "import numpy as np    #es una biblioteca para operaciones numéricas y procesamiento de matrices.\n",
        "import unicodedata    #proporciona acceso a la base de datos de caracteres Unicode.\n",
        "import re             #es un módulo para expresiones regulares, que se utilizan para la coincidencia de patrones y el procesamiento de texto.\n",
        "import os             #proporciona una forma de utilizar la funcionalidad dependiente del sistema operativo.\n",
        "import csv            #es un módulo para leer y escribir archivos CSV.\n",
        "\n",
        "\"\"\"es una biblioteca para aprendizaje automático y modelado estadístico en Python\"\"\"\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn import metrics, feature_selection\n",
        "\n",
        "\"\"\"math proporciona funciones matemáticas como sqr\"\"\"\n",
        "from math import sqrt\n",
        "\n",
        "\"\"\"transformers es un paquete de Python de Hugging Face que proporciona\"\"\"\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AutoTokenizer, AutoModel\n",
        "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EvalPrediction\n",
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\"\"\"torch es una biblioteca de PyTorch para el cálculo de tensores con un enfoque en el aprendizaje profundo.\"\"\"\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\"\"\"SequenceClassifierOutputes un módulo de Hugging Face que define la salida de un modelo de clasificación de secuencias.\"\"\"\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etgQqQv53zpI",
        "outputId": "6c4213f9-019a-4d44-d626-4b407b27a538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing\n",
            "Tipo de modelo: bert\n",
            "Modelo: bert-base-spanish-wwm-uncased\n",
            "Corpus: Adminlex_single_train_normaliza.xlsx\n",
            "Usa Características: True\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing\")\n",
        "    #SE ESCRIBE EL DIRECTORIO DEL CORPUS A CARGAR\n",
        "DATA_FILE_PATH = \"/content/drive/MyDrive/Tesis/corpus/Adminlex_single_train_normaliza.xlsx\"\n",
        "\n",
        "   #SE ESCRIBE EL DIRECTORIO DEL MODELO QUE SE EJECUTARÁ\n",
        "# MODEL_PATH = \"/content/xlm-roberta-base\"       #SE USA MODELO ROBERTA-BASE PARA MULTILENGUAJE\n",
        "# MODEL_PATH = \"/content/roberta-large-bne\"      #SE USA MODELO ROBERTA-LARGE PARA IDIOMA ESPAÑOL\n",
        "# MODEL_PATH =  \"/content/xlm-roberta-large\"     #SE USA MODELO XLM-ROBERTA-LARGE PARA MULTILENGUAJE\n",
        "MODEL_PATH =  \"/content/bert-base-spanish-wwm-uncased\" #SE USA MODELO bert-base-spanish-wwm-uncased PARA IDIOMA ESPAÑOL\n",
        "\n",
        "    #SE SELECCIONA QUE TIPO DE MODELO SE EJECUTARÁ\"\"\"\n",
        "MODEL_TYPE = 'bert'\n",
        "# MODEL_TYPE = 'roberta'\n",
        "\n",
        "USE_FEATURES = True    #True para ejecutar con características, False para ejecutar sin características\n",
        "EPOCHS = 100           #Se escribe la cantidad de épocas que se ejecutarán\n",
        "\n",
        "    #SE ESCRIBE EL DIRECTORIO DONDE SE GUARDARÁ EL NUEVO MODELO EJECUTADO\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/Tesis/\" + MODEL_PATH.split('/')[-1] + str(USE_FEATURES).split('/')[-1] + str(EPOCHS).split('/')[-1] + \"-finetuned\"\n",
        "\n",
        "print('Tipo de modelo:',MODEL_TYPE)\n",
        "print('Modelo:',MODEL_PATH.split('/')[-1])\n",
        "print('Corpus:',DATA_FILE_PATH.split('/')[-1])\n",
        "print('Usa Características:',USE_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c5rc3d134GQ",
        "outputId": "88563dfc-ec03-4b09-8614-8e5fa9e4f282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading data\")\n",
        "df = pd.read_excel(DATA_FILE_PATH)               #Lee el corpus ubicado en DATA_FILE_PATH\n",
        "NUM_FEATURES = 23                               #SE ESCRIBE EL NÚMERO DE CARACTERÍSTICAS QUE TIENE EL CORPUS CARGADO\n",
        "df['features'] = df.iloc[:,5:].values.tolist()   #SE ESCRIBE EL NÚMERO DE LA COLUMNA QUE EMPIEZAN LAS CARACTERÍSTICAS\n",
        "#df['features'] = df.iloc[:,5:8].values.tolist()\n",
        "df = df.dropna()                                 #elimina cualquier fila en el marco de datos que contenga valores faltantes\n",
        "\n",
        "df['sentence_token'] = df.apply(                #crea una nueva columna llamada 'sentence_token'\n",
        "      lambda x: str(x['sentence']).lower()      #La nueva columna se crea usando las columnas existentes 'sentence' y 'token'\n",
        "      + ' </s> ' + str(x['token']).lower(),    # método 'lower() convierte ambos valores a minúsculas\n",
        "      axis=1)                                   # axis=1 indica que la función debe aplicarse por filas\n",
        "#print(df)\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X60UYTF3jy3"
      },
      "outputs": [],
      "source": [
        "#print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM1T-M9137Wp",
        "outputId": "86ea30ba-5264-4907-9dca-4c46a9cf57fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizing texts\n"
          ]
        }
      ],
      "source": [
        "print(\"Vectorizing texts\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) #inicializa un tokenizador mediante la clase AUTOTOKENIZER -> carga un tokenizador previamente entrenado\n",
        "\n",
        "def vectorize_text(s, max_length):\n",
        "\n",
        "    # Unicode normalization\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')  # elimina cualquier diacrítico o acento de la cadena s\n",
        "    s = re.sub(r\"[^a-zA-Záéíóú.,!?;:()$€]+\", r\" \", s)   # reemplaza todas las coincidencias del patrón con un espacio\n",
        "\n",
        "    '''convierte la entrada de texto sin formato en un formato numérico que se puede introducir en un modelo de aprendizaje automático'''\n",
        "    input_ids = tokenizer.encode(   # utiliza el tokenizador previamente entrenado\n",
        "      s,                            # para codificar una cadena \"s\" en sus identificadores de token correspondientes\n",
        "      add_special_tokens=True,      # especifica si se agregan tokens especiales al principio y al final de la secuencia de tokens\n",
        "      max_length=max_length,        # especifica la longitud máxima de la secuencia de tokens resultante\n",
        "      padding='longest',            # especifica cómo rellenar secuencias más cortas a la misma longitud que la secuencia más larga.\n",
        "      truncation=True,              # especifica si se truncan las secuencias que son más largas que \"max_length\"\n",
        "      return_tensors='np'           # especifica que la salida debe devolverse como una matriz numpy.\n",
        "    )\n",
        "    return input_ids[0]             # devuelve solo el primer elemento de la matriz como una matriz numpy\n",
        "\n",
        "    '''Crea una nueva columna en el DataFrame llamado 'text_vec'\n",
        "    Los valores de 'text_vec' se calculan aplicando la función \"vectorize_text\" a la columna de la fila 'sentence_token'\n",
        "    '''\n",
        "df['text_vec'] = df.apply(lambda r: vectorize_text(r['sentence_token'], 512), axis=1)\n",
        "\n",
        "#print(df)\n",
        "#df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wu_f2HV4A8Q",
        "outputId": "5ae61054-2d3b-44e4-b12b-f533c32f7f39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets\n",
            "6250 train,  1563 test\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating datasets\")\n",
        "\n",
        "df = df.sample(frac=1)                    # seleccionar aleatoriamente todas las filas en el DataFrame\n",
        "train_portion = 0.8                       # especifica la proporción de los datos que se usarán para el entrenamiento\n",
        "split_point = int(train_portion*len(df))  # calcula el índice de la fila en la que dividir los datos en subconjuntos de entrenamiento y prueba\n",
        "train_data, test_data =  df[:split_point], df[split_point:] # asigna las filas anteriores al punto de división train_data y las filas posteriores al punto de división a test_data\n",
        "print(len(train_data), 'train, ', len(test_data), 'test')   # imprimen luego en la consola para indicar el número de filas en cada uno\n",
        "\n",
        "class MyDataset(Dataset):             # define una nueva clase MyDataset que hereda de Dataset\n",
        "    def __init__(self, dataframe):    # define el constructor  \"__init__\"  que toma un solo argumento dataframe\n",
        "        #print(dataframe)\n",
        "        self.len = len(dataframe)   # calcula la longitud de la entrada dataframe usando la funcion \"len\" y la almacena como una variable de instancia \"self.len\"\n",
        "        self.data = dataframe       # se asigna la entrada dataframe a una variable de instancia \"self.data\"\n",
        "\n",
        "    def __getitem__(self, index):   # define el método \"__getitem__\" que toma un solo argumento index\n",
        "        ''' el metodo __getitem__ devuelve un diccionario que contiene cuatro claves: 'input_ids', 'attention_mask', 'labels'y 'added_features' '''\n",
        "        input_ids = torch.tensor(self.data.text_vec.iloc[index]).cpu() # almacena las características de los datos de \"text_vec\" ​​que se han convertido en un vector de longitud fija.\n",
        "        attention_mask = torch.ones([input_ids.size(0)]).cpu()  # attention_mask almacena los elementos de entrada que se debe prestar atención y cuáles se deben ignorar\n",
        "        targets = self.data.complexity.iloc[index]              # almacena un valor escalar que representa la etiqueta de salida para la puntuación de complejidad\n",
        "        added_features = self.data.features.iloc[index] if USE_FEATURES else None  #almacena las características adicionales que pueden usarse, esto solo se incluye si USE_FEATURES es True.\n",
        "        return {\n",
        "            'input_ids': input_ids,               # devuelve las características de entrada para el punto de datos\n",
        "            'attention_mask': attention_mask,     # devuelve la máscara de atención para el punto de datos\n",
        "            'labels': targets,                    # devuelve un valor escalar que representa la puntuación de complejidad\n",
        "            'added_features': added_features,     # devuelve cualquier función adicional solo se incluye si USE_FEATURES es True\n",
        "         }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len   # devuelve la longitud del conjunto de datos personalizado\n",
        "\n",
        "train_set, test_set = MyDataset(train_data), MyDataset(test_data) # train_set contiene los datos para entrenar el modelo\n",
        "                                                                  # test_set  contiene los datos para evaluar el rendimiento del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKSlRHM-4D1J",
        "outputId": "55a7060b-c29d-4466-f10d-3072614a112f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating model\")\n",
        "\n",
        "def collate_batch(batch):   # recopila muestras de datos en lotes antes de enviarlos a la red neuronal\n",
        "    \"\"\" Optimize memory by setting all vectors in batch to a length equal to max\n",
        "        length found\n",
        "    \"\"\"\n",
        "\n",
        "    def pad_sequence(in_tensor, max_size):    # toma el tensor de entrada in_tensor y lo rellena con ceros para que su longitud sea igual a max_size\n",
        "        \"\"\" Fill tensor with zeros up to max_size\n",
        "        \"\"\"\n",
        "        out_tensor = np.zeros(max_size)                     # crea un tensor de salida out_tensor de ceros con longitud max_size\n",
        "        out_tensor[:in_tensor.size(0)] = in_tensor.numpy()  #copia los valores del tensor de entrada in_tensor al tensor de salida comenzando en el primer índice.\n",
        "                                                            # Si el tensor de entrada es más corto que max_size, entonces las entradas restantes en el tensor de salida ya están llenas de ceros\n",
        "        return out_tensor                                   # devuelve el tensor de salida\n",
        "\n",
        "    print(\"BATCH SIZE:\", len(batch))      # imprime el tamaño de batch, que es el número de muestras en el lote.\n",
        "\n",
        "    batch_inputs = []                     # crea listas vacías para batch_inputs, batch_attention_masks,\n",
        "    batch_attention_masks = []            # batch_targetsy batch_added_featurespara que se llenen con datos de batch\n",
        "    batch_targets = []\n",
        "    batch_added_features = []\n",
        "\n",
        "    ''' llena las listas vacías creadas previamente con datos de cada muestra en el lote '''\n",
        "    max_size = max([ex['input_ids'].size(0) for ex in batch])  #  calcula el tamaño máximo del tensor de entrada input_ids para la muestra con la secuencia de entrada más larga\n",
        "    for item in batch:                                         # el for itera sobre cada muestra del lote y extrae sus input_ids, attention_mask y labels\n",
        "        batch_inputs.append(pad_sequence(item['input_ids'], max_size))    # rellena la secuencia de entrada hasta la longitud máxima del lote\n",
        "        batch_attention_masks.append(pad_sequence(item['attention_mask'], max_size))  # rellena la máscara de atención hasta la longitud máxima del lote\n",
        "        batch_targets.append([float(item['labels'])])         # labels se convierte en un flotante y se envuelve en una lista con un solo elemento\n",
        "        if USE_FEATURES:                                       ## Si USE_FEATURES es True\n",
        "            batch_added_features.append(item['added_features']) # las funciones adicionales de cada muestra del lote se agregan a la lista batch_added_features\n",
        "\n",
        "    input_ids = torch.tensor(np.array(batch_inputs), dtype=torch.long)    # convierte las listas batch_inputs en tensores\n",
        "    attention_mask = torch.tensor(np.array(batch_attention_masks), dtype=torch.long) # convierte las listas batch_attention_masks en tensores\n",
        "\n",
        "    labels = torch.tensor(batch_targets, dtype=torch.float)    # convierte las listas batch_targets en tensores\n",
        "    if USE_FEATURES:                                           # si USE_FEATURES es True\n",
        "        added_features = torch.tensor(batch_added_features, dtype=torch.float) # convierte las listas batch_added_features en tensores\n",
        "    else:                                                      # de lo contrario\n",
        "        added_features = None                                  # se establece added_features en None\n",
        "\n",
        "    return {                                # devuelve como salida los datos de la función collate_batch\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"added_features\": added_features\n",
        "    }\n",
        "\n",
        "class SequenceClassifierWithFeatures(nn.Module):   # se utiliza para tareas de clasificación de secuencias en las que se incluyen características adicionales\n",
        "                                                   # el módulo hereda de la clase nn.Module y es la clase base para todos los módulos de redes neuronales\n",
        "\n",
        "    def __init__(self, num_labels=1, model_path=MODEL_PATH, model_type=MODEL_TYPE): # define el método constructor del módulo SequenceClassifierWithFeatures\n",
        "        \"\"\"\n",
        "        model_type: \"bert\" or \"roberta\"\n",
        "        \"\"\"\n",
        "        super(SequenceClassifierWithFeatures, self).__init__() # llama al constructor de la clase principal nn.Module para inicializar el módulo SequenceClassifierWithFeatures\n",
        "                                # método __init__ nos aseguramos de que el módulo de red neuronal herede las propiedades y métodos de la clase nn.Module\n",
        "\n",
        "        ''' input_dim corresponde a la dimensionalidad de las incrustaciones de entrada para el modelo pre entrenado'''\n",
        "        input_dim = 768  #El input_dim se elije dependiendo del archivo config.json del modelo a usar\n",
        "        # input_dim = 1024  # 1024 USA CONFIG.JSON DE LOS MODELOS LARGE\n",
        "        if USE_FEATURES:                                            #si USE_FEATURES es True\n",
        "            input_dim += NUM_FEATURES                               # agrega el número de caracteristicas adicionales a la input_dim\n",
        "        print(\"input_dim:\", input_dim, USE_FEATURES, NUM_FEATURES)  # imprime el valor de input_dim, USE_FEATURES y NUM_FEATURES\n",
        "\n",
        "        output_dim = num_labels       # output_dim sea igual al número de etiquetas en el conjunto de datos\n",
        "\n",
        "        ''' define la arquitectura del SequenceClassifierWithFeatures'''\n",
        "        self.num_labels= num_labels         # almacena el número de etiquetas para el problema\n",
        "        self.problem_type='regression'      # establece el tipo de problema en \"regresión\"\n",
        "        self.model_path = model_path        # almacene la ruta al modelo pre-entrenado el tipo de modelo\n",
        "        self.model_type = model_type        # almacene la ruta al tipo de modelo\n",
        "        self.base_model = AutoModel.from_pretrained(model_path, num_labels=self.num_labels) # carga el modelo previamente entrenado desde la ruta dada\n",
        "                                                        #establece el número de etiquetas que se usarán en la capa final y lo almacena en self.base_model\n",
        "        self.dropout = nn.Dropout(0.5)      # crea una capa de abandono con una probabilidad de abandono de 0,5\n",
        "        self.dense = nn.Linear(input_dim, input_dim)    # crea una capa completamente conectada con dimensiones de entrada y salida de input_dim\n",
        "        self.linear = nn.Linear(input_dim, output_dim)  # crea una capa completamente conectada con la dimensión de entrada input_dim y la dimensión de salida de output_dim\n",
        "        self.loss_func = nn.MSELoss()       # establece la función de pérdida que se utilizará durante el entrenamiento en la pérdida del error cuadrático medio (MSE).\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, added_features=None): # FOWARD calcula la pérdida y devuelve el resultado final\n",
        "\n",
        "        outputs = self.base_model(input_ids, attention_mask=attention_mask) # pasa input_ids y attention_mask como entradas al base_model\n",
        "                                                                            # y asigna las salidas resultantes a la variable outputs\n",
        "        if self.model_type == \"bert\":   # si model_tyoe es igual a bert\n",
        "            outputs = outputs[1]        # toma solo el segundo elemento de la tupla outputs\n",
        "                                        #Para los modelos BERT, el primer elemento de la tupla representa la salida a nivel de secuencia\n",
        "                                        #y el segundo elemento representa la salida agrupada.'''\n",
        "            if USE_FEATURES:\n",
        "                outputs = torch.cat((outputs, added_features), dim = -1) # concatena el tensor outputs con tensor added_features a lo largo de la última dimensión\n",
        "        elif self.model_type == \"roberta\":\n",
        "            outputs = outputs[0][:,0,:]  # toma solo el primer elemento de la tupla outputs\n",
        "                                        # [:,0,:] selecciona solo el primer token  de cada secuencia de entrada\n",
        "            if USE_FEATURES:\n",
        "                outputs = torch.cat((outputs, added_features), dim = -1) # concatena el tensor outputs con tensor added_features a lo largo de la última dimensión\n",
        "            outputs = self.dropout(outputs) # aplica regularización de abandono al tensor outputs.\n",
        "            outputs = self.dense(outputs)   # pasa el tensor outputs a través de una capa completamente conectada\n",
        "            outputs = torch.tanh(outputs)   # aplica la función de activación de la tangente hiperbólica (tanh) al tensor outputs.\n",
        "        else:\n",
        "            raise Exception('Invalid model_type: only \"bert\" or \"roberta\" models are supported')\n",
        "\n",
        "        outputs = self.dropout(outputs) # aplica regularización de abandono al tensor outputs.\n",
        "        logits = self.linear(outputs) # pasa el tensor outputs a través de una capa completamente conectada que produce los logits de salida para cada etiqueta de clase posible.\n",
        "\n",
        "        loss = None # Inicializa la variable loss a None\n",
        "        if labels is not None:  # Si el argumento labels no es None, se calcula la pérdida\n",
        "          loss = self.loss_func(logits.view(-1), labels.view(-1))   # calcula la pérdida usando la función loss_func de pérdida\n",
        "          print(\"loss:\", loss)  # imprime la pérdida calculada\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq-w9g0B4PAA",
        "outputId": "1612f92b-ba81-478d-cd14-9b3d7c1e3674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining custom training\n",
            "input_dim: 791 True 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at /content/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "print(\"Defining custom training\")   # Definimos el entrenamiento personalizado\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  def get_train_dataloader(self):   # carga el conjunto de datos de entrenamiento\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        collate_fn=collate_batch, # procesa varias muestras de datos de entrenamiento y convertirlas en un solo lote\n",
        "        batch_size=self.args.per_device_train_batch_size  # el tamaño de lote que se utilizará\n",
        "                                                          # durante el entrenamiento del modelo será igual al tamaño de lote\n",
        "                                                          # que se entrena en cada dispositivo de procesamiento\n",
        "    )\n",
        "\n",
        "  def get_eval_dataloader(self, eval_dataset): # carga el conjunto de datos de evaluacion\n",
        "    return DataLoader(\n",
        "        self.eval_dataset,\n",
        "        collate_fn=collate_batch, # procesar varias muestras de datos de evaluación y convertirlas en un solo lote\n",
        "        batch_size=self.args.per_device_eval_batch_size # el tamaño de lote que se utilizará\n",
        "                                                        # durante la evaluacion del modelo será igual al tamaño de lote\n",
        "                                                        # que se entrena en cada dispositivo de procesamiento\n",
        "    )\n",
        "\n",
        "def compute_metrics(p: EvalPrediction): # calcula diversas métricas de evaluación\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "                    # si es una tupla, entonces preds se establece como el primer elemento de la tupla\n",
        "                    # si no es una tupla, entonces se asume que es un tensor de predicciones, y preds se establece en este tensor.\n",
        "    preds = np.squeeze(preds) # elimina dimensiones de tamaño 1 del tensor de predicciones preds\n",
        "    labels = np.squeeze(p.label_ids) # elimina dimensiones de tamaño 1 del y del tensor de etiqueta p.label_ids\n",
        "    print(\"SIZES::::\",labels.shape, preds.shape)\n",
        "    mse = metrics.mean_squared_error(labels, preds)\n",
        "    return {\"MAE\": metrics.mean_absolute_error(labels, preds), # error absoluto medio EVALUAR LA PRECISIÓN\n",
        "            'MSE': mse,       # el error cuadrático medio PREDICCIONES MÁS PRECISAS\n",
        "            'RMSE': sqrt(mse),  # la raíz del error cuadrático medio DIFERENCIA ENTRE LOS VALORES PREDICHOS POR UN MODELO Y LOS VALORES REALES\n",
        "            'R2': metrics.r2_score(labels, preds)#,# coeficiente de determinación INDICA QUÉ TAN BIEN EL MODELO DE REGRESIÓN SE AJUSTA A LOS DATOS OBSERVADOS\n",
        "            # 'Poisson': metrics.mean_poisson_deviance(labels, preds), # devianza de Poisson media EVALUAR LA CALIDAD\n",
        "            # 'Pearson': np.corrcoef(labels, preds)[0,1]\n",
        "            } # correlación de Pearson\n",
        "                              # EVALUAR LA RELACIÓN ENTRE LAS PREDICCIONES DEL MODELO Y LOS VALORES REALES DE LA VARIABLE OBJETIVO\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(    # configuracion o argumentos de la forma en que se realizará el entrenamiento y evaluacion del modelo\n",
        "    output_dir= 'output',  # Directorio de salida donde se guardarán los archivos generados durante el entrenamiento\n",
        "    evaluation_strategy='epoch',  # la evaluación se realiza después de cada época\n",
        "    logging_strategy='epoch', # se realizará logging después de cada época\n",
        "    num_train_epochs=EPOCHS,  # número de épocas de entrenamiento que se realizarán\n",
        "    remove_unused_columns=False,  # se eliminarán las columnas no utilizadas en los datos de entrenamiento y evaluación\n",
        "    per_device_train_batch_size=32, # Tamaño del lote de entrenamiento por dispositivo\n",
        "    per_device_eval_batch_size=32,  # Tamaño del lote de evaluación por dispositivo\n",
        ")\n",
        "\n",
        "model = SequenceClassifierWithFeatures(model_path=MODEL_PATH, model_type=MODEL_TYPE)\n",
        "\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    model=model,  # modelo que se entrenará.\n",
        "    args=training_args, # argumentos de entrenamiento y evaluación que se utilizarán durante el proceso de entrenamiento y evaluación\n",
        "    train_dataset=train_set,  # conjunto de datos de entrenamiento\n",
        "    eval_dataset=test_set,  # conjunto de datos de evaluación\n",
        "    compute_metrics=compute_metrics  # calcula las métricas de evaluación.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh92RGxK4U5A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4af671e2-7ade-419e-87c6-87a7f56e2ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training\n",
            "BATCH SIZE: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(0.5159, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19600' max='19600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19600/19600 1:19:36, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mae</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Rmse</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.086800</td>\n",
              "      <td>0.045774</td>\n",
              "      <td>0.185958</td>\n",
              "      <td>0.045733</td>\n",
              "      <td>0.213854</td>\n",
              "      <td>0.133360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.060400</td>\n",
              "      <td>0.043850</td>\n",
              "      <td>0.159085</td>\n",
              "      <td>0.043830</td>\n",
              "      <td>0.209357</td>\n",
              "      <td>0.169427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>0.049986</td>\n",
              "      <td>0.160727</td>\n",
              "      <td>0.049985</td>\n",
              "      <td>0.223573</td>\n",
              "      <td>0.052795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.036900</td>\n",
              "      <td>0.067649</td>\n",
              "      <td>0.214944</td>\n",
              "      <td>0.067624</td>\n",
              "      <td>0.260047</td>\n",
              "      <td>-0.281469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.032300</td>\n",
              "      <td>0.076142</td>\n",
              "      <td>0.230890</td>\n",
              "      <td>0.076109</td>\n",
              "      <td>0.275879</td>\n",
              "      <td>-0.442251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.024700</td>\n",
              "      <td>0.057247</td>\n",
              "      <td>0.188660</td>\n",
              "      <td>0.057199</td>\n",
              "      <td>0.239162</td>\n",
              "      <td>-0.083903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.020900</td>\n",
              "      <td>0.072366</td>\n",
              "      <td>0.219563</td>\n",
              "      <td>0.072352</td>\n",
              "      <td>0.268983</td>\n",
              "      <td>-0.371055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.016500</td>\n",
              "      <td>0.077063</td>\n",
              "      <td>0.223503</td>\n",
              "      <td>0.076981</td>\n",
              "      <td>0.277455</td>\n",
              "      <td>-0.458777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.014200</td>\n",
              "      <td>0.060481</td>\n",
              "      <td>0.185613</td>\n",
              "      <td>0.060425</td>\n",
              "      <td>0.245816</td>\n",
              "      <td>-0.145049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.013600</td>\n",
              "      <td>0.057010</td>\n",
              "      <td>0.170638</td>\n",
              "      <td>0.056950</td>\n",
              "      <td>0.238641</td>\n",
              "      <td>-0.079184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.010500</td>\n",
              "      <td>0.052825</td>\n",
              "      <td>0.162579</td>\n",
              "      <td>0.052764</td>\n",
              "      <td>0.229703</td>\n",
              "      <td>0.000139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>0.052038</td>\n",
              "      <td>0.160167</td>\n",
              "      <td>0.051968</td>\n",
              "      <td>0.227964</td>\n",
              "      <td>0.015223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.008400</td>\n",
              "      <td>0.066249</td>\n",
              "      <td>0.182562</td>\n",
              "      <td>0.066166</td>\n",
              "      <td>0.257227</td>\n",
              "      <td>-0.253827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.007800</td>\n",
              "      <td>0.064175</td>\n",
              "      <td>0.188474</td>\n",
              "      <td>0.064112</td>\n",
              "      <td>0.253204</td>\n",
              "      <td>-0.214915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.065738</td>\n",
              "      <td>0.187858</td>\n",
              "      <td>0.065632</td>\n",
              "      <td>0.256187</td>\n",
              "      <td>-0.243706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.061495</td>\n",
              "      <td>0.176660</td>\n",
              "      <td>0.061444</td>\n",
              "      <td>0.247878</td>\n",
              "      <td>-0.164346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.005300</td>\n",
              "      <td>0.060108</td>\n",
              "      <td>0.168296</td>\n",
              "      <td>0.060013</td>\n",
              "      <td>0.244975</td>\n",
              "      <td>-0.137229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>0.060216</td>\n",
              "      <td>0.178157</td>\n",
              "      <td>0.060147</td>\n",
              "      <td>0.245249</td>\n",
              "      <td>-0.139772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.058499</td>\n",
              "      <td>0.165934</td>\n",
              "      <td>0.058457</td>\n",
              "      <td>0.241779</td>\n",
              "      <td>-0.107748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.060260</td>\n",
              "      <td>0.165956</td>\n",
              "      <td>0.060185</td>\n",
              "      <td>0.245326</td>\n",
              "      <td>-0.140493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>0.058591</td>\n",
              "      <td>0.165031</td>\n",
              "      <td>0.058485</td>\n",
              "      <td>0.241836</td>\n",
              "      <td>-0.108274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.062995</td>\n",
              "      <td>0.171602</td>\n",
              "      <td>0.062907</td>\n",
              "      <td>0.250812</td>\n",
              "      <td>-0.192072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.060304</td>\n",
              "      <td>0.169713</td>\n",
              "      <td>0.060240</td>\n",
              "      <td>0.245438</td>\n",
              "      <td>-0.141532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.069376</td>\n",
              "      <td>0.177915</td>\n",
              "      <td>0.069297</td>\n",
              "      <td>0.263243</td>\n",
              "      <td>-0.313163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.060811</td>\n",
              "      <td>0.171662</td>\n",
              "      <td>0.060745</td>\n",
              "      <td>0.246465</td>\n",
              "      <td>-0.151108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>0.062737</td>\n",
              "      <td>0.165972</td>\n",
              "      <td>0.062643</td>\n",
              "      <td>0.250287</td>\n",
              "      <td>-0.187081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>0.058021</td>\n",
              "      <td>0.167926</td>\n",
              "      <td>0.057918</td>\n",
              "      <td>0.240662</td>\n",
              "      <td>-0.097541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.059108</td>\n",
              "      <td>0.161703</td>\n",
              "      <td>0.059074</td>\n",
              "      <td>0.243051</td>\n",
              "      <td>-0.119440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.003300</td>\n",
              "      <td>0.058424</td>\n",
              "      <td>0.164270</td>\n",
              "      <td>0.058358</td>\n",
              "      <td>0.241575</td>\n",
              "      <td>-0.105879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.061536</td>\n",
              "      <td>0.164316</td>\n",
              "      <td>0.061438</td>\n",
              "      <td>0.247866</td>\n",
              "      <td>-0.164229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.059741</td>\n",
              "      <td>0.176625</td>\n",
              "      <td>0.059626</td>\n",
              "      <td>0.244184</td>\n",
              "      <td>-0.129895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.061407</td>\n",
              "      <td>0.163443</td>\n",
              "      <td>0.061313</td>\n",
              "      <td>0.247615</td>\n",
              "      <td>-0.161868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.058977</td>\n",
              "      <td>0.165407</td>\n",
              "      <td>0.058904</td>\n",
              "      <td>0.242702</td>\n",
              "      <td>-0.116219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.060320</td>\n",
              "      <td>0.162220</td>\n",
              "      <td>0.060266</td>\n",
              "      <td>0.245492</td>\n",
              "      <td>-0.142032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.002600</td>\n",
              "      <td>0.059999</td>\n",
              "      <td>0.162123</td>\n",
              "      <td>0.059931</td>\n",
              "      <td>0.244808</td>\n",
              "      <td>-0.135680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.061201</td>\n",
              "      <td>0.165781</td>\n",
              "      <td>0.061068</td>\n",
              "      <td>0.247119</td>\n",
              "      <td>-0.157223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>0.056715</td>\n",
              "      <td>0.159786</td>\n",
              "      <td>0.056624</td>\n",
              "      <td>0.237958</td>\n",
              "      <td>-0.073016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.060349</td>\n",
              "      <td>0.166104</td>\n",
              "      <td>0.060272</td>\n",
              "      <td>0.245504</td>\n",
              "      <td>-0.142149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.062378</td>\n",
              "      <td>0.166674</td>\n",
              "      <td>0.062284</td>\n",
              "      <td>0.249567</td>\n",
              "      <td>-0.180265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.060053</td>\n",
              "      <td>0.163811</td>\n",
              "      <td>0.059980</td>\n",
              "      <td>0.244909</td>\n",
              "      <td>-0.136616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.060313</td>\n",
              "      <td>0.161199</td>\n",
              "      <td>0.060281</td>\n",
              "      <td>0.245522</td>\n",
              "      <td>-0.142318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.061226</td>\n",
              "      <td>0.165205</td>\n",
              "      <td>0.061184</td>\n",
              "      <td>0.247354</td>\n",
              "      <td>-0.159422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.062112</td>\n",
              "      <td>0.165443</td>\n",
              "      <td>0.062071</td>\n",
              "      <td>0.249140</td>\n",
              "      <td>-0.176226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.062729</td>\n",
              "      <td>0.169582</td>\n",
              "      <td>0.062698</td>\n",
              "      <td>0.250396</td>\n",
              "      <td>-0.188121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.065548</td>\n",
              "      <td>0.167224</td>\n",
              "      <td>0.065525</td>\n",
              "      <td>0.255978</td>\n",
              "      <td>-0.241676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.058294</td>\n",
              "      <td>0.159504</td>\n",
              "      <td>0.058270</td>\n",
              "      <td>0.241392</td>\n",
              "      <td>-0.104207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.058763</td>\n",
              "      <td>0.160203</td>\n",
              "      <td>0.058754</td>\n",
              "      <td>0.242391</td>\n",
              "      <td>-0.113368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.058353</td>\n",
              "      <td>0.155382</td>\n",
              "      <td>0.058332</td>\n",
              "      <td>0.241520</td>\n",
              "      <td>-0.105376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.060230</td>\n",
              "      <td>0.159336</td>\n",
              "      <td>0.060205</td>\n",
              "      <td>0.245368</td>\n",
              "      <td>-0.140877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.058247</td>\n",
              "      <td>0.158712</td>\n",
              "      <td>0.058231</td>\n",
              "      <td>0.241312</td>\n",
              "      <td>-0.103472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.061919</td>\n",
              "      <td>0.166281</td>\n",
              "      <td>0.061909</td>\n",
              "      <td>0.248815</td>\n",
              "      <td>-0.173162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.062643</td>\n",
              "      <td>0.163089</td>\n",
              "      <td>0.062625</td>\n",
              "      <td>0.250250</td>\n",
              "      <td>-0.186731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.057623</td>\n",
              "      <td>0.154190</td>\n",
              "      <td>0.057581</td>\n",
              "      <td>0.239961</td>\n",
              "      <td>-0.091155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.059435</td>\n",
              "      <td>0.157243</td>\n",
              "      <td>0.059400</td>\n",
              "      <td>0.243722</td>\n",
              "      <td>-0.125621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.061200</td>\n",
              "      <td>0.162697</td>\n",
              "      <td>0.061151</td>\n",
              "      <td>0.247287</td>\n",
              "      <td>-0.158791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.060614</td>\n",
              "      <td>0.160610</td>\n",
              "      <td>0.060586</td>\n",
              "      <td>0.246143</td>\n",
              "      <td>-0.148097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.064976</td>\n",
              "      <td>0.163043</td>\n",
              "      <td>0.064974</td>\n",
              "      <td>0.254900</td>\n",
              "      <td>-0.231245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.063714</td>\n",
              "      <td>0.160470</td>\n",
              "      <td>0.063708</td>\n",
              "      <td>0.252405</td>\n",
              "      <td>-0.207255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.060225</td>\n",
              "      <td>0.159916</td>\n",
              "      <td>0.060203</td>\n",
              "      <td>0.245364</td>\n",
              "      <td>-0.140841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.060464</td>\n",
              "      <td>0.157141</td>\n",
              "      <td>0.060466</td>\n",
              "      <td>0.245898</td>\n",
              "      <td>-0.145819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.062860</td>\n",
              "      <td>0.165650</td>\n",
              "      <td>0.062855</td>\n",
              "      <td>0.250708</td>\n",
              "      <td>-0.191081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.062771</td>\n",
              "      <td>0.159968</td>\n",
              "      <td>0.062769</td>\n",
              "      <td>0.250538</td>\n",
              "      <td>-0.189461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.061701</td>\n",
              "      <td>0.159220</td>\n",
              "      <td>0.061686</td>\n",
              "      <td>0.248368</td>\n",
              "      <td>-0.168945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.060410</td>\n",
              "      <td>0.159161</td>\n",
              "      <td>0.060364</td>\n",
              "      <td>0.245690</td>\n",
              "      <td>-0.143879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.060318</td>\n",
              "      <td>0.158885</td>\n",
              "      <td>0.060297</td>\n",
              "      <td>0.245554</td>\n",
              "      <td>-0.142608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.056279</td>\n",
              "      <td>0.158246</td>\n",
              "      <td>0.056225</td>\n",
              "      <td>0.237119</td>\n",
              "      <td>-0.065459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.058928</td>\n",
              "      <td>0.155641</td>\n",
              "      <td>0.058890</td>\n",
              "      <td>0.242672</td>\n",
              "      <td>-0.115950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.058949</td>\n",
              "      <td>0.156250</td>\n",
              "      <td>0.058923</td>\n",
              "      <td>0.242740</td>\n",
              "      <td>-0.116574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.058441</td>\n",
              "      <td>0.156451</td>\n",
              "      <td>0.058391</td>\n",
              "      <td>0.241643</td>\n",
              "      <td>-0.106501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.058376</td>\n",
              "      <td>0.156751</td>\n",
              "      <td>0.058323</td>\n",
              "      <td>0.241501</td>\n",
              "      <td>-0.105202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.060876</td>\n",
              "      <td>0.162505</td>\n",
              "      <td>0.060822</td>\n",
              "      <td>0.246621</td>\n",
              "      <td>-0.152566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.059583</td>\n",
              "      <td>0.156046</td>\n",
              "      <td>0.059511</td>\n",
              "      <td>0.243948</td>\n",
              "      <td>-0.127717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.061500</td>\n",
              "      <td>0.160789</td>\n",
              "      <td>0.061430</td>\n",
              "      <td>0.247850</td>\n",
              "      <td>-0.164079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.059606</td>\n",
              "      <td>0.157699</td>\n",
              "      <td>0.059562</td>\n",
              "      <td>0.244053</td>\n",
              "      <td>-0.128685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.160183</td>\n",
              "      <td>0.059645</td>\n",
              "      <td>0.244224</td>\n",
              "      <td>-0.130268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.058712</td>\n",
              "      <td>0.154742</td>\n",
              "      <td>0.058653</td>\n",
              "      <td>0.242183</td>\n",
              "      <td>-0.111456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.058318</td>\n",
              "      <td>0.158319</td>\n",
              "      <td>0.058266</td>\n",
              "      <td>0.241384</td>\n",
              "      <td>-0.104137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.059908</td>\n",
              "      <td>0.157315</td>\n",
              "      <td>0.059848</td>\n",
              "      <td>0.244639</td>\n",
              "      <td>-0.134112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.060591</td>\n",
              "      <td>0.159113</td>\n",
              "      <td>0.060546</td>\n",
              "      <td>0.246061</td>\n",
              "      <td>-0.147337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.061298</td>\n",
              "      <td>0.161235</td>\n",
              "      <td>0.061254</td>\n",
              "      <td>0.247496</td>\n",
              "      <td>-0.160756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.061007</td>\n",
              "      <td>0.161084</td>\n",
              "      <td>0.060961</td>\n",
              "      <td>0.246903</td>\n",
              "      <td>-0.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.062116</td>\n",
              "      <td>0.159312</td>\n",
              "      <td>0.062060</td>\n",
              "      <td>0.249118</td>\n",
              "      <td>-0.176018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.061729</td>\n",
              "      <td>0.160995</td>\n",
              "      <td>0.061701</td>\n",
              "      <td>0.248398</td>\n",
              "      <td>-0.169229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.061121</td>\n",
              "      <td>0.160987</td>\n",
              "      <td>0.061091</td>\n",
              "      <td>0.247167</td>\n",
              "      <td>-0.157669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.060608</td>\n",
              "      <td>0.158445</td>\n",
              "      <td>0.060579</td>\n",
              "      <td>0.246129</td>\n",
              "      <td>-0.147968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.061302</td>\n",
              "      <td>0.159841</td>\n",
              "      <td>0.061278</td>\n",
              "      <td>0.247543</td>\n",
              "      <td>-0.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.061385</td>\n",
              "      <td>0.160422</td>\n",
              "      <td>0.061362</td>\n",
              "      <td>0.247713</td>\n",
              "      <td>-0.162793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.060887</td>\n",
              "      <td>0.161880</td>\n",
              "      <td>0.060855</td>\n",
              "      <td>0.246689</td>\n",
              "      <td>-0.153196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.060129</td>\n",
              "      <td>0.158909</td>\n",
              "      <td>0.060102</td>\n",
              "      <td>0.245158</td>\n",
              "      <td>-0.138930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.060394</td>\n",
              "      <td>0.160725</td>\n",
              "      <td>0.060360</td>\n",
              "      <td>0.245682</td>\n",
              "      <td>-0.143803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.060215</td>\n",
              "      <td>0.159856</td>\n",
              "      <td>0.060170</td>\n",
              "      <td>0.245295</td>\n",
              "      <td>-0.140206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.060917</td>\n",
              "      <td>0.158119</td>\n",
              "      <td>0.060888</td>\n",
              "      <td>0.246756</td>\n",
              "      <td>-0.153821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.060656</td>\n",
              "      <td>0.157692</td>\n",
              "      <td>0.060626</td>\n",
              "      <td>0.246224</td>\n",
              "      <td>-0.148853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061646</td>\n",
              "      <td>0.159445</td>\n",
              "      <td>0.061617</td>\n",
              "      <td>0.248229</td>\n",
              "      <td>-0.167638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061866</td>\n",
              "      <td>0.160492</td>\n",
              "      <td>0.061838</td>\n",
              "      <td>0.248672</td>\n",
              "      <td>-0.171815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061285</td>\n",
              "      <td>0.159688</td>\n",
              "      <td>0.061256</td>\n",
              "      <td>0.247500</td>\n",
              "      <td>-0.160790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061855</td>\n",
              "      <td>0.160631</td>\n",
              "      <td>0.061829</td>\n",
              "      <td>0.248654</td>\n",
              "      <td>-0.171647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061774</td>\n",
              "      <td>0.160104</td>\n",
              "      <td>0.061747</td>\n",
              "      <td>0.248488</td>\n",
              "      <td>-0.170083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.061615</td>\n",
              "      <td>0.160300</td>\n",
              "      <td>0.061586</td>\n",
              "      <td>0.248166</td>\n",
              "      <td>-0.167047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.061469</td>\n",
              "      <td>0.160033</td>\n",
              "      <td>0.061439</td>\n",
              "      <td>0.247868</td>\n",
              "      <td>-0.164252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n",
            "loss: tensor(0.0664, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0469, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0387, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0731, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0682, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0990, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0838, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0651, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0689, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0683, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0745, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0470, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0929, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0575, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0617, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0325, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0334, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0421, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0452, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0589, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0622, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0293, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0636, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0378, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0643, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0630, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0819, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0449, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0599, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0406, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0747, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0622, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0706, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0578, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0763, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0454, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0592, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0718, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0829, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0674, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0461, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0332, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0677, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0677, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0710, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0033, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0049, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0707, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0433, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0676, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0500, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0670, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0461, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0456, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0686, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0641, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0965, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0853, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0640, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0665, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0678, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0749, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0485, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0921, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0516, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0634, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0324, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0339, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0424, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0452, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0600, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0629, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0289, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0637, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0378, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0668, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0615, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0832, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0448, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0604, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0404, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0776, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0587, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0714, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0582, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0726, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0456, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0559, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0738, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0837, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0669, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0470, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0343, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0627, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0699, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0743, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0697, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0441, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0707, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0478, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0684, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0450, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0483, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0695, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0680, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0882, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0899, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0753, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0712, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0634, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0759, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0425, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0929, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0494, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0689, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0420, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0312, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0469, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0434, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0602, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0602, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0234, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0589, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0456, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0626, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0584, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0873, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0458, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0597, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0433, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0787, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0676, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0649, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0615, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0772, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0479, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0545, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0780, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0807, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0699, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0499, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0349, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0625, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0689, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0700, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0024, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0052, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0061, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0713, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0435, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0697, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0472, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0675, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0442, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0477, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0700, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0677, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0888, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0891, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0754, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0684, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0759, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0420, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0924, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0494, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0685, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0436, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0317, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0466, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0431, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0603, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0229, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0603, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0481, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0628, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0587, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0879, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0590, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0441, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0771, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0680, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0646, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0602, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0740, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0479, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0532, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0761, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0806, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0696, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0492, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0352, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0618, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0685, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0700, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0038, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0756, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0442, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0714, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0484, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0672, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0452, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0465, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0710, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0698, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0930, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0885, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0756, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0706, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0624, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0777, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0428, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0933, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0514, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0705, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0427, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0319, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0460, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0441, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0636, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0237, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0600, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0473, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0644, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0592, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0869, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0448, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0617, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0437, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0779, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0720, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0657, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0602, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0773, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0470, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0544, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0771, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0815, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0706, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0497, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0352, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0647, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0697, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0705, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0043, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0770, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0712, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0509, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0657, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0460, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0458, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0716, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0701, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0950, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0867, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0757, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0689, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0626, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0786, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0446, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0932, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0522, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0735, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0411, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0323, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0453, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0451, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0635, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0634, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0274, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0617, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0455, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0650, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0593, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0860, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0441, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0628, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0432, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0773, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0723, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0674, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0606, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0747, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0458, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0564, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0757, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0833, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0714, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0485, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0336, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0642, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0702, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0708, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0020, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0767, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0429, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0701, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0513, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0646, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0460, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0453, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0707, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0696, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0965, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0850, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0731, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0682, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0655, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0777, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0460, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0920, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0520, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0722, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0378, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0332, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0436, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0449, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0613, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0645, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0296, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0617, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0412, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0663, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0605, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0843, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0435, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0625, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0418, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0773, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0690, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0686, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0587, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0736, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0450, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0579, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0735, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0829, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0698, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0473, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0334, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0639, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0699, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0702, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0029, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0046, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0781, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0433, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0737, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0505, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0697, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0460, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0455, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0720, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0706, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0949, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0872, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0746, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0687, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0640, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0785, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0444, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0928, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0524, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0723, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0394, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0323, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0445, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0625, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0642, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0293, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0619, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0430, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0647, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0608, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0864, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0444, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0627, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0778, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0710, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0677, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0584, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0744, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0461, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0575, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0761, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0830, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0705, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0471, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0335, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0640, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0702, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0699, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0030, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0022, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0026, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0770, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0432, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0732, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0499, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0696, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0458, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0456, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0711, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0709, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0946, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0873, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0749, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0678, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0631, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0778, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0928, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0527, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0718, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0393, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0324, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0444, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0447, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0624, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0643, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0291, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0619, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0437, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0648, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0589, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0865, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0450, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0625, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0441, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0773, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0725, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0674, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0591, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0740, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0459, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0571, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0764, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0831, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0712, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0470, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0337, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0641, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0704, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0703, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0040, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0032, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0028, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0017, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0019, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0039, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0770, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0428, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0720, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0499, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0680, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0458, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0454, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0712, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0714, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0959, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0869, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0740, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0674, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0640, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0775, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0448, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0926, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0521, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0716, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0382, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0330, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0442, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0445, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0647, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0296, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0425, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0646, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0592, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0859, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0440, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0438, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0770, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0720, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0679, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0592, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0744, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0456, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0576, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0755, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0833, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0709, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0469, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0337, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0636, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0702, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0704, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0035, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0001, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0011, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0018, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0021, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0041, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0016, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0008, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0007, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0009, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0014, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0013, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0004, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0036, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0006, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0002, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 10\n",
            "loss: tensor(0.0003, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0769, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0424, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0718, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0499, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0680, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0457, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0451, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0713, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0714, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0958, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0867, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0737, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0672, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0641, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0774, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0448, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0922, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0519, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0717, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0379, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0330, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0440, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0443, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0618, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0646, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0296, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0620, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0422, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0644, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0594, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0856, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0437, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0618, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0437, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0768, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0718, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0678, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0590, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0740, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0455, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0576, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0750, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0831, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0706, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0468, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0336, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0633, device='cuda:0')\n",
            "BATCH SIZE: 32\n",
            "loss: tensor(0.0699, device='cuda:0')\n",
            "BATCH SIZE: 27\n",
            "loss: tensor(0.0708, device='cuda:0')\n",
            "SIZES:::: (1563,) (1563,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=19600, training_loss=0.00554052259773016, metrics={'train_runtime': 4780.4399, 'train_samples_per_second': 130.741, 'train_steps_per_second': 4.1, 'total_flos': 0.0, 'train_loss': 0.00554052259773016, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "print(\"Training\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOJdcsW67ug3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8511dee0-c8f3-4b22-c746-50099624b5e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef save_state(self):\\n\\n    if not self.is_world_process_zero():\\n        return\\n\\n    path = os.path.join(self.args.output_dir, \"trainer_state.json\")\\n    self.state.save_to_json(path)\\n\\ntrainer.save_state()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "print(\"Saving model\")\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.mkdir(OUTPUT_PATH)\n",
        "trainer.save_model(OUTPUT_PATH)\n",
        "\n",
        "\n",
        "# SE USA PARA QUE SE GUARDE UN ARCHIVO LLAMADO trainer_state.json DONDE SE GUARDAN LOS VALORES DE LAS METRICAS\n",
        "\"\"\"\n",
        "def save_state(self):\n",
        "\n",
        "    if not self.is_world_process_zero():\n",
        "        return\n",
        "\n",
        "    path = os.path.join(self.args.output_dir, \"trainer_state.json\")\n",
        "    self.state.save_to_json(path)\n",
        "\n",
        "trainer.save_state()\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}