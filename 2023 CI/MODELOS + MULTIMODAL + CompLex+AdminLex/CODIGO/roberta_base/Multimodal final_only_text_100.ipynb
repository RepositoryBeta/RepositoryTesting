{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688238646936,"user_tz":300,"elapsed":36637,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"7d3ba67d-e7dc-4f1c-e102-0dc3115b012c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688238655408,"user_tz":300,"elapsed":7321,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688238657045,"user_tz":300,"elapsed":239,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688238663487,"user_tz":300,"elapsed":4134,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688238665168,"user_tz":300,"elapsed":241,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688238666582,"user_tz":300,"elapsed":249,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688238668458,"user_tz":300,"elapsed":484,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"322d9475-711c-43f7-9bd1-2be84e5b90c4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688238670477,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    combine_feat_method='text_only',\n","    # combine_feat_method='concat',\n","    # combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    #combine_feat_method='attention_on_cat_and_numerical_feats',\n","    #combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688238682345,"user_tz":300,"elapsed":216,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688238694775,"user_tz":300,"elapsed":216,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"97811f63-3627-4285-fa0b-763f953e7f0b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688238698355,"user_tz":300,"elapsed":2136,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"b4e35397-7a43-4231-da94-866317e1454f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688238699752,"user_tz":300,"elapsed":228,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688238702775,"user_tz":300,"elapsed":1367,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"07ed024f-2489-41ae-cd13-17e586113be4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_classifier.layers.1.weight', 'tabular_classifier.bn.3.running_mean', 'classifier.dense.weight', 'tabular_classifier.bn.1.running_var', 'tabular_classifier.bn.0.running_var', 'classifier.dense.bias', 'tabular_classifier.bn.0.weight', 'tabular_classifier.layers.0.bias', 'tabular_classifier.bn.1.weight', 'tabular_classifier.layers.3.weight', 'tabular_classifier.bn.1.bias', 'tabular_classifier.bn.2.running_var', 'tabular_classifier.layers.0.weight', 'tabular_classifier.layers.2.bias', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_classifier.bn.1.num_batches_tracked', 'tabular_classifier.layers.1.bias', 'tabular_classifier.bn.1.running_mean', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_combiner.num_bn.running_var', 'tabular_classifier.bn.3.running_var', 'tabular_classifier.bn.2.weight', 'tabular_classifier.bn.3.num_batches_tracked', 'tabular_classifier.layers.4.weight', 'tabular_classifier.bn.2.running_mean', 'tabular_classifier.bn.3.weight', 'tabular_classifier.layers.4.bias', 'classifier.out_proj.weight', 'tabular_classifier.bn.0.running_mean', 'tabular_classifier.bn.2.bias', 'tabular_classifier.layers.3.bias', 'tabular_classifier.bn.3.bias', 'tabular_combiner.num_bn.bias', 'tabular_combiner.num_bn.weight', 'tabular_classifier.bn.0.bias', 'classifier.out_proj.bias', 'tabular_combiner.num_bn.running_mean', 'tabular_classifier.layers.2.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688238706386,"user_tz":300,"elapsed":243,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688238712996,"user_tz":300,"elapsed":4682,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688238717875,"user_tz":300,"elapsed":237,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"f37343db-6fab-48e8-fa9f-54208f4119c4"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'text_only'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688248009714,"user_tz":300,"elapsed":9289625,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"94f1a598-76e0-4e40-c3f9-031e93f93409"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:34:45, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.109700</td>\n","      <td>0.097600</td>\n","      <td>0.283149</td>\n","      <td>0.097600</td>\n","      <td>0.312410</td>\n","      <td>-4.583146</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.098600</td>\n","      <td>0.087114</td>\n","      <td>0.264138</td>\n","      <td>0.087114</td>\n","      <td>0.295151</td>\n","      <td>-3.983308</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.088000</td>\n","      <td>0.076421</td>\n","      <td>0.243354</td>\n","      <td>0.076421</td>\n","      <td>0.276443</td>\n","      <td>-3.371600</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.078000</td>\n","      <td>0.065558</td>\n","      <td>0.220427</td>\n","      <td>0.065558</td>\n","      <td>0.256042</td>\n","      <td>-2.750189</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.068300</td>\n","      <td>0.059714</td>\n","      <td>0.207306</td>\n","      <td>0.059714</td>\n","      <td>0.244365</td>\n","      <td>-2.415918</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.059800</td>\n","      <td>0.050832</td>\n","      <td>0.185839</td>\n","      <td>0.050832</td>\n","      <td>0.225460</td>\n","      <td>-1.907819</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.051900</td>\n","      <td>0.042249</td>\n","      <td>0.163295</td>\n","      <td>0.042249</td>\n","      <td>0.205545</td>\n","      <td>-1.416816</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.044900</td>\n","      <td>0.037001</td>\n","      <td>0.148896</td>\n","      <td>0.037001</td>\n","      <td>0.192356</td>\n","      <td>-1.116608</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.039500</td>\n","      <td>0.032412</td>\n","      <td>0.135861</td>\n","      <td>0.032412</td>\n","      <td>0.180033</td>\n","      <td>-0.854102</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.035000</td>\n","      <td>0.029031</td>\n","      <td>0.126127</td>\n","      <td>0.029031</td>\n","      <td>0.170385</td>\n","      <td>-0.660698</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.031600</td>\n","      <td>0.025315</td>\n","      <td>0.115839</td>\n","      <td>0.025315</td>\n","      <td>0.159105</td>\n","      <td>-0.448104</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.028700</td>\n","      <td>0.023527</td>\n","      <td>0.111146</td>\n","      <td>0.023527</td>\n","      <td>0.153386</td>\n","      <td>-0.345872</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.026700</td>\n","      <td>0.021470</td>\n","      <td>0.106297</td>\n","      <td>0.021470</td>\n","      <td>0.146527</td>\n","      <td>-0.228182</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.025100</td>\n","      <td>0.019633</td>\n","      <td>0.102522</td>\n","      <td>0.019633</td>\n","      <td>0.140118</td>\n","      <td>-0.123094</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.024100</td>\n","      <td>0.018899</td>\n","      <td>0.101309</td>\n","      <td>0.018899</td>\n","      <td>0.137475</td>\n","      <td>-0.081130</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.023100</td>\n","      <td>0.018833</td>\n","      <td>0.101190</td>\n","      <td>0.018833</td>\n","      <td>0.137234</td>\n","      <td>-0.077335</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.022500</td>\n","      <td>0.018607</td>\n","      <td>0.100911</td>\n","      <td>0.018607</td>\n","      <td>0.136408</td>\n","      <td>-0.064404</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.022000</td>\n","      <td>0.018029</td>\n","      <td>0.100527</td>\n","      <td>0.018029</td>\n","      <td>0.134272</td>\n","      <td>-0.031334</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.022200</td>\n","      <td>0.017752</td>\n","      <td>0.100804</td>\n","      <td>0.017752</td>\n","      <td>0.133237</td>\n","      <td>-0.015498</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.021600</td>\n","      <td>0.017663</td>\n","      <td>0.101031</td>\n","      <td>0.017663</td>\n","      <td>0.132902</td>\n","      <td>-0.010397</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.021500</td>\n","      <td>0.017930</td>\n","      <td>0.100557</td>\n","      <td>0.017930</td>\n","      <td>0.133904</td>\n","      <td>-0.025692</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.021200</td>\n","      <td>0.017645</td>\n","      <td>0.101087</td>\n","      <td>0.017645</td>\n","      <td>0.132835</td>\n","      <td>-0.009384</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.021200</td>\n","      <td>0.017716</td>\n","      <td>0.100883</td>\n","      <td>0.017716</td>\n","      <td>0.133102</td>\n","      <td>-0.013445</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.020800</td>\n","      <td>0.017686</td>\n","      <td>0.100964</td>\n","      <td>0.017686</td>\n","      <td>0.132989</td>\n","      <td>-0.011723</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.020900</td>\n","      <td>0.017875</td>\n","      <td>0.100608</td>\n","      <td>0.017875</td>\n","      <td>0.133696</td>\n","      <td>-0.022514</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.021000</td>\n","      <td>0.017698</td>\n","      <td>0.100931</td>\n","      <td>0.017698</td>\n","      <td>0.133034</td>\n","      <td>-0.012413</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.020800</td>\n","      <td>0.017565</td>\n","      <td>0.101424</td>\n","      <td>0.017565</td>\n","      <td>0.132534</td>\n","      <td>-0.004818</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.020900</td>\n","      <td>0.017689</td>\n","      <td>0.100955</td>\n","      <td>0.017689</td>\n","      <td>0.133000</td>\n","      <td>-0.011897</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.020500</td>\n","      <td>0.017486</td>\n","      <td>0.102366</td>\n","      <td>0.017486</td>\n","      <td>0.132236</td>\n","      <td>-0.000301</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.020600</td>\n","      <td>0.017611</td>\n","      <td>0.101207</td>\n","      <td>0.017611</td>\n","      <td>0.132708</td>\n","      <td>-0.007446</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.020300</td>\n","      <td>0.017503</td>\n","      <td>0.101988</td>\n","      <td>0.017503</td>\n","      <td>0.132300</td>\n","      <td>-0.001264</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.020300</td>\n","      <td>0.017601</td>\n","      <td>0.101252</td>\n","      <td>0.017601</td>\n","      <td>0.132668</td>\n","      <td>-0.006847</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.020700</td>\n","      <td>0.017676</td>\n","      <td>0.100994</td>\n","      <td>0.017676</td>\n","      <td>0.132950</td>\n","      <td>-0.011125</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.020600</td>\n","      <td>0.017668</td>\n","      <td>0.101015</td>\n","      <td>0.017668</td>\n","      <td>0.132922</td>\n","      <td>-0.010698</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.020400</td>\n","      <td>0.017531</td>\n","      <td>0.101652</td>\n","      <td>0.017531</td>\n","      <td>0.132405</td>\n","      <td>-0.002851</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.020200</td>\n","      <td>0.017554</td>\n","      <td>0.101493</td>\n","      <td>0.017554</td>\n","      <td>0.132491</td>\n","      <td>-0.004164</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.020100</td>\n","      <td>0.017509</td>\n","      <td>0.101904</td>\n","      <td>0.017509</td>\n","      <td>0.132322</td>\n","      <td>-0.001592</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.020100</td>\n","      <td>0.017682</td>\n","      <td>0.100974</td>\n","      <td>0.017682</td>\n","      <td>0.132975</td>\n","      <td>-0.011512</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.019900</td>\n","      <td>0.017781</td>\n","      <td>0.100746</td>\n","      <td>0.017781</td>\n","      <td>0.133345</td>\n","      <td>-0.017147</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.020300</td>\n","      <td>0.017489</td>\n","      <td>0.102289</td>\n","      <td>0.017489</td>\n","      <td>0.132245</td>\n","      <td>-0.000439</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.019800</td>\n","      <td>0.017572</td>\n","      <td>0.101390</td>\n","      <td>0.017572</td>\n","      <td>0.132558</td>\n","      <td>-0.005178</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.020000</td>\n","      <td>0.017500</td>\n","      <td>0.102048</td>\n","      <td>0.017500</td>\n","      <td>0.132286</td>\n","      <td>-0.001059</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.020300</td>\n","      <td>0.017550</td>\n","      <td>0.101519</td>\n","      <td>0.017550</td>\n","      <td>0.132475</td>\n","      <td>-0.003916</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.020100</td>\n","      <td>0.017727</td>\n","      <td>0.100858</td>\n","      <td>0.017727</td>\n","      <td>0.133144</td>\n","      <td>-0.014075</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.019800</td>\n","      <td>0.017590</td>\n","      <td>0.101303</td>\n","      <td>0.017590</td>\n","      <td>0.132627</td>\n","      <td>-0.006216</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.019900</td>\n","      <td>0.017706</td>\n","      <td>0.100910</td>\n","      <td>0.017706</td>\n","      <td>0.133064</td>\n","      <td>-0.012870</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.019600</td>\n","      <td>0.017481</td>\n","      <td>0.102819</td>\n","      <td>0.017481</td>\n","      <td>0.132217</td>\n","      <td>-0.000013</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.019900</td>\n","      <td>0.017673</td>\n","      <td>0.101001</td>\n","      <td>0.017673</td>\n","      <td>0.132941</td>\n","      <td>-0.010990</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.019600</td>\n","      <td>0.017682</td>\n","      <td>0.100974</td>\n","      <td>0.017682</td>\n","      <td>0.132975</td>\n","      <td>-0.011516</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.019700</td>\n","      <td>0.017526</td>\n","      <td>0.101702</td>\n","      <td>0.017526</td>\n","      <td>0.132385</td>\n","      <td>-0.002548</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.019700</td>\n","      <td>0.017532</td>\n","      <td>0.101644</td>\n","      <td>0.017532</td>\n","      <td>0.132409</td>\n","      <td>-0.002919</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.019700</td>\n","      <td>0.017539</td>\n","      <td>0.101595</td>\n","      <td>0.017539</td>\n","      <td>0.132434</td>\n","      <td>-0.003292</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.019800</td>\n","      <td>0.017482</td>\n","      <td>0.102584</td>\n","      <td>0.017482</td>\n","      <td>0.132219</td>\n","      <td>-0.000046</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.019600</td>\n","      <td>0.017706</td>\n","      <td>0.100908</td>\n","      <td>0.017706</td>\n","      <td>0.133066</td>\n","      <td>-0.012889</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.019600</td>\n","      <td>0.017577</td>\n","      <td>0.101364</td>\n","      <td>0.017577</td>\n","      <td>0.132577</td>\n","      <td>-0.005465</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.019300</td>\n","      <td>0.017577</td>\n","      <td>0.101362</td>\n","      <td>0.017577</td>\n","      <td>0.132579</td>\n","      <td>-0.005496</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.019700</td>\n","      <td>0.017635</td>\n","      <td>0.101122</td>\n","      <td>0.017635</td>\n","      <td>0.132795</td>\n","      <td>-0.008779</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.019500</td>\n","      <td>0.017501</td>\n","      <td>0.102018</td>\n","      <td>0.017501</td>\n","      <td>0.132292</td>\n","      <td>-0.001153</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.019700</td>\n","      <td>0.017495</td>\n","      <td>0.103441</td>\n","      <td>0.017495</td>\n","      <td>0.132269</td>\n","      <td>-0.000794</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.019600</td>\n","      <td>0.017486</td>\n","      <td>0.103158</td>\n","      <td>0.017486</td>\n","      <td>0.132236</td>\n","      <td>-0.000298</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.019700</td>\n","      <td>0.017657</td>\n","      <td>0.105440</td>\n","      <td>0.017657</td>\n","      <td>0.132878</td>\n","      <td>-0.010031</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.019500</td>\n","      <td>0.017492</td>\n","      <td>0.102203</td>\n","      <td>0.017492</td>\n","      <td>0.132257</td>\n","      <td>-0.000618</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.019700</td>\n","      <td>0.017496</td>\n","      <td>0.103463</td>\n","      <td>0.017496</td>\n","      <td>0.132272</td>\n","      <td>-0.000843</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.019500</td>\n","      <td>0.017542</td>\n","      <td>0.101574</td>\n","      <td>0.017542</td>\n","      <td>0.132445</td>\n","      <td>-0.003463</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.019200</td>\n","      <td>0.017586</td>\n","      <td>0.104761</td>\n","      <td>0.017586</td>\n","      <td>0.132614</td>\n","      <td>-0.006022</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.019300</td>\n","      <td>0.017496</td>\n","      <td>0.103470</td>\n","      <td>0.017496</td>\n","      <td>0.132273</td>\n","      <td>-0.000865</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.019400</td>\n","      <td>0.017508</td>\n","      <td>0.103733</td>\n","      <td>0.017508</td>\n","      <td>0.132319</td>\n","      <td>-0.001561</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.019400</td>\n","      <td>0.017522</td>\n","      <td>0.103969</td>\n","      <td>0.017522</td>\n","      <td>0.132371</td>\n","      <td>-0.002347</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.019500</td>\n","      <td>0.017600</td>\n","      <td>0.104893</td>\n","      <td>0.017600</td>\n","      <td>0.132664</td>\n","      <td>-0.006787</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.019600</td>\n","      <td>0.017486</td>\n","      <td>0.103155</td>\n","      <td>0.017486</td>\n","      <td>0.132234</td>\n","      <td>-0.000273</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.019600</td>\n","      <td>0.017495</td>\n","      <td>0.103441</td>\n","      <td>0.017495</td>\n","      <td>0.132269</td>\n","      <td>-0.000793</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.019500</td>\n","      <td>0.017565</td>\n","      <td>0.104528</td>\n","      <td>0.017565</td>\n","      <td>0.132532</td>\n","      <td>-0.004782</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.019300</td>\n","      <td>0.017496</td>\n","      <td>0.103451</td>\n","      <td>0.017496</td>\n","      <td>0.132271</td>\n","      <td>-0.000823</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.019400</td>\n","      <td>0.017495</td>\n","      <td>0.103436</td>\n","      <td>0.017495</td>\n","      <td>0.132268</td>\n","      <td>-0.000789</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.019200</td>\n","      <td>0.017554</td>\n","      <td>0.101492</td>\n","      <td>0.017554</td>\n","      <td>0.132492</td>\n","      <td>-0.004168</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.019200</td>\n","      <td>0.017481</td>\n","      <td>0.102650</td>\n","      <td>0.017481</td>\n","      <td>0.132217</td>\n","      <td>-0.000016</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.019200</td>\n","      <td>0.017547</td>\n","      <td>0.104317</td>\n","      <td>0.017547</td>\n","      <td>0.132466</td>\n","      <td>-0.003787</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.019100</td>\n","      <td>0.017563</td>\n","      <td>0.101436</td>\n","      <td>0.017563</td>\n","      <td>0.132527</td>\n","      <td>-0.004706</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.018900</td>\n","      <td>0.017503</td>\n","      <td>0.103613</td>\n","      <td>0.017503</td>\n","      <td>0.132297</td>\n","      <td>-0.001223</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.019100</td>\n","      <td>0.017534</td>\n","      <td>0.104139</td>\n","      <td>0.017534</td>\n","      <td>0.132415</td>\n","      <td>-0.003014</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.019400</td>\n","      <td>0.017516</td>\n","      <td>0.103870</td>\n","      <td>0.017516</td>\n","      <td>0.132349</td>\n","      <td>-0.002008</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.019000</td>\n","      <td>0.017494</td>\n","      <td>0.103399</td>\n","      <td>0.017494</td>\n","      <td>0.132263</td>\n","      <td>-0.000710</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.019300</td>\n","      <td>0.017728</td>\n","      <td>0.106054</td>\n","      <td>0.017728</td>\n","      <td>0.133147</td>\n","      <td>-0.014132</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.019200</td>\n","      <td>0.017529</td>\n","      <td>0.104067</td>\n","      <td>0.017529</td>\n","      <td>0.132396</td>\n","      <td>-0.002725</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.019200</td>\n","      <td>0.017516</td>\n","      <td>0.103870</td>\n","      <td>0.017516</td>\n","      <td>0.132349</td>\n","      <td>-0.002008</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.019200</td>\n","      <td>0.017587</td>\n","      <td>0.104768</td>\n","      <td>0.017587</td>\n","      <td>0.132616</td>\n","      <td>-0.006057</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.019200</td>\n","      <td>0.017541</td>\n","      <td>0.104231</td>\n","      <td>0.017541</td>\n","      <td>0.132441</td>\n","      <td>-0.003408</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.019000</td>\n","      <td>0.017590</td>\n","      <td>0.104802</td>\n","      <td>0.017590</td>\n","      <td>0.132629</td>\n","      <td>-0.006253</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.019200</td>\n","      <td>0.017524</td>\n","      <td>0.103990</td>\n","      <td>0.017524</td>\n","      <td>0.132377</td>\n","      <td>-0.002433</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.019400</td>\n","      <td>0.017557</td>\n","      <td>0.104441</td>\n","      <td>0.017557</td>\n","      <td>0.132504</td>\n","      <td>-0.004358</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.019100</td>\n","      <td>0.017520</td>\n","      <td>0.103925</td>\n","      <td>0.017520</td>\n","      <td>0.132362</td>\n","      <td>-0.002198</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.019100</td>\n","      <td>0.017597</td>\n","      <td>0.104865</td>\n","      <td>0.017597</td>\n","      <td>0.132653</td>\n","      <td>-0.006620</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.019200</td>\n","      <td>0.017583</td>\n","      <td>0.104730</td>\n","      <td>0.017583</td>\n","      <td>0.132602</td>\n","      <td>-0.005848</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.019200</td>\n","      <td>0.017558</td>\n","      <td>0.104452</td>\n","      <td>0.017558</td>\n","      <td>0.132507</td>\n","      <td>-0.004409</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.019200</td>\n","      <td>0.017539</td>\n","      <td>0.104210</td>\n","      <td>0.017539</td>\n","      <td>0.132436</td>\n","      <td>-0.003319</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.019300</td>\n","      <td>0.017592</td>\n","      <td>0.104814</td>\n","      <td>0.017592</td>\n","      <td>0.132633</td>\n","      <td>-0.006319</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.019300</td>\n","      <td>0.017538</td>\n","      <td>0.104193</td>\n","      <td>0.017538</td>\n","      <td>0.132431</td>\n","      <td>-0.003243</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.019400</td>\n","      <td>0.017545</td>\n","      <td>0.104285</td>\n","      <td>0.017545</td>\n","      <td>0.132457</td>\n","      <td>-0.003646</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.019100</td>\n","      <td>0.017559</td>\n","      <td>0.104461</td>\n","      <td>0.017559</td>\n","      <td>0.132510</td>\n","      <td>-0.004453</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.019000</td>\n","      <td>0.017561</td>\n","      <td>0.104486</td>\n","      <td>0.017561</td>\n","      <td>0.132519</td>\n","      <td>-0.004578</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 34min 21s, sys: 57.3 s, total: 2h 35min 19s\n","Wall time: 2h 34min 49s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.024998704609386423, metrics={'train_runtime': 9289.3862, 'train_samples_per_second': 64.278, 'train_steps_per_second': 2.013, 'total_flos': 1.1435629531772851e+17, 'train_loss': 0.024998704609386423, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688248093506,"user_tz":300,"elapsed":215,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688248097661,"user_tz":300,"elapsed":1407,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688248105542,"user_tz":300,"elapsed":4491,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"cbfc6636-c1fc-4667-847c-e2a042226ae8"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.01619710959494114,\n"," 'eval_MAE': 0.10214120853403805,\n"," 'eval_MSE': 0.016197110855012534,\n"," 'eval_RMSE': 0.12726787047410093,\n"," 'eval_R2': -0.019692959308356572,\n"," 'eval_runtime': 4.3183,\n"," 'eval_samples_per_second': 172.984,\n"," 'eval_steps_per_second': 21.768,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":23}]}]}