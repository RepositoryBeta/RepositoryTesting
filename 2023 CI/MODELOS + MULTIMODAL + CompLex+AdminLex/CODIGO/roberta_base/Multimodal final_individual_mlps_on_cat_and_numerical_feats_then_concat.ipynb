{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d41Pt5A2BV2l"},"outputs":[],"source":["!pip install multimodal-transformers\n","!pip install --upgrade accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","source":["!git clone https://huggingface.co/roberta-base"],"metadata":{"id":"Uyg3tdj8B-d6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308813128,"user_tz":300,"elapsed":45802,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"fd711c71-6c8b-4d34-f6e2-7d4f6082df49"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'roberta-base'...\n","remote: Enumerating objects: 81, done.\u001b[K\n","remote: Counting objects: 100% (3/3), done.\u001b[K\n","remote: Compressing objects: 100% (3/3), done.\u001b[K\n","remote: Total 81 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n","Unpacking objects: 100% (81/81), 1.63 MiB | 1.67 MiB/s, done.\n","Filtering content: 100% (5/5), 2.61 GiB | 61.80 MiB/s, done.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9bW9UKIRCAuK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308839836,"user_tz":300,"elapsed":22202,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"0e81eb8e-992c-4ffe-d2d7-9d6b46fb4d67"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from dataclasses import dataclass, field\n","import logging\n","import os\n","from typing import Optional\n","\n","import numpy as np\n","import pandas as pd\n","from transformers import (\n","    AutoTokenizer,\n","    AutoConfig,\n","    Trainer,\n","    EvalPrediction,\n","    set_seed\n",")\n","\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","from transformers.training_args import TrainingArguments\n","\n","from multimodal_transformers.data import load_data_from_folder\n","from multimodal_transformers.model import TabularConfig\n","from multimodal_transformers.model import RobertaWithTabular\n","from sklearn.preprocessing import LabelEncoder\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from math import sqrt\n","\n","logging.basicConfig(level=logging.INFO)\n","os.environ['COMET_MODE'] = 'DISABLED'"],"metadata":{"id":"er4girZHCCji","executionInfo":{"status":"ok","timestamp":1688308850933,"user_tz":300,"elapsed":7885,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["DATA_PATH = '/content/drive/MyDrive/Bert_complex/corpus/Complex.xlsx'\n","#DATA_PATH = 'Prueba.tsv'"],"metadata":{"id":"Cdv18i0gCVai","executionInfo":{"status":"ok","timestamp":1688308852329,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel(DATA_PATH)"],"metadata":{"id":"Z4fkGFPKCWu6","executionInfo":{"status":"ok","timestamp":1688308858209,"user_tz":300,"elapsed":4522,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['sentence'] = df['sentence'].str.replace('\"', '').str.replace(\"'\", '')"],"metadata":{"id":"Cxqsifa5CbvK","executionInfo":{"status":"ok","timestamp":1688308860315,"user_tz":300,"elapsed":455,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df['sentence_token'] = df.apply(\n","      lambda x: str(x['sentence']).lower()+ ' </s> ' + str(x['token']).lower(),\n","      axis=1)"],"metadata":{"id":"tiofhfmSCepC","executionInfo":{"status":"ok","timestamp":1688308861362,"user_tz":300,"elapsed":521,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n","print('Num ejemplos train-val-test')\n","print(len(train_df), len(val_df), len(test_df))\n","train_df.to_csv('train.csv')\n","val_df.to_csv('val.csv')\n","test_df.to_csv('test.csv')"],"metadata":{"id":"SnQezY-lCibq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308863304,"user_tz":300,"elapsed":493,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"ec0bb9d2-0db9-4cc6-d429-e4ca14a57cf7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Num ejemplos train-val-test\n","5971 746 747\n"]}]},{"cell_type":"code","source":["@dataclass\n","class MultimodalDataTrainingArguments:\n","  \"\"\"\n","  Arguments pertaining to how we combine tabular features\n","  Using `HfArgumentParser` we can turn this class\n","  into argparse arguments to be able to specify them on\n","  the command line.\n","  \"\"\"\n","\n","  data_path: str = field(metadata={\n","                            'help': 'the path to the csv file containing the dataset'\n","                        })\n","  column_info_path: str = field(\n","      default=None,\n","      metadata={\n","          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n","  })\n","\n","  column_info: dict = field(\n","      default=None,\n","      metadata={\n","          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n","                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n","  })\n","\n","  categorical_encode_type: str = field(default='none',\n","                                        metadata={\n","                                            'help': 'sklearn encoder to use for categorical data',\n","                                            'choices': ['ohe', 'binary', 'label', 'none']\n","                                        })\n","  numerical_transformer_method: str = field(default='yeo_johnson',\n","                                            metadata={\n","                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n","                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n","                                            })\n","  task: str = field(default=\"regression\",\n","                    metadata={\n","                        \"help\": \"The downstream training task\",\n","                        \"choices\": [\"classification\", \"regression\"]\n","                    })\n","\n","  mlp_division: int = field(default=4,\n","                            metadata={\n","                                'help': 'the ratio of the number of '\n","                                        'hidden dims in a current layer to the next MLP layer'\n","                            })\n","  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","                                    metadata={\n","                                        'help': 'method to combine categorical and numerical features, '\n","                                                'see README for all the method'\n","                                    })\n","  mlp_dropout: float = field(default=0.1,\n","                              metadata={\n","                                'help': 'dropout ratio used for MLP layers'\n","                              })\n","  numerical_bn: bool = field(default=True,\n","                              metadata={\n","                                  'help': 'whether to use batchnorm on numerical features'\n","                              })\n","  use_simple_classifier: str = field(default=False,\n","                                      metadata={\n","                                          'help': 'whether to use single layer or MLP as final classifier'\n","                                      })\n","  mlp_act: str = field(default='relu',\n","                        metadata={\n","                            'help': 'the activation function to use for finetuning layers',\n","                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n","                        })\n","  gating_beta: float = field(default=0.2,\n","                              metadata={\n","                                  'help': \"the beta hyperparameters used for gating tabular data \"\n","                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n","                              })"],"metadata":{"id":"qZ9Pkm2lCmEK","executionInfo":{"status":"ok","timestamp":1688308864779,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model_name = 'roberta-base'\n","\n","column_info_dict = {\n","    'text_cols': ['corpus','sentence_token'],\n","    'num_cols': ['abs_frecuency','rel_frecuency','length','number_syllables','token_possition','number_token_sentences','number_synonyms',\n","                 'number_hyponyms','number_hypernyms','Part_of_speech','freq_relative_word_before','freq_relative_word_after','len_word_before',\n","                 'len_word_after','mtld_diversity','propn','aux','verb','adp','noun','nn','sym','num'],\n","    'label_col': ['complexity']\n","}\n","\n","data_args = MultimodalDataTrainingArguments(\n","    data_path='.',\n","    # combine_feat_method='text_only',\n","    # combine_feat_method='concat',\n","    combine_feat_method='individual_mlps_on_cat_and_numerical_feats_then_concat',\n","    #combine_feat_method='attention_on_cat_and_numerical_feats',\n","    #combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n","    #combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n","    column_info=column_info_dict,\n","    task='regression',\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs/model_name\",\n","    logging_dir=\"./logs/runs\",\n","    overwrite_output_dir=True,\n","    do_train=True,\n","    do_eval=True,\n","    per_device_train_batch_size=32,\n","    num_train_epochs=100,\n","    evaluation_strategy='epoch',\n","    logging_strategy='epoch',\n","    logging_steps=16,\n","    eval_steps=5\n",")\n","\n","set_seed(training_args.seed)"],"metadata":{"id":"Og1amekrCuDK","executionInfo":{"status":"ok","timestamp":1688308867567,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","print('Specified tokenizer: ', model_name)"],"metadata":{"id":"SuJTJ5P0DBn6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308870947,"user_tz":300,"elapsed":3,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"6f88caa4-68bd-40cb-fd28-a54df6f2e099"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Specified tokenizer:  roberta-base\n"]}]},{"cell_type":"code","source":["# Get Datasets\n","train_dataset, val_dataset, test_dataset = load_data_from_folder(\n","    data_args.data_path,\n","    data_args.column_info['text_cols'],\n","    tokenizer,\n","    label_col=data_args.column_info['label_col'],\n","    label_list = None,\n","    categorical_cols = None,\n","    numerical_transformer_method = 'yeo_johnson',\n","    numerical_cols=data_args.column_info['num_cols'],\n","    sep_text_token_str=' </s> ',\n","    categorical_encode_type = None\n",")"],"metadata":{"id":"FjkrhWMkDGFD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308874273,"user_tz":300,"elapsed":1887,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"bca3d04b-2f41-48d9-e180-54b633fc3666"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:3253: RuntimeWarning: divide by zero encountered in log\n","  loglike = -n_samples / 2 * np.log(x_trans.var())\n"]}]},{"cell_type":"code","source":["config = AutoConfig.from_pretrained(model_name)\n","tabular_config = TabularConfig(num_labels=1,\n","                               #cat_feat_dim=train_dataset.cat_feats.shape[1],\n","                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n","                               **vars(data_args))\n","config.tabular_config = tabular_config"],"metadata":{"id":"PSHHjioQDMda","executionInfo":{"status":"ok","timestamp":1688308875726,"user_tz":300,"elapsed":2,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model = RobertaWithTabular.from_pretrained(\n","        model_name,\n","        config=config\n","    )"],"metadata":{"id":"qhO4UbUqDQnz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688308879172,"user_tz":300,"elapsed":1501,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"4ccdb03f-8071-4e60-e425-6cd0b821f0fe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaWithTabular: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaWithTabular were not initialized from the model checkpoint at roberta-base and are newly initialized: ['tabular_combiner.num_mlp.layers.1.weight', 'tabular_combiner.num_mlp.layers.0.bias', 'tabular_combiner.num_mlp.bn.0.bias', 'tabular_combiner.num_mlp.bn.0.weight', 'tabular_classifier.layers.0.bias', 'tabular_combiner.num_mlp.bn.0.running_var', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'tabular_classifier.bn.0.weight', 'tabular_combiner.num_bn.weight', 'tabular_classifier.bn.0.running_var', 'tabular_classifier.bn.2.running_mean', 'tabular_classifier.bn.1.num_batches_tracked', 'tabular_classifier.bn.3.num_batches_tracked', 'classifier.dense.bias', 'tabular_classifier.bn.2.weight', 'tabular_classifier.bn.1.running_mean', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_classifier.layers.4.bias', 'tabular_classifier.bn.2.bias', 'tabular_combiner.num_bn.bias', 'tabular_classifier.bn.2.num_batches_tracked', 'tabular_classifier.bn.1.weight', 'tabular_classifier.layers.0.weight', 'tabular_classifier.bn.2.running_var', 'tabular_combiner.num_mlp.bn.0.running_mean', 'tabular_classifier.layers.3.bias', 'tabular_classifier.bn.3.running_var', 'tabular_combiner.num_bn.running_var', 'tabular_classifier.layers.1.weight', 'tabular_classifier.bn.3.weight', 'tabular_classifier.bn.3.running_mean', 'tabular_classifier.bn.1.bias', 'tabular_classifier.bn.0.num_batches_tracked', 'tabular_combiner.num_mlp.layers.0.weight', 'tabular_combiner.num_mlp.layers.1.bias', 'tabular_classifier.bn.0.running_mean', 'tabular_classifier.bn.1.running_var', 'tabular_classifier.layers.2.weight', 'tabular_classifier.bn.0.bias', 'tabular_classifier.bn.3.bias', 'tabular_combiner.num_mlp.bn.0.num_batches_tracked', 'tabular_combiner.num_bn.running_mean', 'tabular_classifier.layers.1.bias', 'classifier.dense.weight', 'tabular_classifier.layers.3.weight', 'tabular_classifier.layers.4.weight', 'tabular_classifier.layers.2.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def calc_regression_metrics(p: EvalPrediction):\n","    predictions = p.predictions[0]\n","    preds = np.squeeze(predictions)\n","    labels = np.squeeze(p.label_ids)\n","    mse = metrics.mean_squared_error(labels, preds)\n","    rmse = sqrt(mse)\n","    mae = metrics.mean_absolute_error(labels, preds)\n","    return {\n","        \"MAE\": mae,\n","        \"MSE\": mse,\n","        \"RMSE\": rmse,\n","        'R2': metrics.r2_score(labels, preds)\n","    }"],"metadata":{"id":"dNy5UYWMDVha","executionInfo":{"status":"ok","timestamp":1688308879634,"user_tz":300,"elapsed":1,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    compute_metrics=calc_regression_metrics\n",")"],"metadata":{"id":"LZXXseCyDWeK","executionInfo":{"status":"ok","timestamp":1688308887505,"user_tz":300,"elapsed":5343,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["data_args.combine_feat_method"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"OjTnZXQqmUIP","executionInfo":{"status":"ok","timestamp":1688308887506,"user_tz":300,"elapsed":17,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"27c4967f-02ca-4e86-b395-d2775f69e6e0"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'individual_mlps_on_cat_and_numerical_feats_then_concat'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["%%time\n","trainer.train()"],"metadata":{"id":"gcUhKE-vDzHr","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1688318310959,"user_tz":300,"elapsed":9415288,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"7680dec2-86dc-41a6-fbcf-a7ba611efb00"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='18700' max='18700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18700/18700 2:36:50, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Mae</th>\n","      <th>Mse</th>\n","      <th>Rmse</th>\n","      <th>R2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.097600</td>\n","      <td>0.096904</td>\n","      <td>0.281638</td>\n","      <td>0.096904</td>\n","      <td>0.311294</td>\n","      <td>-4.137360</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.084800</td>\n","      <td>0.077326</td>\n","      <td>0.251760</td>\n","      <td>0.077326</td>\n","      <td>0.278075</td>\n","      <td>-3.099421</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.070000</td>\n","      <td>0.046807</td>\n","      <td>0.192898</td>\n","      <td>0.046807</td>\n","      <td>0.216349</td>\n","      <td>-1.481455</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.059400</td>\n","      <td>0.046708</td>\n","      <td>0.191038</td>\n","      <td>0.046708</td>\n","      <td>0.216121</td>\n","      <td>-1.476234</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.048200</td>\n","      <td>0.038859</td>\n","      <td>0.173331</td>\n","      <td>0.038859</td>\n","      <td>0.197126</td>\n","      <td>-1.060077</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.040000</td>\n","      <td>0.052539</td>\n","      <td>0.207302</td>\n","      <td>0.052539</td>\n","      <td>0.229213</td>\n","      <td>-1.785323</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.033400</td>\n","      <td>0.034746</td>\n","      <td>0.160122</td>\n","      <td>0.034746</td>\n","      <td>0.186404</td>\n","      <td>-0.842079</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.029100</td>\n","      <td>0.020922</td>\n","      <td>0.119283</td>\n","      <td>0.020922</td>\n","      <td>0.144645</td>\n","      <td>-0.109186</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.024100</td>\n","      <td>0.021249</td>\n","      <td>0.120909</td>\n","      <td>0.021249</td>\n","      <td>0.145772</td>\n","      <td>-0.126534</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.020900</td>\n","      <td>0.018840</td>\n","      <td>0.112652</td>\n","      <td>0.018840</td>\n","      <td>0.137260</td>\n","      <td>0.001187</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.018400</td>\n","      <td>0.017279</td>\n","      <td>0.106105</td>\n","      <td>0.017279</td>\n","      <td>0.131449</td>\n","      <td>0.083971</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.016700</td>\n","      <td>0.009972</td>\n","      <td>0.075337</td>\n","      <td>0.009972</td>\n","      <td>0.099857</td>\n","      <td>0.471362</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.014700</td>\n","      <td>0.017862</td>\n","      <td>0.110874</td>\n","      <td>0.017862</td>\n","      <td>0.133650</td>\n","      <td>0.053031</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.013400</td>\n","      <td>0.013609</td>\n","      <td>0.093220</td>\n","      <td>0.013609</td>\n","      <td>0.116655</td>\n","      <td>0.278548</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.011700</td>\n","      <td>0.013149</td>\n","      <td>0.091244</td>\n","      <td>0.013149</td>\n","      <td>0.114669</td>\n","      <td>0.302907</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>0.011700</td>\n","      <td>0.009277</td>\n","      <td>0.074282</td>\n","      <td>0.009277</td>\n","      <td>0.096318</td>\n","      <td>0.508173</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>0.010300</td>\n","      <td>0.013625</td>\n","      <td>0.090875</td>\n","      <td>0.013625</td>\n","      <td>0.116728</td>\n","      <td>0.277648</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>0.010600</td>\n","      <td>0.012113</td>\n","      <td>0.086854</td>\n","      <td>0.012113</td>\n","      <td>0.110059</td>\n","      <td>0.357833</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>0.012200</td>\n","      <td>0.011468</td>\n","      <td>0.081395</td>\n","      <td>0.011468</td>\n","      <td>0.107088</td>\n","      <td>0.392032</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.013400</td>\n","      <td>0.019462</td>\n","      <td>0.104808</td>\n","      <td>0.019462</td>\n","      <td>0.139507</td>\n","      <td>-0.031790</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>0.013000</td>\n","      <td>0.010428</td>\n","      <td>0.078108</td>\n","      <td>0.010428</td>\n","      <td>0.102120</td>\n","      <td>0.447139</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>0.009800</td>\n","      <td>0.012859</td>\n","      <td>0.088133</td>\n","      <td>0.012859</td>\n","      <td>0.113398</td>\n","      <td>0.318277</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>0.014200</td>\n","      <td>0.012412</td>\n","      <td>0.087304</td>\n","      <td>0.012412</td>\n","      <td>0.111408</td>\n","      <td>0.341989</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>0.010700</td>\n","      <td>0.008754</td>\n","      <td>0.071190</td>\n","      <td>0.008754</td>\n","      <td>0.093560</td>\n","      <td>0.535932</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.008400</td>\n","      <td>0.009612</td>\n","      <td>0.076352</td>\n","      <td>0.009612</td>\n","      <td>0.098040</td>\n","      <td>0.490426</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.008500</td>\n","      <td>0.008698</td>\n","      <td>0.071293</td>\n","      <td>0.008698</td>\n","      <td>0.093265</td>\n","      <td>0.538861</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.008800</td>\n","      <td>0.008083</td>\n","      <td>0.068202</td>\n","      <td>0.008083</td>\n","      <td>0.089903</td>\n","      <td>0.571504</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.008000</td>\n","      <td>0.007554</td>\n","      <td>0.066048</td>\n","      <td>0.007554</td>\n","      <td>0.086912</td>\n","      <td>0.599544</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.007400</td>\n","      <td>0.010529</td>\n","      <td>0.079076</td>\n","      <td>0.010529</td>\n","      <td>0.102609</td>\n","      <td>0.441833</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.007600</td>\n","      <td>0.012349</td>\n","      <td>0.086445</td>\n","      <td>0.012349</td>\n","      <td>0.111125</td>\n","      <td>0.345331</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.007100</td>\n","      <td>0.008125</td>\n","      <td>0.068839</td>\n","      <td>0.008125</td>\n","      <td>0.090138</td>\n","      <td>0.569264</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.007100</td>\n","      <td>0.009396</td>\n","      <td>0.076414</td>\n","      <td>0.009396</td>\n","      <td>0.096934</td>\n","      <td>0.501867</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.006800</td>\n","      <td>0.008381</td>\n","      <td>0.071333</td>\n","      <td>0.008381</td>\n","      <td>0.091549</td>\n","      <td>0.555675</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.006800</td>\n","      <td>0.012810</td>\n","      <td>0.088200</td>\n","      <td>0.012810</td>\n","      <td>0.113181</td>\n","      <td>0.320887</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.006500</td>\n","      <td>0.010167</td>\n","      <td>0.077165</td>\n","      <td>0.010167</td>\n","      <td>0.100832</td>\n","      <td>0.460992</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.006500</td>\n","      <td>0.009108</td>\n","      <td>0.074045</td>\n","      <td>0.009108</td>\n","      <td>0.095438</td>\n","      <td>0.517121</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.006200</td>\n","      <td>0.007871</td>\n","      <td>0.067953</td>\n","      <td>0.007871</td>\n","      <td>0.088721</td>\n","      <td>0.582698</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.006200</td>\n","      <td>0.007976</td>\n","      <td>0.068017</td>\n","      <td>0.007976</td>\n","      <td>0.089307</td>\n","      <td>0.577168</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.006400</td>\n","      <td>0.008035</td>\n","      <td>0.069365</td>\n","      <td>0.008035</td>\n","      <td>0.089639</td>\n","      <td>0.574020</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.006100</td>\n","      <td>0.008459</td>\n","      <td>0.070533</td>\n","      <td>0.008459</td>\n","      <td>0.091975</td>\n","      <td>0.551531</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.006100</td>\n","      <td>0.009384</td>\n","      <td>0.072300</td>\n","      <td>0.009384</td>\n","      <td>0.096873</td>\n","      <td>0.502494</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.006200</td>\n","      <td>0.008451</td>\n","      <td>0.071036</td>\n","      <td>0.008451</td>\n","      <td>0.091929</td>\n","      <td>0.551972</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.006100</td>\n","      <td>0.009054</td>\n","      <td>0.072869</td>\n","      <td>0.009054</td>\n","      <td>0.095153</td>\n","      <td>0.519998</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.006300</td>\n","      <td>0.008497</td>\n","      <td>0.069138</td>\n","      <td>0.008497</td>\n","      <td>0.092178</td>\n","      <td>0.549544</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.005800</td>\n","      <td>0.015114</td>\n","      <td>0.097298</td>\n","      <td>0.015114</td>\n","      <td>0.122940</td>\n","      <td>0.198719</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.007300</td>\n","      <td>0.012788</td>\n","      <td>0.084276</td>\n","      <td>0.012788</td>\n","      <td>0.113084</td>\n","      <td>0.322045</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.006100</td>\n","      <td>0.008508</td>\n","      <td>0.070322</td>\n","      <td>0.008508</td>\n","      <td>0.092241</td>\n","      <td>0.548924</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.006100</td>\n","      <td>0.010927</td>\n","      <td>0.078954</td>\n","      <td>0.010927</td>\n","      <td>0.104531</td>\n","      <td>0.420724</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.006100</td>\n","      <td>0.009238</td>\n","      <td>0.073254</td>\n","      <td>0.009238</td>\n","      <td>0.096112</td>\n","      <td>0.510271</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.005900</td>\n","      <td>0.007904</td>\n","      <td>0.069781</td>\n","      <td>0.007904</td>\n","      <td>0.088903</td>\n","      <td>0.580981</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.005900</td>\n","      <td>0.007938</td>\n","      <td>0.067949</td>\n","      <td>0.007938</td>\n","      <td>0.089094</td>\n","      <td>0.579183</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.005700</td>\n","      <td>0.008102</td>\n","      <td>0.068884</td>\n","      <td>0.008102</td>\n","      <td>0.090013</td>\n","      <td>0.570452</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.005500</td>\n","      <td>0.008854</td>\n","      <td>0.073271</td>\n","      <td>0.008854</td>\n","      <td>0.094096</td>\n","      <td>0.530607</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.005900</td>\n","      <td>0.012007</td>\n","      <td>0.077870</td>\n","      <td>0.012007</td>\n","      <td>0.109576</td>\n","      <td>0.363458</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.005600</td>\n","      <td>0.007752</td>\n","      <td>0.067326</td>\n","      <td>0.007752</td>\n","      <td>0.088047</td>\n","      <td>0.589017</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.005900</td>\n","      <td>0.011029</td>\n","      <td>0.080169</td>\n","      <td>0.011029</td>\n","      <td>0.105017</td>\n","      <td>0.415320</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.006200</td>\n","      <td>0.011945</td>\n","      <td>0.084925</td>\n","      <td>0.011945</td>\n","      <td>0.109292</td>\n","      <td>0.366757</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.005600</td>\n","      <td>0.007590</td>\n","      <td>0.066742</td>\n","      <td>0.007590</td>\n","      <td>0.087120</td>\n","      <td>0.597627</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.005500</td>\n","      <td>0.008469</td>\n","      <td>0.071169</td>\n","      <td>0.008469</td>\n","      <td>0.092026</td>\n","      <td>0.551030</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.005600</td>\n","      <td>0.011421</td>\n","      <td>0.083540</td>\n","      <td>0.011421</td>\n","      <td>0.106868</td>\n","      <td>0.394531</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.005300</td>\n","      <td>0.007570</td>\n","      <td>0.065983</td>\n","      <td>0.007570</td>\n","      <td>0.087005</td>\n","      <td>0.598689</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.004900</td>\n","      <td>0.008195</td>\n","      <td>0.069708</td>\n","      <td>0.008195</td>\n","      <td>0.090526</td>\n","      <td>0.565543</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.005100</td>\n","      <td>0.008430</td>\n","      <td>0.071854</td>\n","      <td>0.008430</td>\n","      <td>0.091815</td>\n","      <td>0.553085</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.005000</td>\n","      <td>0.010068</td>\n","      <td>0.078679</td>\n","      <td>0.010068</td>\n","      <td>0.100339</td>\n","      <td>0.466253</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.004900</td>\n","      <td>0.007480</td>\n","      <td>0.065837</td>\n","      <td>0.007480</td>\n","      <td>0.086485</td>\n","      <td>0.603471</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.004900</td>\n","      <td>0.008139</td>\n","      <td>0.069513</td>\n","      <td>0.008139</td>\n","      <td>0.090216</td>\n","      <td>0.568513</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.004900</td>\n","      <td>0.008424</td>\n","      <td>0.070720</td>\n","      <td>0.008424</td>\n","      <td>0.091784</td>\n","      <td>0.553384</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.005000</td>\n","      <td>0.008099</td>\n","      <td>0.069526</td>\n","      <td>0.008099</td>\n","      <td>0.089993</td>\n","      <td>0.570644</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.004600</td>\n","      <td>0.007901</td>\n","      <td>0.068316</td>\n","      <td>0.007901</td>\n","      <td>0.088889</td>\n","      <td>0.581113</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.004800</td>\n","      <td>0.007980</td>\n","      <td>0.067607</td>\n","      <td>0.007980</td>\n","      <td>0.089333</td>\n","      <td>0.576921</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.004800</td>\n","      <td>0.007813</td>\n","      <td>0.067318</td>\n","      <td>0.007813</td>\n","      <td>0.088392</td>\n","      <td>0.585792</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.004600</td>\n","      <td>0.008039</td>\n","      <td>0.069441</td>\n","      <td>0.008039</td>\n","      <td>0.089661</td>\n","      <td>0.573811</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.004600</td>\n","      <td>0.008178</td>\n","      <td>0.070319</td>\n","      <td>0.008178</td>\n","      <td>0.090435</td>\n","      <td>0.566423</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.004500</td>\n","      <td>0.008718</td>\n","      <td>0.073188</td>\n","      <td>0.008718</td>\n","      <td>0.093368</td>\n","      <td>0.537842</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.004600</td>\n","      <td>0.007626</td>\n","      <td>0.067370</td>\n","      <td>0.007626</td>\n","      <td>0.087328</td>\n","      <td>0.595698</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.004500</td>\n","      <td>0.007824</td>\n","      <td>0.067752</td>\n","      <td>0.007824</td>\n","      <td>0.088455</td>\n","      <td>0.585193</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.004400</td>\n","      <td>0.007849</td>\n","      <td>0.068126</td>\n","      <td>0.007849</td>\n","      <td>0.088595</td>\n","      <td>0.583882</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.004500</td>\n","      <td>0.008307</td>\n","      <td>0.069215</td>\n","      <td>0.008307</td>\n","      <td>0.091142</td>\n","      <td>0.559613</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.004700</td>\n","      <td>0.007912</td>\n","      <td>0.068753</td>\n","      <td>0.007912</td>\n","      <td>0.088949</td>\n","      <td>0.580551</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.004200</td>\n","      <td>0.008234</td>\n","      <td>0.068414</td>\n","      <td>0.008234</td>\n","      <td>0.090742</td>\n","      <td>0.563470</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.004400</td>\n","      <td>0.008645</td>\n","      <td>0.072131</td>\n","      <td>0.008645</td>\n","      <td>0.092977</td>\n","      <td>0.541699</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.004300</td>\n","      <td>0.007937</td>\n","      <td>0.068889</td>\n","      <td>0.007937</td>\n","      <td>0.089089</td>\n","      <td>0.579225</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.004300</td>\n","      <td>0.007923</td>\n","      <td>0.067566</td>\n","      <td>0.007923</td>\n","      <td>0.089011</td>\n","      <td>0.579969</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.004300</td>\n","      <td>0.008170</td>\n","      <td>0.069487</td>\n","      <td>0.008170</td>\n","      <td>0.090389</td>\n","      <td>0.566863</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.004300</td>\n","      <td>0.007830</td>\n","      <td>0.067535</td>\n","      <td>0.007830</td>\n","      <td>0.088487</td>\n","      <td>0.584899</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.004200</td>\n","      <td>0.007972</td>\n","      <td>0.068110</td>\n","      <td>0.007972</td>\n","      <td>0.089288</td>\n","      <td>0.577347</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.004400</td>\n","      <td>0.008232</td>\n","      <td>0.069822</td>\n","      <td>0.008232</td>\n","      <td>0.090730</td>\n","      <td>0.563588</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.004200</td>\n","      <td>0.007879</td>\n","      <td>0.067681</td>\n","      <td>0.007879</td>\n","      <td>0.088766</td>\n","      <td>0.582271</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.004200</td>\n","      <td>0.007913</td>\n","      <td>0.068275</td>\n","      <td>0.007913</td>\n","      <td>0.088954</td>\n","      <td>0.580502</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.004100</td>\n","      <td>0.007743</td>\n","      <td>0.067189</td>\n","      <td>0.007743</td>\n","      <td>0.087996</td>\n","      <td>0.589494</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.004100</td>\n","      <td>0.007846</td>\n","      <td>0.068171</td>\n","      <td>0.007846</td>\n","      <td>0.088578</td>\n","      <td>0.584046</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.004100</td>\n","      <td>0.007684</td>\n","      <td>0.067149</td>\n","      <td>0.007684</td>\n","      <td>0.087656</td>\n","      <td>0.592657</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.004100</td>\n","      <td>0.007959</td>\n","      <td>0.068658</td>\n","      <td>0.007959</td>\n","      <td>0.089214</td>\n","      <td>0.578050</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.004100</td>\n","      <td>0.008051</td>\n","      <td>0.069018</td>\n","      <td>0.008051</td>\n","      <td>0.089726</td>\n","      <td>0.573187</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.004100</td>\n","      <td>0.007889</td>\n","      <td>0.068256</td>\n","      <td>0.007889</td>\n","      <td>0.088819</td>\n","      <td>0.581777</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.004000</td>\n","      <td>0.008019</td>\n","      <td>0.068839</td>\n","      <td>0.008019</td>\n","      <td>0.089548</td>\n","      <td>0.574881</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.004100</td>\n","      <td>0.007785</td>\n","      <td>0.067640</td>\n","      <td>0.007785</td>\n","      <td>0.088234</td>\n","      <td>0.587269</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.004100</td>\n","      <td>0.007835</td>\n","      <td>0.067909</td>\n","      <td>0.007835</td>\n","      <td>0.088513</td>\n","      <td>0.584652</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.003900</td>\n","      <td>0.007852</td>\n","      <td>0.068063</td>\n","      <td>0.007852</td>\n","      <td>0.088612</td>\n","      <td>0.583726</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.003900</td>\n","      <td>0.008060</td>\n","      <td>0.068908</td>\n","      <td>0.008060</td>\n","      <td>0.089776</td>\n","      <td>0.572712</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CPU times: user 2h 36min 29s, sys: 58.7 s, total: 2h 37min 28s\n","Wall time: 2h 36min 55s\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=18700, training_loss=0.011004272167057915, metrics={'train_runtime': 9414.7397, 'train_samples_per_second': 63.422, 'train_steps_per_second': 1.986, 'total_flos': 1.1466989717335726e+17, 'train_loss': 0.011004272167057915, 'epoch': 100.0})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["OUTPUT_PATH = '/content/drive/MyDrive/Bert_complex/results/' + model_name.split('/')[-1] + '-Multimodal-' + data_args.combine_feat_method.split('/')[-1]"],"metadata":{"id":"A7FGcNPBDYia","executionInfo":{"status":"ok","timestamp":1688318325883,"user_tz":300,"elapsed":377,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["trainer.save_model(OUTPUT_PATH)"],"metadata":{"id":"3ve-v5rzkhyB","executionInfo":{"status":"ok","timestamp":1688318329099,"user_tz":300,"elapsed":883,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"rbq7-KkxkmPR","executionInfo":{"status":"ok","timestamp":1688318371336,"user_tz":300,"elapsed":1957,"user":{"displayName":"María Fernanda Arteaga Burgos","userId":"10222738349358087086"}},"outputId":"05cc3187-9f99-45d9-d31d-e0fd60aade8c"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='282' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [94/94 00:40]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.00888506043702364,\n"," 'eval_MAE': 0.07431876728679752,\n"," 'eval_MSE': 0.008885060858950404,\n"," 'eval_RMSE': 0.09426060077758047,\n"," 'eval_R2': 0.5300616925234536,\n"," 'eval_runtime': 1.8662,\n"," 'eval_samples_per_second': 400.272,\n"," 'eval_steps_per_second': 50.369,\n"," 'epoch': 100.0}"]},"metadata":{},"execution_count":24}]}]}